Hello, if you are reading this, this is a bit of like a personal diary I am keeping to help me, (and anyone else who 
stumbles upon this repository) understand what is going on in my code. They are sectioned into days, though I do not always
remember to update this txt file each day.

DAY 1
----------------------------------
Assembly code goes from top to bottom, (duh). Most CPUs begin by booting in 16 bits, then immediately switch to 32 or 64 
bits depending on the OS, for example, Windows is a 64 bit OS, so an intel cpu in a windows machine might boot in 16 bits,
then transition immediately to 64 bits. Since this assembly code is just for booting, we specify that we are coding in 16
bits.

Next, for the BIOS to know when the OS wants to boots, it reads starting from memory address 0x7C00 and checks if there is
any code there being run. If there is, great, it'll run it and, hopefully, boot up. Another thing is the BIOS requires that
the last two lines of the boot code contain the word 0xAA55 and times 510 - ($ - $$) db 0, which tells the assembler to pad 
the remaining addresses up to 0xAA55 with null.

Now, since assembly starts at the top and goes down to a HLT instruction, it enters the start function, which tells it to
unconditionally jump to main, which it does and, (to my knowledge), saves the return address to some register. 

In the main function, we fill the register AX with 0x0000. AX is a general purpose register that is used in the x86 ISA for 
numbers-based-things. So imagine multiplcation, division, but it can also be used to just hold numbers. We put 0 into the AX 
register so we can put 0 into the DS register, because the DS register cannot manually have integers inserted into it.

Next, we have SI point to MESSAGE. MESSAGE is a static variable in the assembly code and it points to, actually, only the 
byte with ASCII value 'H'. So, in actuality, MESSAGE -> 'H'
MESSAGE + 1 -> 'E'
...
up until 0, which we define as the sentinel. So technically, this is a big data segment (a DS, one could say!)

Also, we use the DS register because the DS registers main purpose is to store the addresses of static variables and, hey, our MESSAGE variable is in fact a static variable. Eventually we want to print out the contents of the MESSAGE variable, and we do this through the instruction LODSB, which implicitly uses [DS:SI] and stores the contents of [DS:SI] into the AL register before incrementing SI to point to the next letter in the segment of code, (the letter 'E').

Of course, we're getting ahead of ourselves right now though. The next thing we do is use MOV SS, AX and MOV SP, 0x7C00.

SS and SP are similar to DS and SI in that they both use memory segmentation and both registers are basically used in pairs. They are used in pairs to point to the beginning of a segment. So, the memory segment DS:SI points to the beginning of the message "Hello World!". While SS:SP point to the beginning of the stack. Since SS is also equal to 0, and SP is equal to 0x7C00, our stack begins at 0x7C00. Sounds dangerous, right? Maybe, but the space behind 0x7C00 is safe until a limit. That is usable memory space, and since stacks are LIFO, everytime we push variables or functions into the stack, it will not actually interfere with the bootloader code. 

Next up, we CALL the puts function (since we call it, that means we save the return address into the stack, I think). Puts pushes the CURRENT INFORMATION in the SI and AX registers (pointer to the letter 'H' and 0x0000 respectively) into the stack. Then, we go into the local function of puts, print_loop, which does the LODSB which loads [DS:SI] into the AL register. We then check to see if the value in AL is the sentinel, 0, which if it is, then we are finished. Otherwise, we must now interrupt the BIOS and tell it that we want to print to video. We do this through this code in particular:

MOV AH, 0x0E
INT 0X10

The MOV AH, 0x0E tells the bios we want to interrupt, and it prepares the BIOS
INT 0x10 is an interrupt code which tells the BIOS we want to display to video the contents in the AL register, (I THINK. I know the AL register is not actually its own register, but just the lower 8 bits of the AX register, so I don't fully know how it works...).

Once we've fully displayed "HELLO WORLD!" and we hit the sentinel, the JE instruction jumps us to the done local function, which pops the AX and SI registers from the stack (thereby returning them to their original values), and then returns by also popping the saved return address from the stack.

Afterwards, the next instruction is simply a HLT, which tells the assembler that we are done.



DAY 2 TO LIKE 4, I THINK?
----------------------------------
A lot has been added now, and I feel like I need to explain some new stuff to myself so I hopefully don't forget it all later. 

1. Real Mode vs. Protected Mode.
x86 processors all start in real mode, and real mode is SUPER limiting. It is locked to 16-bit, and the total memory we have
access to is (I believe) a measly 4 Kb. This is nothing, and if we really want to get into the big boy leagues, we need more
memory soon. So, generally, people will often switch to protected mode at the end of their bootloader. Some people are 
weird, (like one of the youtubers I was following, Nanobyte), and they instead first implement a FAT16 file system, but I
did not learn that yet, and I am not very far into the dinosaur book or comet book to really know about file allocation 
table yet, so I didn't touch it.
Switching to protected mode is actually very easy, even though my code might seem to complicate it. One of the registers,
CR0, has a PE (protection enabled) bit that, on boot, is set to 0. All we have to do, to switch to protected mode, is flip 
that bit to 1, and boom! Now we're in protected mode and we're coding in 32 bits with a LOT more memory. Of course, now that 
we're in protected mode, there are also some things we've lost. In day 1, we were able to print "Hello World!" through the 
use of BIOS interrupts. The instruction "INT 0x10" allowed us to signal to the BIOS that we wanted to display to video. Now 
that we're in protected mode, we no longer have such a luxury, but in return, we do have the ability to abandon assembly 
code altogether and, eventually, switch over to C, or C++, or Python, or whatever you wanna use, (I'm gonna use C because I 
also need to learn that language). The actual switching over to C is handled by our kernel, because we can do whatever we 
want in the kernel, basically. Our bootloader is limited to 512 bytes, which, once we implement the FAT16 file system, we 
will soon realize is not a lot.

2. Global Descriptor Table
This is the 2nd thing I did, and was one of the most confusing things ever, because it surprisingly is not super well 
documented (in my opinion). In order to put it very simply, and how I understand it, the GDT works like the following:
Each segment register (DS, ES, SS, CS) have two parts to them. One is the visible part, which you can access and play around
with with the MOV instruction in x86, and it also contains the "segment selector". The other is the hidden part, which 
contains what is known as the "segment descriptor". Whenever a new segment selector is loaded onto the visible part, the CPU
automatically copies the corresponding segment descriptor from the GDT (or LDT) and puts it into the hidden part. So, what 
is contained in this hidden part? Well, the hidden, segment descriptor, contains the description for the segment, such as:
How long is the segment?
What are the access rights for the segment? (Kernel only or can the user also access them?).
Where does this memory segment start?
etc.
This method all falls under memory segmentation which, supposedly, is an obsolete method of memory storage that has now been 
replaced by paging. But I CURRENTLY do not know what paging is, as it is irrelevant to me as of right now. Eventually, I 
will find out about it. 

3. Kernel 
I haven't written the kernel as of this writing, (hopefully I will have a very simple one that just says 'Hello World from 
Kernel!' by the end of today), however, as stated previously, we eventually want to switch to the Kernel because the kernel
allows us to do a lot more and code in C. 

First things first is establishing where the Kernel should be in memory. Generally, people say to put the Kernel at 0x100000 
from the beginning of the memory, and the reason for that is because from 0x0 to 0x500 is occuped. Then, there is 512 bytes 
reserved for the bootloader in the next 638 KB, (0x500 to 0x9fc00), and finally, from 0x9fc00 to 0x100000 is occupied too.
After that, it is all free space for our taking. So, we decide to put the kernel there.

But how do we actually load the kernel? Well, to do that, we can use a form of addressing to do a huge jump from somewhere 
in the bootloader all the way to 0x100000. The easiest form of addressing to do that is CHS, (Cylinder-Head-Sector). CHS 
addressing follows the way we read the HDD, or hard drive. See, the HDD is made up of a bunch of disks stacked on top of 
each other. To better explain this, I will be using polar coordinates. Imagine the cylinder being controlled by your r, 
(radius), or how far away you are from the center. The sectors are the thetha, or like a slice of pizza from a pizza pie. 
The head is what disk we are currently on, (there are usually multiple disks stacked on each other in a modern HDD). The 
bootloader begins in 
CYLINDER: 0, HEAD: 0, SECTOR: 1. 
Most tutorials told me, and will likely tell you, to just stay in the same cylinder and head, (CYLINDER: 0 and HEAD: 0), but
just move to SECTOR 2, because that is the closest available sector where we can store the kernel binary, (or the second 
bootloader if you want one), onto.

DAY 5
----------------------------------
Day 5 was a bit uneventful. I was really tired having worked on this project all day everyday so I didn't do a whole lot and
kind of just took a break. I did do two very important things, though
1. I fixed the kernel loading up! Now it actually says "Hello World from Kernel!" from the correct memory address, (0x100000)
I will now explain a bit about how this stuff works. Volatile memory, like the main memory (or RAM) is very, very easy and 
quick and easy for the CPU to access. Hence, the CPU requires that any instructions, code, etc. is to be loaded onto the 
main memory first before it is executed. Hence also why we load the kernel onto the memory, so that one, we can execute it 
and run it, but two, so that the CPU has quick and easy access to the kernel. So, you may ask, "why even have stuff in the 
disk in the first place if its so great for the CPU?" Well, for two reasons. The first reason is because it is volative, 
meaning that
once we shut down the PC, everything in the RAM dissapears. This is a big issue, and it means that we will have to restart 
everything everytime we open the system. But wait, that might bring up a question, "how does my system store my files and 
whatnot?". It does that by saving it to the disk. The disk is non-volatile, meaning it can be saved, AND it can store a LOT 
MORE information. But it has the issue where it requires more effort and interrupts from the CPU to access, and hence, is
much less efficient. 
So, "for the CPU to process data from disk, those data must first be transferred to main memory by CPU-generated I/O calls.
In the same way, instructions must be in memory for the CPU to execute them.", (from dinosaur book, 10th edition). 
However, there is a bit of a caveat. Since the bootloader begins in real mode, (16-bit), we cannot immediately load 
the kernel into where we want, (0x100000), because of the fact that 16-bit addressing simply cannot go that high 
(even with the segmentation technique I talked about earlier, the max it can go to is 0x10000). SO, to get around this, 
I made a second stage bootloader that the first bootloader jumps to once it enters protected mode, and then in the 
second stage bootloader, since we're in 32 bits now, we are able to move the contents in 0x10000 (where the kernel 
temporarily is) to 0x100000, and then jump to 0x100000.

2. I've finally entered C! 
I still need to learn more about this process, and how link loaders exactly work, as I mostly did this step through 
tutorials.
On a later day, (probably tomorrow, as I am reserving tomorrow for catch-up-study-day, I will come back and explain 
everything
in much greater detail, and probably even fix some mistakes in my explanations). But yeah! We've finally entered C, and this
is also where, unfortunately, most tutorials end. As all that is left to do now is dive into the more "complex" stuff, and
also design the kernel to OUR fitting. It's a bit worrying, but again, I will try whatever I can. If it doesn't work, then 
I'll try to make it later. That's all for today.

DAY 6
----------------------------------
Day 6 was quite uneventful. I didn't do much. However, I did start learning about memory paging, and realized: Oh shoot, I 
should've probably maybe enabled it.

Though I don't fully understand it, I do get its basics. Memory paging is basically a form of memory allocation, in which we
create a virtual address for each program and make it believe that it has the full range from 0x0 to 0xFFFFFFFF. Of course,
it doesn't actually have the full range. That's where we come in, and we can create and use a paging table to then say "ok,
program 1 is page 0. Page 0 is supposed to be mapped to 0x200000 in the phyiscal address". Thus, we get to control the way 
memory is allocated for each program. Now, why is this better than memory segmentation? I don't know, I still need to study
that. My guess is because each page is going to be the same in its size, whilst the coding and data segments are not equal 
in memory segmentation. I don't fully know, I still need to figure that out. 
Either way, today was boring. I didn't do anything because I was feeling a bit burnt out, and instead focussed on myself 
(played games, went to the gym pet cooper)


DAY 7
----------------------------------
Day 7 I finally got around towards figuring out what I wanna do next. One of my biggest worries with this project is that
I'll hit a wall and not know what to do next. I hope that doesn't happen. Today, though, I figured "why not create a print
function?" To do this, I mostly followed nanobytes tutorial that he had, but I differed from it a little bit. Nanobyte has
paging enabled AND is still in 16 bit real mode. I have neither of those things, so I had to figure out how to make the code
myself. Though it is incomplete, and probably VERY inefficient (I am literally calling stdio.c into kernel.c, which is NOT
good), it works! I've managed to print "Hello World from C!" onto the window. The way it works is abusing the stack. In my 
kernel.s we initialized a stack. Now, before I get into anything, I should preface I am using the GCC compiler. The GCC
compiler, by default, uses a calling convention known as CDECL. The way you save stuff in CDECL is irrelevant (at least for 
now). What's important is how it deals with the stack. When you call a function with CDECL enabled, and that function has 
parameters, the CPU will then first push the arguments into the stack from right to left. For example, say I have:

foo(x, y, z). Then, with CDECL calling convention, we will first push z, then y, then x into the stack. After all the 
arguments are pushed in, then we push in the return address into the stack. Then, the ESP register (the stack pointer 
register), will point to the "top" of the stack, which just so happens to be the return address. If we want to access the
arguments sent to us in the function we just called, we can then manipulate the ESP by adding or subtracting values. Since 
each "thing" in the stack takes up 4 bytes, if we add 4 to the ESP, we get x. Add 4 again, we get y, add 4 another time, and
we get z. This is how we can access arguments sent to us in a function. When all is said is done, we can then simply RET,
which will assume that the ESP still points to the return address (which it should), and then it'll return to the address
specified on the stack. Using this knowledge, I managed to print stuff onto the video! Woohoo!

Oh, I should specify how we print stuff onto video. We do this through VGA text mode. VGA has its own segment in the memory,
and that segment lives at address 0xB8000 and continues on for a while. The way it works is that each 0xB8000 stores the 
character, and 0xB8001 stores the background color, or other stuff, of the character at 0xB8000. By the way, 0xB8000 is the 
top left corner, and adding 2 increases the column to the next character position. So, if we store the register EAX with the
character, then that means AL (bottom 8 bits of EAX) has the character, and we just have to change AH (top 8 bits of EAX) to
be the background color and stuff (and we set it to 0x0F, which means WHITE ON BLACK). This allows us to print ONE CHARACTER.
But wait, what if we want to print MORE characters? Well, to do that, we should move to C. We can define a header that has 
C's putc and puts functions. Then, we can define a C file that uses the header, and then actually implements our putc and 
puts functions. Putc is easy, since all we have to do to display a character is just, well, put in a character into our
assembly code. Puts is a bit hard, but, since we know strings are simply just an array of characters, we can just send each
character one by one into the assembly code, and voila! We are printing strings! Woohoo!
Now all that's left is to include the c files and headers into the main kernel function, use puts("Hello World!") and now
we have an actually printing function! 

Tomorrow, I will enable paging and destroy everything. I will also try to make an actual printf() function in our main
kernel.c, so we can actually start printing for real in C, instead of fake printing in C.
----------------------------------
DAY 8 & 9
I got sick on these days, so I wasn't able to work on the project much. :(
----------------------------------
DAY 10
On Day 7 I decided I wanted to turn on paging.
I decided to NOT do that just yet, as I figured out there were some issues with my puts and putc functions. First, if I 
had a long enough message, that message would not wrap around. There was no overflow management happening, so I needed to fix
that. But by far the most GLARING issue was the fact that if I had multiple puts and putc functions, puts and putc would 
magically stop working. Even MORE confusing was that if I had another function in kernel.c that I wasn't even using, 
(like it was just EXISTING in kernel.c), puts and putc would ALSO stop working. Immediately I knew this had to be an issue
between two-three culprits.
The first culprit was that I assumed something incorrectly. Perhaps I used a register that had an implicit rule, or
I was just being unoptimized and it was messing something up. For this, I decided to clean up my x86_video_whatever it was
called function, and actually use the BP register, which is what a lot of people do whenever they're messing around with the
stack.
Speaking of the stack, this brings us to the second culprit. The stack. I assumed that, maybe, I was just being stupid and
I had actually not properly set up the stack, or I just forgot how the cdecl calling convention worked??? I dunno, but
I DID end up finding that I wasn't actually using the stack at all, and that my code just so happened to work. Originally I
was using the linker script 'ld' with the flag --oformat to directly create the .bin file by linking all the C and assembly
files. This turned out to be a BAD IDEA, because for some reason, "--oformat binary" sets the size of the .bss section to 
be 0, or just drops the .bss section. This is because the .bss section is not treated the same as .data or .text, as it 
is considered "unallocated memory", and has type SHT_NOBITS, (at least I think). A section of type SHT_NOBITS may be in 
the file, but occupies no space in the file, so we have to force it to see the .bss section. The linker ld uses the 
objcopy through the BFD library. The objcopy is the real culprit, as it does not include anything of type SHT_NOBITS to
copy over. So we convert it to an ELF type first, as ELF preserves the full memory layout, THEN we use objcopy to convert
to binary, (it wont think .bss is empty this time, because the ELF file contains it already). Also, it's a good idea to
use this anyway since, according to the OSDev wiki, generally binary files are "a flat binary with no formatting at all."
which is not great. (At least, I would assume it's not great, since we would like it to format to elf-i386 32-bit)
And finally, the third culprit. I don't understand the third culprit yet, and so I will not talk about it yet.
It is late, and I still have some glaring issues with puts and putc that I need to iron out before I can make printf and whatnot. 
The biggest issue has to do with reaching the edges. The window is 80x25. So, if I have two puts functions that are 40 characters each then,
you would think, I would reach the limit. But no, instead, it breaks, and nothing is displayed instead. Oh but wait, if I just have ONE puts
function with 80 characters, then it works just fine! But if I try to add clamps, then that one puts function fails too! What is even going on??
I'm going to try fixing this tomorrow, hopefully.

----------------------------------
DAY 11
Alright so it turns out none of the culrpits I talked about previously were the issue,  (well, I mean the .bss --oformat binary stuff was 
certainly an issue that I'm glad I fixed). Instead, the issue was that in my kernel file, I was specifying it with:
SECTION _text
instead of
SECTION .text
And my link loader doesn't know what _text is, as it only knows about .text, leading to the huge issues. That one character fix seemingly fixed 
everything, (or at least I am hoping it fixed everything. I can't seem to find any bugs at the moment..).
OK. New bug was discovered while I was implementing printf. The more I implemented printf, the more and more my strings were truncating. I knew 
that the issue HAD to be memory allocation, but I couldn't for the life of me figure out what the issue was. Eventually, while looking through
the boot code, and genuinely feeling like giving up and restarting this project from scratch and actually following nanobytes tutorial 100%, 
I found out in my boot2.s I was only copying the first 512 bytes of code over to 0x100000. That is nothing, so I changed it to 4096 and 
Everything started working again. 

Furthermore, today I implemented the functionality for printf. The OSDev wiki had a page on it, so I assumed I needed to do it. Also it is
quite nice to be able to actually print things stored in variables, so why not, right? Anyway. The implementation is only really half complete.
I followed Nanobyte's tutorial, but he doesn't make use of the va_list, (even though OSDev wiki tells me to), and he's in 16-bit real mode still
so I have to change some stuff. Unfortunately, though, the stuff I changed ended up not being great ideas and now the code doesn't function. 
For one, the "continue" I have doesn't increment fmt, so in some cases, (in literally evey case), I end up being stuck in an infinite loop in
printf. Also, the number to ASCII thing definitely does not work (my emulator keeps flickering). But oh well, this is all stuff I will do for
tomorrow.
Oh yes, as for how the implementation works. On paper, it is simple. We observe printf through a sort of finite state machine. Since strings are
just arrays, we will increment through the string starting from the far left and moving rightward towards the sentinel at the end. We begin at
state NORMAL. If we discover there exists no "%", and we are in no other state, then we simply print whatever character we are currently at
and move on. If we discover a '%', then we move to a state "LENGTH". Here, we once again increment and check the next character after the '%'.
If this next character is 'h', or 'l', that means we are thinking about short or long respectively. So we increment once again to see the thing
AFTER 'h' or 'l'. If there is another 'h' or 'l', that means we're in SHORT SHORT or LONG LONG. You can't be SHORT SHORT SHORT or LONG LONG LONG
so that means the next thing that comes must mean we are printing something out, so we move to the PRINT_STATE state, where we have a bunch of
cases for various commands, like %d, %i, %x, %o, etc. 
%d implies that we have to be printing numbers. Now, we cannot just prints numbers by themselves, as they are not characters, they are ints, and
ints are usually 32 bits (4 bytes), and chars are 8 bits (1 byte). So, we have to somehow make 32 bits into 8 bits and keep the same display 
value. How would we do that? Through the power of ASCII tables. ASCII tables allow us to take integers and print them as characters. So, we must
do that through a special function. That special function is printf_number, which takes a value and prints it into an ASCII value based on a 
specific base, (base 10, 16, or 8 depending on % code). It uses long division to repeatedly divide the numbers by the base to extract digits,
and then stores the corresponding digits in reverse order. Then we can just print it in reverse order and get the ASCII value for the number. 
If you want a better-ish explanation, I would recommend watching Nanobyte's printf implementation video.
Now back to printf. After it has processed this one character, it finishes, resets the state and length back to NORMAL, and then moves onto the
next letter.
Oh, by the way, our printf is technically wrong. The header is supposed to return an int according to the C++ documentation, but I don't care. 
This is my printf. So I made it return nothing, but the parameters are the same! (const char* fmt, ...). 
Anyway, tomorrow I'll fix printf and I'll do some more studying and whatnot. Then hopefully this week I can turn on memory paging to protect
my memory.

----------------------------------
DAY 12
Today was a bit of a nothing burger day in terms of new stuff. It was really just more debugging and studying. As of right now, I believe I have
EVERYTHING in my program working smoothly. Printf actually prints everything I want it to print, (including long long hexadecimals, which is
something I almost never see, but why not). I've also updated my x86_div64_32 algorithm, after realizing that my original algorithm was trying to
divide a 64 bit number by the base (base 10, 16, or 8), and then store it. This is stupid, and obviously won't work. So I went back to Nanobyte's
video, and I discovered a new "tutorial" channel, Queso Fuego, whom also talked about the "long division method" where they take the first 32
bits, divide them by the base, store the quotient and remainder, then take the last 32 bits, append the remainder, and divide it by the base, 
before storing everything once more. Adding the two will give you the actual division, just like if you were to try and divide the 64 bits in 
one go. This worked much better, (duh), and actually allowed me to use %llx, as before I was unable to use them with my implementation. 

As for studying, I just read more of the intel manual, OSDev wiki, and the dinosaur book (specifically chapter 2), to get a better understanding
of where I want to go next. I think the next plan of action is the following
1. More studying. I really need to read more of the dinosaur book, it is a very good book and teaches the concepts quite well. I want to have read
a large chunk of it by the time I'm done with v1.0 of this project.
2. Set up memory paging. I've been talking about doing this for a while, but I think I've held it off for long enough and I need to do it soon.
3. Set up TSS w/ ESP0 stack. I don't fully know what these are yet, but the OSDev wiki recommends them
3. Set up IDT. The IDT is not only necessary for entering user mode (ring 3), but also will be quite helpful in the future for debugging purposes.
4. Enter user mode. I'm quite excited for this, as there is a lot to do after I enter user mode still. 
5. This part I need to study some more, but I believe I need to:
  - implement various user programs (a simple clock, maybe)
  - implement system calls for said user programs (a system call interface)
  - set up a timer 
  - set up scheduling (so I can multitask and not force the user to have one thing at a time. Timer is also necessary for this, paging too)
After that, I should probably implement a file system, drivers, and other shenanigans that I need to study up on. (FAT32?)

----------------------------------
DAY 13
Today was very confusing, and that is why I will be dedicating tomorrow towards a study day. Perhaps even the rest of the week, I will dedicate
to being a study-remainder-week. I need to catch up more on the x86 ASM language, as I believe I am starting to reach the point where I'm not
fulling understanding the assembly code the tutorials are giving me.
The things I did understand, though, is the newly updated putc and whatnot with proper cursor handling. Beforehand I was relying on the assembly
code and then sending in a global variable, cursor_pos. This worked, but it didn't literally update the cursor position. Now, I use a volatile
8-bit memory pointer that uses 0xB8000 as a pointer itself (bleh, jumbo of words). As I've explained before, the VGA memory address works like
the following: 0xB8000 stores a character, and 0xB8001 stores the attributes of said character, (like background color, font-color, etc.). So,
by having a buffer integer pointer that points to an 8-bit integer, we are basically saying that the variable uint8_t* g_ScreenBuffer points to
the address 0xB8000, which we are able to then print directly to, and then we can move onto the next character by playing around with the pointer.

I also started working on setting up the IDT. The IDT is, of course, for interrupts. I believe I've mentioned in this dev-log that, once we enter
protected mode, we lose the ability to use interrupts (like INT 0x10, INT 0x13, both of which we used quite extensively. 0x10 is for video memory,
0x13 combined with AH 0x02 is for reading from a disk. Fun fact: INT 0x13 AH 0x03 is for WRITING to a disk, so that's fun). I never actually
explained WHY we lose the ability to use interrupts. That's because in real mode, to be able to use interrupts, we had the IVT (interrupt vector
table) already available to us, and so we could use its many functions. But, in protected mode, the CPU now requires the implementation of the
IDT, (interrupt descriptor table) for us to not only be able to use some interrupts (like being able to get keyboard input. This will be VERY 
important once we get into user mode), but also for the hardware to give us some interrupts (divide by 0 and whatnot). Now I still don't FULLY
understand the implementation of the IDT, so I will get back on this devlog tomorrow when I have more understanding, but to my knowledge, the
IDT is very similar to the GDT, in that when an interrupt is issued to the CPU, it will look to the GDT to figure out what ISR to call, (ISRs are
interrupt service routines, they will handle interrupts, so we would have an ISR for divide by 0, for example, that would process, and then return
function back to the CPU to continue reading memory). When an interrupt is handled, the CPU, of course, needs to stop what it is doing. But it
can't just DROP what it is doing, that would be very inconvenient, imagine having to download something, and you accidentally press a key, making
the download process just stop. So, the CPU needs a way to save the information of what it does, I believe it does this through the IDT too,
in particular, if an interrupt occurs on kernel mode, (which is good), then all it has to do is push the error code, if there is one, onto the
stack, the CS register, the ES register, and the EFLAGS, (if the interrupt is from user mode, it has to switch to kernel mode and also push a
lot more stuff onto the stack). The Intel manual is more specific, so I'd recommend checking it out (go to figure 6.7, it explains it quite well).
The IDT also has some other stuff, like the interrupt gate and trap gate. I, again, don't fully understand the IDT just yet, but to my 
understanding, and according to the OSDev wiki, the trap gate sucks and is useless, so don't implement it. Just stick to the 32-bit interrupt
gate. We use the interrupt gate because it clears the IE bit (interrupt enabled) in the EFLAGS register, so if one interrupt is being processed,
more interrupts cannot come and mess up the process. The trap gate doesn't do that. Furthermore, the severity of an exception/interrupt is 
organized from most severe (0) to least severe (256). There's a big ol' table on the Intel manual that gives us the list of interrupts, 
(table 6.1 in the manual). 
Furthermore, I started to implement the PIC. My thought process, as of right now, is to follow what the OSDev wiki has told me, which is, to
be able to implement and code in interrupts into your OS in 32-bit protected mode, we need to do the following: 
- Make space for the interrupt descriptor table
- Tell the CPU where that space is (see GDT Tutorial: lidt works the very same way as lgdt)
- Tell the PIC that you no longer want to use the BIOS defaults (see Programming the PIC chips)
- Write a couple of ISR handlers (see Interrupt Service Routines) for both IRQs and exceptions
- Put the addresses of the ISR handlers in the appropriate descriptors (in Interrupt Descriptor Table)
- Enable all supported interrupts in the IRQ mask (of the PIC)
Currently, I believe I've done steps 1, 2, and 3. BUT, before moving onto implementing ISR handlers, I have come across a bug that I believe has
to do with memory (I don't have any memory management techniques as of right now, so it makes sense for these issues to arise). I believe the 
issues are arising because I'm stupid and I don't understand the things I've implemented, so I will take a step back and try to understand
EVERYTHING I've implemented, and play around with it to see how they work. And genuinely this time. Yesterday was supposed to be a study day, 
but I ended up debugging instead. I will try not to debug tomorrow, instead, I'll play around with stuff and try to understand it. Also I've been
using a lot of tutorial code. This is not good. I will also try to change and make stuff mine.

----------------------------------
DAY 14

Today was a study day, but I also decided to do some debugging. Thankfully, the debugging was easy, because it was something I had seen before.
If I had too many functions, or if I had too many kprintf() function calls, or just too many lines, something would stop functioning, 
generally my kprintf(). So, I knew the issue had to be memory related. Low and behold, after like 10 minutes of figuring out what the issue was,
I figured out that I was copying only 8 sectors from the disk to the memory in my boot.s, when my kernel was taking up 9 sectors. SO, I just 
changed it to 9 sectors, and everything works now. I really need to make a dynamic memory allocator for that.

As for the IDT and PIC, as those were the main things I studied today, I think I finally understand them. Interrupts 0 - 31 are occupied by the
CPU, and are directly thrown by the CPU itself, so our PIC doesn't actually manage it. PIC is a piece of circuitry that ENABLES interrupt-based 
I/O to occur, otherwise, we would be forced to use polling. Basically, what happens is that the I/O has some lines connected to it and the various
external hardware, like the mouse, keyboard, etc.. A PIC usually has 8 IRQ lines, (IRQ 0 thru 7), and each line corresponds to a different 
external hardware sending an interrupt signal. Now, when the PIC recieves the signal it does some things to it. First, the PIC_DATA port contains
an 8-bit binary that is basically a mask for the various IRQ lines. (So, it'd be like 00000000 on default). If we don't want keyboard interrupts
to interrupt our CPU, we can mask the IRQ line that the keyboard hardware is associated with (for example, lets say keyboard is associated with
IRQ line 7, I don't think it actually is, but I'm too lazy to check right now). Then, we flip the 7th bit to 1 and we get: 10000000. So, if
the keyboard ever sends a signal through IRQ_LINE_7 now, the PIC will see that the PIC_DATA port's 8-bit binary mask has the 7th bit flipped
to 1, and so we now ignore any signal coming from IRQ Line 7, or the keyboard. That's masking. The priority resolver, which is the next thing
the PIC does, sees how to assign the interrupts if MULTIPLE IRQ lines have sent signals. The general scheme is the following: the timer is the
most important, followed by keyboard, followed by the cascade to slave, etc.. That's the priority resolver. Finally, the ISR is a register that
basically is the security guard at a club. It checks to see if an interrupt is being processed (i.e., has the CPU sent an EOI signal back to
the PIC yet?), and if it is being processed, then it tells the interrupts in line to wait. Once the CPU sends an EOI signal back, then we can send
the next interrupt to the CPU. The transfering of the interrupt to the CPU is controlled by a variety of things, like the data bus buffer, and
the read/write logic thing, and that's all kind of un-important to really understand the PIC right now. There is another thing, and it has to do
with the existence of the slave PIC's. You see, I said the PIC's have 8 IRQ lines. That's not a lot, and the IDT, supposedly, can manage 256 
descriptors for different interrupts. If the first 31 are to be controlled by the CPU, then the interrupts 32-255 (external hardware interrupts)
surely can not be managed by 8 IRQ line alone, correct? Correct. They can't. That's why we can have up to 8 master-slave PIC's, but for now, 
I only have one other PIC, or slave PIC, allowing us to have 15 IRQs. Yes, 15. Not 16. That's because the master and slave PICs still have to
communicate with each other, so IRQ2 is used to communicate with the slave PIC. If the slave PIC recieves an interrupt (say we get an interrupt
on IRQ 14 or something, clearly out of the master PIC's range), then the slave PIC can send the signal, after it has done all the masking and 
whatnot in its OWN registers, to the master pic through IRQ2, where the master PIC can then send it to the CPU. So, basically, the way it works
as of right now, with the slave pic included is:

One of the PICs recieve a signal from external hardware via the IRQ lines (0 - 7 for master, 8 - 15 for slave). 
The PICs manage said signal, and see if it should even be sent. 
PICs check to see if the CPU has sent an EOI, (is it processing an interrupt right now?)
If not, then the slave PIC will send the INT to the master PIC through IRQ2, which will then send the INT to the CPU
the CPU, if it allows, will send back an INTA signal, which is just the CPU acknowledging the interrupt, and asking for the interrupt number, or
vector, or whatever
If the master PIC found the interrupt, it will then calculate the interrupt number based off of the IDT:
(interrupt number = IRQ_Line + base_offset)
And then it'll send it to the CPU.
If the slave PIC found the interrupt, the master PIC will instruct the slave PIC to calculate the interrupt number, again based off of the IDT,
and then it'll send it back to the master PIC, who will send it off to the CPU.

I believe that is basically how these interrupts work.
Oh, I noted that interrupt number = IRQ_Line + base_offset.
The IRQ_Lines are from 0 - 7 and 8 - 15, and base_offset is generally going to be 0x20 and above, because anything lower is being used the IVT,
or the CPU itself reserves those offsets, (or at least, I placed it as 0x20. The OSDev wiki doesn't specify, but I used 0x20, because the
IDT states that the external hardware interrupt starts from 0x20 and goes onward, so we should probably specify it to that for the vector offset).

That's basically how the PIC works. As for the IDT, we haven't implemented any ISR's yet, nor are we in user mode, so it isn't a huge issue,
but there is the issue regarding how the CPU should switch into an ISR. Generally, there are two types of interrupts, precise and imprecise 
interrupts. I will be really only dealing with precise interrupts, because imprecise interrupts seem impossible to manage. Basically,
precise interrupts assume and require that every instruction up to the instruction the CPU is currently at has been 100% executed, and every
instruction AFTER the instruction the CPU is currently at hasn't even been touched. This allows for easy flow between the CPU doing the actual
process and handling the interrupt. Of course, Tanenbaum also says there are some issues with this, as there is no guarantee, and also you CAN
have some interrupts be imprecise, (like a divide by zero interrupt can be imprecise, or a trap gate interrupt can be imprecise, because you
aren't really gonna be going back to the process after these interrupts), so I'll probably take that into heart when developing the ISRs. Also,
I've grossly simplified the PIC, or at least somewhat simplified it, because there are actually a lot more registers it has at play, but the 
basic concept of the PIC I think I've managed to explain quite well. For more information, I would highly recommend checking this out:
https://pdos.csail.mit.edu/6.828/2014/readings/hardware/8259A.pdf
Very good PDF that explains the PIC and its inner mechanisms. 
That's all for today. The weekends I have decided to make into rest days, but I am quite restless about this project, so I'll probably work on
something anyway, or at least I'll study.

----------------------------------
DAY 15 & 16

As stated above, I've made the weekends into rest days. I didn't do anything these days except hang out with friends and stuff

----------------------------------
DAY 17

Today was an ok day. Somehow, due to the fact that I hadn't touched the code for two days, it felt as though I was re-learning everything,
(not really). 
Today, though, I did learn some new stuff. For one, I have a better understanding of the IDT and PIC code, largely thanks to the studying I did
on Friday, and also largely thanks to the fact that I had started writing ISRs today! I have created a stub for ALL 256 interrupts available, 
though, I have disabled the external hardware interrupts (the ones that come from the PIC via IRQ lines), due to the fact that I haven't
implemented any of them at all, and they all require the use of the EOI method I've made to signal to the PIC that we're done processing that
interrupt.
However, I DID start on implementing the keyboard handler! We now are able to actually interract with the operating system and type stuff!
Of course, the stuff we type isn't actually worth anything, as we're just typing the ASCII representation of the keys we're inputting. I believe
now would be a good time to explain keyboard interrupts, along with more information regarding interrupts. 
Firstly, I will begin with interrupts. I've already explained a great deal about them with my day 14 explanation on PIC, but I will explain WHY
we use interrupts. 
Interrupts is NECESSARY for a CPU to work to its fullest potential. Without them, our OS would be moving at snails pace. Why is that? Because, 
well, take a very basic example for now: typing "Hello World"
Say, for example, you are a super fast typer, and you can type at an astonishing 300 WPM. While to you, and to literally every living species, 
300 WPM might seem fast, to the computer that is operating at microsecond speeds, 300 WPM is a snails pace. So, say we weren't using the interrupt
method, and instead, we were using the polling method. There would be quite a lot of issues. The polling method is relatively simple to implement,
and is actually what, (at least I think), quite a few real mode (16-bit) OS's use for user I/O. Basically, if we ever want the user to type 
something, instead of using interrupts, we can have the CPU drop everything it's doing and constantly nag the keyboard hardware. We keep asking
the keyboard, and its data ports, "hey, is there anything here yet?" over and over, every CPU cycle, until the keyboard eventually says "yes,
there is something in my data port now". Then, we ask the keyboard, "Well, can I take the stuff out of the data port?". We do this because,
although there is stuff in the data port, it isn't necessarily ours for taking immediately. We have to ask keyboard for permission to take it.
Eventually, the keyboard says "yeah, ok, take it now". Then we can take the letter, process it, display it, do whatever we want with it, 
throw it away, then go back to asking the keyboard for the next letter. In theory, this SOUNDS ok, because you might imagine a scenario where the
user is asked to put in a command. Then, you would imagine that the CPU would want to have its attention focused towards you, right? Well, not
exactly. The issue with polling is that it restricts the CPU to being able to do nothing except wait for the user to type at what feels like
a snails pace. Imagine being the Flash in speed-time and having to watch a snail crawl a kilometer. That's probably what the CPU would feel like.
See, we NEVER want the CPU to be doing nothing, because the CPU can ALWAYS be doing something, like other processes. That's where interrupts come
in. Instead of having the CPU sit and wait for the user to type stuff, we can have the CPU instead go and do other stuff while it's waiting. THEN,
when the keyboard is ready (which is signaled to the CPU via the PIC/APIC), the CPU has two options: either 1, it says "hold on a second, I am
doing a HIGHER PRIORITY task/doing a different interrupt", OR, it can say "alright, give me the interrupt". This can save immeasurable time,
and it allows for the CPU to effectively multitask multiple things, and that is why it is so desired.
Next up, the keyboard handler. I'm too lazy to explain it. So, instead, I will link to a very good website I found that explained it quite nicely:
https://www.basicinputoutput.com/2024/11/the-keyboard-controller-interface.html

----------------------------------
DAY 18
Today was a more productive day than yesterday. I managed to set up the keyboard handler to actually display the characters the user is typing, AND I've managed to
make it so the user can use stuff like caps lock, shift characters, etc. AND, I've managed to make it so I can use the stuff the user types (this will be very useful for
when I switch to user mode and create system calls/programs). 
The way I've managed to do this is because, again, how the keyboard handler works. I was lazy and didn't explain it yesterday, so I'll explain it a little bit today, or
at least, I'll explain the parts that are important to us. First and foremost, whenever the user presses a key, there is an external piece of hardware on the computer 
that puts in a SCANCODE into the port 0x60. Notice how I said scancode and not the hex code, or ASCII code of the characters. The scan code is... Weird.. Thankfully, scan
codes are similar to hex and ASCII codes, where each character corresponds to its own unique character. So "A" = 0x1E. To see all the scan codes and their mapping, go
to the OSDev wiki: https://wiki.osdev.org/PS/2_Keyboard. Anyway, to fix this, all we need is an array that maps the scan codes to their respective keys, (which is exactly
what we do). The way that I do it might not be the most elegant or nicest looking, but I don't care. It works. As for actually handling the inputs, it's pretty simple. 
We simply define an input buffer array that accepts a certain number of characters (I used 256, it doesn't really matter though). We also create various boolean values, 
particularly the "input_ready" boolean value, that basically tells us that the user has created a message for us to use. We will specify that the message is only available
to be used IF the user has pressed the "enter" key. Since a string of characters, when it is read, must have the \0 at the end of it as a sentinel, whenever the user
inputs a new character, or deletes a character, we will ALWAYS have the current index pointer for the array be pointing to the \0 sentinel at the end of the string.
Example:             this is where our pointer is 
                                  V
"H, e, l, l, o, , W, o, r, l, d, \0, (NULL), (NULL), (NULL), ..."

If we add another character, we simply override the \0 to be the newly pressed character, then we increment the pointer (so it's pointing to nothing), and then place
a \0 there. We do a similar thing but in reverse for whenever the user presses backspace or wants to delete a character. We decrement the pointer, and then override 
whatever character was there with \0. 

Now that's the keyboard stuff. That is IRQ1. As I've stated before, there are multiple IRQ lines, and one of the most IMPORTANT IRQ lines is IRQ0. I have yet to fully
implement IRQ0, and I have yet to finally understand the external thing connected to IRQ0, but I will explain everything I understand right now. 
IRQ0 is connected the PIT. The PIT, though, isn't only connected to IRQ0. The PIT, (known as the Programmable Interval Timer), is simply a timer. It has multiple uses,
primarily for the system to keep track of the time, enable multi-tasking/scheduling (whatever you wanna call it), and to stop one process from taking too much time/hang
for too long. It's relatively simple in the way it works. It's simply an oscillator, a prescaler and three independent frequency dividers. The oscillator is what creates
the frequency. The PIT is, historically, known to have a default frequency of 1.193182 MHz. The reasoning for this frequency is due to history, but just know, this is
the frequency we're working with. The purpose of a frequency divider is to achieve a slower frequency, and we can do it relatively simple in software by using a counter
that counts down from a tick. If we have a frequency divider of 3, then we basically can have a counter that counts down every tick from the 1.193182 mhz frequency 
until it reaches 0, where it will send a signal to SOMETHING. As we said, the PIT has three independent frequency dividers. These independent frequency dividers are more
commonly known as the PITs channels, and each channel does something different and, thus, we can also specify the program to not only have different frequencies in each
channel, but also execute/do different things in each channel, (duh). "Each PIT channel also has a "gate input" pin which can be used to control whether the input signal
(the 1.193182MHz one) gets to the channel or not. For PIT channels 0 and 1, the associated gate input pin is not connected to anything. The PIT channel 2 gate is
controlled by IO port 0x61, bit 0." (stole this from the OSDev wiki because they explained this really well). Now to explain what each channel is contected to and what
each channel does. 
Channel 0 is directly connected to IRQ0. Therefore, that makes it incredibly good for setting up a system clock. Typically during boot, the BIOS sets channel 0 with a
count of 65535, (which is the frequency divisor), thus having the frequency be 18.2065 Hz (or about 54.9254 ms).
Channel 1 is useless. Literally. It used to be used for refreshing the DRAM (Dynamic Random Access Memory) or RAM (Random Access Memory), but on later machines, the
refreshing of the DRAM and RAM is now done by dedicated hardware, and now channel 1 is utterly useless and sometimes not even allowed to be touched.
Channel 2 is directly connected to the PC Speaker, so the frequency of the output determines the frequency of of the sound produced by the speaker. I won't be explaining
the PC Speaker here, because I haven't read about it yet, but if you want more information, here it is: https://wiki.osdev.org/PC_Speaker. 
Next up are the ports and the stuff accepted by the PIT. Instead of explaining it in my own words, I think the OSDev wiki does an excellent job in this scenario, so
I will just copy and paste it here:
"The PIT chip uses the following I/O ports:
I/O port     Usage
0x40         Channel 0 data port (read/write)
0x41         Channel 1 data port (read/write)
0x42         Channel 2 data port (read/write)
0x43         Mode/Command register (write only, a read is ignored)"

"The Mode/Command register at I/O address 0x43 contains the following:              (AGAIN, THIS IS FOR 0x43)
Bits         Usage
7 and 6      Select channel :
                0 0 = Channel 0
                0 1 = Channel 1
                1 0 = Channel 2
                1 1 = Read-back command (8254 only)
5 and 4      Access mode :
                0 0 = Latch count value command
                0 1 = Access mode: lobyte only
                1 0 = Access mode: hibyte only
                1 1 = Access mode: lobyte/hibyte
3 to 1       Operating mode :
                0 0 0 = Mode 0 (interrupt on terminal count)
                0 0 1 = Mode 1 (hardware re-triggerable one-shot)
                0 1 0 = Mode 2 (rate generator)
                0 1 1 = Mode 3 (square wave generator)
                1 0 0 = Mode 4 (software triggered strobe)
                1 0 1 = Mode 5 (hardware triggered strobe)
                1 1 0 = Mode 2 (rate generator, same as 010b)
                1 1 1 = Mode 3 (square wave generator, same as 011b)
0            BCD/Binary mode: 0 = 16-bit binary, 1 = four-digit BCD"

The stuff you put into a channel port, (0x40, 0x41, or 0x42), the number you put will be the DIVISOR. Basically, if you put 1000, the frequency in the channel, or
the independent frequency divisor will be (1.193182Mhz/divisor).

To create an actual tick/clock thing, I still don't know how it works, lol. I will get back tomorrow to explain how to make an actual clock, I think.

----------------------------------
DAY 19
Today was a bit of a bad day, because I ended up sleeping super late yesterday, waking up super late today, and then thus, not getting a whole lot
of work done. Despite that, though, I did manage to do SOME things today, though I don't feel very accomplished. For one, we finished the IDT. Well, sort of. 
Not really. See, I finished implementing the PIT, and so now, the CPU will be sent an interrupt by the PIT through channel 0 every millisecond or so, (it's a little bit
more than a millisecond, but it's basically a millisecond). For now, this millisecond interruption is utterly useless, and all we do with it is increment a 64 bit 
(long long unsigned) tick counter that starts counting up from 0 as soon as the kernel is loaded into memory, (or almost as soon as the kernel is loaded into memory. 
See, my kernel is very disorganized, and the IRQ0 line is only unmasked until quite a bit later, but who cares! It's basically from as soon as the kernel is loaded
into memory). So, now that the IDT is (basically) finished, (if we ever need more interrupts for the various IRQ lines, or more in-depth interrupt handlers for any other
interrupt, we can always implement them later. It's not as though they are all necessary to have), we can move onto the next, and one of the scariest things about OS
development: memory allocation. I did a LITTLE BIT of studying on memory allocation prior to getting into this, so I know a little bit about how I'm going to do this. 
I will basically be planning it like so:
1. I will implement a physical memory manager. This is the part that I understand, I believe, the 2nd-most, or most about. A physical memory manager is quite important,
as once we get into having processes floating around in our memory, we would like to be able to manage each process, so that we do not have one process's information
overriding the information of another process. Furthermore, memory is SMALL. I believe I talked about this earlier, but RAM is very quick and easy for the CPU to access,
however, the downside is that it is 1. volatile, meaning its contents are lost when the PC shuts down, and 2. it is SMALL. For a 32-bit CPU, (which is what we're dealing 
with), the memory only has 4GB, which, if you have ever looked at your storage, or looked at any application ever, you will realize "oh wow, that is literally nothing". 
Indeed, if you have ever actually looked at your VRAM, and RAM, you will also realize you actually don't have a lot of space in memory, (for example, I have 16GB of RAM,
which is nothing! But I've played huge games that have been over 60+ GB. How do we play with all of these?). The answer to this is with the MMU, or Memory Management Unit.
What is the MMU? Well, as the name implies, it's a component of many computers that handles memory translation, protection, and other stuff related to memory in an 
architecture. What is memory translation and protection? I'm so glad you asked. Memory protection is the physical memory manager, basically. As I stated previously, 
when we have multiple processes on the memory, we don't want one process to override a part of another process when we load it into memory, and thus, we need a way to 
handle allocating an deallocating memory for each process. This is, again, where the physical memory manager comes into play. I have decided to take on the very simple
(although very inefficient) bitmap approach to creating a physical memory manager. Basically, the way that my plan will work is the following:
1. The memory will be divided into 4KB chunks. This is because paging does it the same way. 4KB is nice for a couple of reasons, but before I explain any of those reasons,
I should probably explain the bitmap. The bitmap is simply a very long array in which each index of the array corresponds to a page/block/chunk in the memory. So, for
a very simple example, imagine that the memory is 10 bytes. If we decide to make every page 2 bytes, then we will have a 5-length bitmap array/list, and each index of
the bitmap will correspond to its own unique 2-byte page in the physical address space. Simple, right? There are a couple issues with this, though. First and foremost, 
the size of the pages. Lets say our memory is the standard 32-bit size, 4GB, and we decide to make each page 1 byte. Then, we will have an array of size 4000000000, which
isn't feasible, because, although we can use 32-bit or 64-bit integers to mark the indicies, we still need this bitmap to be on the memory, and our bitmap is size
4 billion, that means our bitmap is the ENTIRE MEMORY, and so we can't actually do this at all, (it isn't feasible). I'm sure you've seen the issue now, if we make our
pages/chunks too small, then our bitmap consumes a large portion of the memory. But, I hear you say, what if we just make it super big? That still has its own issues. 
First, lets say we decide to make the chunks 10 megabytes. Then, our bitmap is super tiny, and doesn't consume a lot of memory. Great! Now, we load in a program that is
a mere 500 bytes into the memory. That 500 bytes goes into the first 10 megabyte chunk and, oh no, would you look at that, the rest of the 9+ megabytes become unusable, 
because the bitmap has 1 bit that corresponds to each chunk. 0 means that nothing is in that chunk, and 1 means something is in that chunk. So, if our chunks are TOO large,
then we end up wasting a lot of memory and have nothing in them. That is why we need a nice middleground, and it is mostly believed the middleground is 4 kilobytes, 
because Linus Trovalds (creator of Linux) used 4 kilobytes for his paging, and so why not, right? You might have looked at my incomplete physical_memory_manager.h and
noticed the other defined thing and asked what that is. That is simply there because generally the physical memory is split into bytes. So, we have that there to say that
one byte will manage 8 different chunks, because 8 bits = 1 byte, and 1 bit manages 1 chunk. Furthermore, as I stated previously, the bit map must exist on the memory.
Of course, that means we want the bitmap to never be touched, because if it ever is touched, that means our PC will likely explode (not really, exaggeration, but it will
be VERY bad). So where would be a good place to put? Well, our kernel is untouched, and we like to put at 0x100000, right? The user space is generally higher up, and sits
at at 0x200000, (or even higher, I haven't really studied the user space yet). So, why not put the bit map as close as we can to the end of memory? This will mean that
we will have to have a LOT of programs running on the memory concurrently for it to actually mess up the bitmap, therefore giving us good leeway. So, to figure out how
much memory we have, we use some fancy schmancy stuff (particularly INT 0x15, EAX 0xE820. For more information, look at the OSDev wiki: 
https://wiki.osdev.org/Detecting_Memory_(x86)), and load it into memory in a safe spot. Then, we can read that information in the kernel, and display it onto the screen
so that we can actually SEE the reserved and available memory. There is also one extra benefit for doing this. Once we implement a genuine physical memory manager, then
we can actually see how many pages exist in our memory, see if it lines up, allocate some new pages/deallocate some pages, and once again see if it lines up. So,
doing this step is just good for debugging (and is why I did it). Anyway, once we have a bitmap, we must simply start the chunking process, create a way to allocate/free
memory chunks, create a way to FIND memory contiguous memory chunks for larger processes (bigger than 4 kilobytes), and then boom! We have a physical memory manager.
Next up is the virtual memory manager. The virtual memory manager is also for paging, (as paging allows us to create virtual memory addresses). Basically, the virtual
address space is not real, but we give each process the illusion that it has access to the full physical address space, when it really doesn't. Then, we can simply 
manage each process, assigning them to a differnet page (or, I guess, frame. It's called pages for virtual memory and frames in physical memory, I believe). I believe
this is how the virtual memory manager works, but admittedly, I haven't studied it yet, so I am mostly spitballing here. After we have a physical and virtual memory manager
I believe we will be safe to turn on paging. We can turn on paging whenever we want, but I decided to make memory managers first. As far as I know, virtual memory only
exists when paging is turned on. It doesn't exist before then, but I may be wrong about this, as, again, I haven't studied this. Anyway, oh yeah, as I was saying 
for my plan:
1. Implement physical memory manager
2. Implement virtual memory manager (will be useless without paging, I THINK?)
3. Turn on paging
And then boom! Our kernel now has memory management! There are other things we can do after we have both memory management AND interrupts (particularly the PIT), like
setting a scheduler, but that is complicated and I want to stay as a single-task/single-thread kernel for now. Later on, if I feel confident, I might make a multi-thread
system. Multi-threads are scary, because they can completely mess up how we manage interrupts (according to Tanenbaum, at least), and how memory might be managed too,
but they have the benefit of being far more efficient. The best way I can explain threads is with this example, (which I HOPE I am getting correct, because if not, then
I will come back later and fix this explanation):

void print_A() {
    while (1) {
        putchar('A');
    }
}

void print_B() {
    while (1) {
        putchar('B');
    }
}

This entire piece of code is a process. But, the process includes two threads, print_A, and print_B. Each thread has its own stack, register set, (duh), thread ID, and PC
and much more. Furthermore, if there exists more threads in a process (which there are), then it will share, with the other thread, its code section, data section, and 
other OS resources. A single thread system will basically say "ok, I see print_A() is first, so I will follow that thread, complete it, then come back and see if there
exists any other threads". But, a multi-thread system, (along with some functions in the threads, like maybe a yield() function), can jump between two threads whilst also
saving the state that each thread originally was in. So, in a multi-thread system, you could feasibly have the code above print something like "ABABAB", (for the sake of
this example, assume the two threads are being called 3 times each), while a single-thread system can only ever print "AAABBB". I hope I got threads right, it is also
a big confusing to me, because a lot of explanations literally just use the definition, "basic unit of CPU utilization", which I think is a stupid definition, because to
a noob like me, it doesn't really make sense. Again, if I am wrong, I will come back and edit my explanation of threads so that future me (and anyone else reading this),
doesn't get confused/get the wrong idea. That is all for today. Tomorrow I will finish the physical memory manager, and hopefully start on the virtual memory manager. 

----------------------------------
DAY 20

Today I did quite a bit, though it might not seem like a lot. Today I started on the development of my memory manager, and I began by working on the physical memory manager
and I've actually (I hope) finished the physical memory manager. The way our physical memory manager works is through a bitmap, (I think I mentioned this yesterday).
While a bitmap has glaring issues, and a lot of people seem to use a buddy-allocator or a stack instead to manage physical memory, I found those to be a lot more
complicated to not only understand, but also implement, so I stuck with the bitmap approach. In fact, my approach is so bad that, even amongst bitmap physical memory
mappers, mine is by far one of the most inefficient, because when looking free chunks, it scans through EVERY bit, instead of trying to do some optimization (like by
scanning 32-bits first, or something. The OSDev wiki talks about it). But I don't care. All I care about is getting it to work first, then I can optimize it to my hearts
content later on, when I feel like it. Anyway, to really understand the physical memory manager, I had to do one other thing before that, and that was to get the 
information regarding the physical memory. There is one really good way to do that, which the OSDev wiki outlines, and it is through INT 0x15, EAX 0xE820. For information
on how this works, I would just suggest looking at the OSDev wiki: https://wiki.osdev.org/Detecting_Memory_(x86), because I don't fully understand it, and I don't really
care (not yet at least). But anyway, all we did with this method is store the information regarding physical memory at a safe address, (I picked 0x9000. The OSDev wiki
uses 0x8000. I can't use that because that's where my second-stage bootlader is). Then, all we have to do is read through 0x9000, and print to the screen the information
regarding the physical memory. INT 0x15, EAX 0xE820 is actually very useful because it also tells us what parts of the memory are reserved or available to us, which
is nice. Anyway, through this I learned that I'm actually temporarily loading my kernel at a reserved section of the memory! This is not good at all, and I should
honestly switch it, but I have yet to encounter a mistake/bug, sooooooo... If it ain't broke, don't fix it. There was actually one bug I was encountering when I was working
on the physical memory manager, and it was a very very odd one. You see, randomly, whenever I tried to run my code via the qemu emulator command, in SOME cases, my code
would run just fine and it'd be all fine and dandy. In other cases, as soon as it finished loading in the physical memory information, it would experience an interrupt
(interrupt number 8). What's even MORE confusing is that it had an error code! Yes, I know that interrupt 8 is supposed to have an error code, but according to OSDev wiki
the error code should be 0. This one was NOT 0. Also, again, it was happening randomly. Eventually, I figured out it was because whenever I initialized my IDT, I was
turning on interrupts and giving interrupts free reign to do whatever they liked before I could actually disable the IRQ interrupts. So, the fix for this was to simply
move the PIC_disable to be the first command run by the kernel, so that even when I do initialize the IDT, nothing funky happens with interrupts just yet until I say
they're allowed to do whatever they like. Also, in more exciting news, the physical memory manager was one of the things I've managed to (mostly) implement myself! I feel
quite proud, and I feel as though I am becoming more and more confident in my ability to code in C. Anyway, enough with the random talk, I should probably talk about my
future plans. So far, here is my gameplan: I begin by calling initialize_pmm(), which will set the entire bitmap to be 1's. This will effectively state, 
"hey, the entire memory is in usage/reserved". But in reality, it is not. Then, in the kernel, based on the available entries in the SMAP (from INT 0x15, EAX E820), 
I will call initialize_memory_region with their base addresses and lengths (all of which is given by the SMAP). Then, I will manually go in and deinitialize some of 
the memory from 0x100000 (a few KB, maybe a full MB), because I want that to be reserved for the kernel. I will also deinitialize some of the memory from 0x7EE0000,
because that is where the bitmap is, and I don't want it to ever override it. Finally comes the creation of malloc, or the memory file in general. My plan for memory is 
that I will have two functions, at least. One function will find the size/length of a process/program/file. How I will find the size, I don't know yet. I'll have to look
at some tutorials, maybe. After that, I will have a memory allocation function, (kmalloc), that will put a given program/file/process into memory based on the base_address
that is going to be provided when I use allocate_blocks(num_blocks). And then, boom! I think that'll be everything. I'll need to make a de-allocation method too, that will
also free memory of the process. Then that should be it.. I should have a genuine physical memory manager then.
Buuuttt, this was all the stuff I typed in my physical_memory_manager.c, which I thought was going to be my plan back then. This is  no longer my plan now, (though it is
still very similar). Tomorrow I was planning on making the memory allocation stuff, but then I decided "hold on, I'm gonna need a virtual memory manager soon anyway, and
the entire reason I made my PMM like this, using 4KB chunks, is because of paging. I should probably enable paging tomorrow and develop the VMM before going into malloc".
Indeed, this is also something the OSDev wiki, and some tutorials/videos I've been watching, agree with it. The OSDev wiki claims that malloc should be one of the last 
things you implement for a memory manager, and so I will do the same. Also, I believe I've come to realize the POINT of a virtual memory manager. See, today, after I
finished implementing the physical memory manager, I decided to then follow my initial plan, which was setting the entire bitmap to be 1's (or reserved). Then, I went
and changed the parts of of the bitmap that memory said were supposed to be available, (as stated by the SMAP). I noticed some nice things. First, we have quite a lot
of memory to work with, almost 32,639 available memory blocks! (Remember, each block is 4KB, so this is quite a lot). Of course, we are going to be decrementing from
some of those blocks, as those blocks still take into account a few things, like our kernel, the bitmap itself, and some other spaces that we would like to reserve
for the kernel to use. But regardless, we have quite a lot of space. I also went and thought about virtual addresses, and I came to realize something. See, available
memory isn't contiguous in the physical memory. There are reserved chunks, available chunks, then reserved, then available, then reserved, reserved, reserved. So, say that
a lot of the available chunks have been filled with processes, and there is a little bit each available chunk left, (say 12 KB in total, but 6KB in 1 chunk, and 6KB in
another chunk). Then, based on my PREDICTION OF VIRTUAL MEMORY, (again, I need to study it), I am guessing that if we try to load in a 12 KB process, virtual memory
management will be able to split up the process and map them into the physical memory (thanks to PMM), but since the process believes the illusion of the virtual memory,
it won't have any issues with this, and it will run as if it were contiguously mapped to the memory. If this is indeed how virtual memory is intended to be used then,
great, that sounds like it's quite useful and important. If it's not, then I really need to figure out what the point of virtual memory is.
Anyway, that's all for today. Tomorrow will be FINALLY turning on paging, (I've been putting it off for SOO long), and implementing the VMM, (I do sincerely believe the 
PMM is easier to implement, but I am hoping I'm wrong, and I'm hoping it turns out that the VMM is much easier to implement than the PMM). After I've done the PMM, we
might make the heap memory allocator. I gotta see, but I will eventually need a way to actually take processes from the disk and put it onto memory based on the specified
address and size of it, (it was easy in real mode- just use INT 0x13 and CHS addressing, or LAS if you're weird, but we don't have that luxury now). Oh, I almost forgot
to mention too. I decided I wanted to get a taste of programming commands. I haven't studied how to do that just yet, so I decided to make a really simple one: basically,
the user gets stuck into a while loop that, obviously, always loops and uses the readLine() function I implemented to read what the user wrote. Then, I wanted to compare
what the user wrote to a specific phrase, and if they say that, it'll print out what we want! But, oh wait, I don't have <string.h>, which is apart of the C standard 
library. So, I had to implement string.h and string.c. It was quite simple, so there is no issues there (simply have a pointer to each character in the string, and since
characters can be checked if they're equal via ==, we can just check each character, and increment each character to their next character IF they are equal. If they are
either not equal, or if they land upon their sentinel, then we terminate. Then, again, since characters are just fake integers, we can subtract their difference. If
they are the same (i.e., they are both the sentinel), they will return 0, (just like the actual strcmp), otherwise, it will return the difference of the first non-equal
characters in both strings, (just like strcmp). Anyway, we used this, and the length function (quite simple, just have a counter that increments for each character in the
string that isnt the sentinel), to make our first "program!". (It's not really a program), but it works! If the user types printmem, it'll showcase the physical
memory! 
Anyway, that's all for today.
Ok, bye.


----------------------------------
DAY 21
Today was not a great day. I didn't finish implementing virtual memory manager, but I did come to understand the point of paging, and virtual memory as a whole. 
Tomorrow, even though it is supposed to be a weekend, I will finish implementing virtual memory management, so I'm not super behind.

I will also add more info (what I learned today) to this devlog entry tomorrow.

It is now DAY 22, and I will be finishing the devlog today.
On day 21, I mostly learned stuff regarding virtual memory and paging, particularly, I learned about the concepts and I had started work on it by following the broken
thorns tutorial, along with the OSDev tutorial. And it turns out, my suspicions were correct regarding virtual memory. It's there to make management of memory and 
processes easier, (duh), but more importantly, it is there to make the management of processes easier. So, one of the biggest issues regarding memory is loading stuff
in from the disk. This takes a lot of time, and it is inefficient for the CPU to be constantly accessing the disk. That is where the power of virtual memory can come in.
Every process can be loaded into their own virtual address, (or, more specifically, each process can have its own page directory, or 4GB of virtual address space). If
the CPU is not using a process, and the OS deems the process to be wasting memory/believes the CPU is in need of memory, it can de-allocate some of the process's pages
from the physical RAM and put it back into the hard disk, before setting the present bit in the page table entry to 0. IF, for some reason, the CPU is in need of that
page again, it will FIRST check the flags and see the "present bit" in the page table entry. If the page table entry says the page is NOT present (present bit is 0),
then the CPU will send a page fault exception (or interrupt 14) to the OS, which it can then handle with its own page fault handler. Now, this might seem complicated,
and to me it was, but I believe it becomes much more clearer with an example.

So, when the computer first boots and everything, and a process is FIRST EXECUTED, the first thing that happens is that it is loaded from the disk into memory by the OS.
The OS then provides the process its own virtual address, by mapping its physical address to some random virtual address, (lets say, I don't know, 0x8000000). Now,
for the sake of this example, and to understand virtual addressing better, lets say this process is fairly large, taking up multiple pages (remember, each page is
4KB, or 4096 bytes). Without the virtual memory manager, if the CPU wanted to run the process, it would have to load in the entire process into memory before executing it,
which is obviously quite wasteful, because the CPU doesn't need the ENTIRE process at once. It first needs the first page, then the second page, then the third page, etc.
until it finishes the process. Basically, if you've ever played a game before, do you remember seeing that "invisible bridge" thing, where you cannot see the bridge,
but as soon as you, the player, start walking over the bridge, the bridge would become visible right under your feet? This is basically what we want to do in the future.
If a process is gigantic, like thousands of pages, (say it is a 5GB process, but we only have 4GB of RAM), we can basically create the illusion of having more RAM than 
we actually do by using this "invisible bridge" illusion. Parts of the process will be first loaded in, (for this example, assume physical memory 0x1000 for simplicity's
sake). Then it will be mapped to a virtual address (assume 0x0 in the virtual address). It will execute that first page, and get to the ending of the
first page, which is around 0x2000 or whatever, (or 0x1000 in virtual memory). The CPU will check the present bit and see the next page is not available, so it will
throw a page fault exception, or interrupt 14, to which the OS will handle. The OS will first see if there is available free memory, and if there isn't, it must then
de-allocate some of the memory. Assume the CPU has completely finished handling the first page, so the OS will then de-allocate that first page, and it MIGHT swap the
contents of the the first page from the physical RAM into the disk. DO NOTE, the OS will only do this if another one of the flags for the page entry was set, that flag
being the DIRTY FLAG, (which means the page was modified). If the page was NOT modified, and it was 100% processed by the CPU, then the OS doesn't care, and it simply
discards the first page entirely. Anyway, the first page will be de-allocated by the CPU, and then the CPU will put the 2nd page where the 1st page originally, (0x1000). 
BUT, it this time maps the 2nd page to virtual address 0x1000, therefore making the process have the illusion that it is still running on contiguous memory, even though it
clearly is not. This is end-goal of virtual memory, and it is one of the reasons for why it is so important. Other important reasons include memory protection, and 
ensuring that no processes override each others data, memory isolation, and whatnot. But, this is the one thing that made really go "oh, that makes sense for why virtual
memory is so seemingly important". 

Now that is the motivation for virtual memory, I will now explain the basics of how virtual memory works, or at least the implementation of it.
Virtual memory is really only applicable when we have paging. Paging is what enables virtual memory to exist in the first place, and a virtual memory manager, without
paging enabled, is completely useless and is just taking up precious memory, and readability, away from your code. Before we implement a virtual memory manager, we first
need a physical memory manager, and I have already explained the motivation and the implementation of a PMM beforehand. The reason we need a PMM is because virtual memory
manages pages, and each page is 4096 bytes, or 4KB. This should sound familiar, because we also just so happen to divide our memory into 4KB blocks in the PMM. The way
paging works is that we have 4GB of physical memory, which is not a lot. As stated previously, we want to create the illusion of having more memory than we actually do,
so we achieve this via the usage of page tables and page directories. Each page table has 1024 entries, and each table entry points to a page, (each entry is a 32-bit 
integer). The entries in the table also point to a physical FRAME, which is just the term for a block in physical memory. Now, I mentione dearlier that pages are 4KB,
and our physical frames are also 4KB. Why 4KB? Historical reasons. It's just considered the standard in most systems today, so we stick to the modern conventions. 
Anyway, each page is 4KB. Page tables have 1024 entries, so they have 1024 pages. 1024 * 4KB is 4MB. Now, we have a page directory, which is just an array of page tables
so 1024 * 4MB = 4GB. If 4GB sounds familiar, it should. 4GB is the maximum amount of physical memory a 32-bit system has. Therefore, we have just created 4GB of virtual
memory. We can create more of these directories, and we WILL, and I will explain why later (I did briefly talk about it in the paragraph above). Anyway, back to 
page tables. Each entry is a 32-bit integer, and the 32 bits are split up: 
Bits 31-12 are the address. They point to the physical frame address
The reset of the bits are simply flags that the CPU uses. Some important bits include: the dirty bit, and the present bit. The present bit is what the CPU uses to see if
a page is present IN the physical memory right now. If it is not, it throws a page fault exception (interrupt 14). If it is, then it continues. The dirty bit is for 
the OS. If the OS deems a page necessary to be relinquished, (as it has been 100% processed), then it will check the dirty bit to see if it has been modified. If it has
been modified, the OS will assume it will be important in the future, so it will save it to the disk. Otherwise, it will merely throw it away and forget about it.
Here are the page table bits:
Bit 0 (P): Present flag
0: Page is not in memory
1: Page is present (in memory)
Bit 1 (R/W): Read/Write flag
0: Page is read only
1: Page is writable
Bit 2 (U/S):User mode/Supervisor mode flag
0: Page is kernel (supervisor) mode
1: Page is user mode. Cannot read or write supervisor pages
Bits 3-4 (RSVD): Reserved by Intel
Bit 5 (A): Access flag. Set by processor
0: Page has not been accessed
1: Page has been accessed
Bit 6 (D): Dirty flag. Set by processor
0: Page has not been written to
1: Page has been written to
Bits 7-8 (RSVD): Reserved
Bits 9-11 (AVAIL): Available for use
Bits 12-31 (FRAME): Frame address

Page directory also stores 32-bit integers, and they have similar meanings to the page table entries. However, instead of having bits 31-12 point to the frame address,
they just point to the page table instead:
Bit 0 (P): Present flag
0: Page is not in memory
1: Page is present (in memory)
Bit 1 (R/W): Read/Write flag
0: Page is read only
1: Page is writable
Bit 2 (U/S):User mode/Supervisor mode flag
0: Page is kernel (supervisor) mode
1: Page is user mode. Cannot read or write supervisor pages
Bit 3 (PWT):Write-through flag
0: Write back caching is enabled
1: Write through caching is enabled
Bit 4 (PCD):Cache disabled
0: Page table will not be cached
1: Page table will be cached
Bit 5 (A): Access flag. Set by processor
0: Page has not been accessed
1: Page has been accessed
Bit 6 (D): Reserved by Intel
Bit 7 (PS): Page Size
0: 4 KB pages
1: 4 MB pages
Bit 8 (G): Global Page (Ignored)
Bits 9-11 (AVAIL): Available for use
Bits 12-31 (FRAME): Page Table Base address

When paging is enabled, memory addresses follow a certain format based on the page directory, table, and whatnot. We want every memory address to reference the directory,
then the table in the directory, so that the memory addresses are not simply identity-mapped (identity mapping is simply when virtual addresses = physical addresses),
and instead point to their respsective frame addresses.
SO, memory addresses will follow THIS format, (example of 0xC0000000):

1100000000         0000000000        000000000000 (this is 0xC0000000 in binary)
 
AAAAAAAAAA         BBBBBBBBBB        CCCCCCCCCCCC
directory index    page table index  offset into page

As you can see, 0xC0000000 points to the 768th table, 1st page, and has a 0x0 offset into the page (I forgot to mention, but every page is 4KB aligned, duh).
So, if we want 0xC0000000 to point to 0x100000, we simply go into the 768th page table in whatever directory, and have the 768th table house the frame address
0x100000, and boom! Now 0x100000 maps to 0xC0000000. 
For more information, (and a better explanation, honestly), go to the brokenthorns guide on virtual memory management and paging:
http://www.brokenthorn.com/Resources/OSDev18.html

Ok, that is all. I am writing this on Sunday, so I will be coming back the next day to work lol

----------------------------------
DAY 22 & 23 

Look at DAY 21 to see what I did on these days.

----------------------------------
DAY 24

Today was quite a bad day in terms of progress.
By that, I mean I didn't progress at all. What I tried to do today, since I had finished memory management, was try and set up the next big thing. That being, I tried
to set up a higher kernel. I thought this was going to be relatively easy, as my plan was simple: 
Have one link loader present for the lower kernel, (which will be setting up the PMM and VMM, maybe even the IDT). Then, once paging is enabled, jump to 0xC0000000.
We'll have another link loader present for the higher kernel at 0xC0000000, which will have the same libraries and it'll all work fine and dandy.
It did not work fine and dandy. My idea was solid, we were able to jump to 0xC0000000 via the usage of another link loader being physically at 0x100000, and then having
the lower kernel (physically at 0x50000) jump to 0xC0000000, that was then mapped to 0x100000 at the lower memory kernel, but for some reason, and I have NO idea why,
all of the libraries (like stdio), were completely unfunctional. After hours and hours of testing and checking, I realized the issue was due to the global variables
I was using in stdio, but I had to use global variables, especially for stuff like the cursor position, because how else am I supposed to keep track of it?? Can I use
macros for that?.. Maybe.. I dunno actually... But, regardless, it was odd why I couldn't use global variables, because it worked just fine in the lower kernel memory, 
so what the hell was the issue now?? Again, I still don't know what the issue is. I checked literally everything, and I do mean everything. I checked the .rodata section,
it was properly loaded in, same with the .data section, same with .bss. I even went as far as to make another assembly file that would zero out the bss section, and set
up another stack, but nothing worked. Even MORE odd was, when I tried debugging in gdb, was that the higher kernel was NOT at 0xC0000000 exactly, it was at 0xC000200.
WHY? 0xC0000000 is aligned to 4KB, there is no reason for that offset to exist, especially because I didn't specify it in the link loader. What the hell is going on??
Why is this happening??
And after going insane for almost the entire day, I came to the conclusion to revert everything (except for the existence of the files), and go back to where we started
for today. Tomorrow, I will shift focus towards trying to implement a heap memory allocation thing (malloc), and then probably, maybe, hopefully, (I am really hoping),
enter into user mode finally.... I did find a video by Queso Fuego that talks about shifting kernel into higher memory but... I don't know, I'm quite worried. I'll
give it one last try tomorrow, and if it doesn't work, for whatever reason, then that's it. I'll just keep the kernel un-divided, existing in 0x100000 FOR NOW. Later on
I might try to actually have it separated and so we can actually say "aha! We use 0xC0000000 to virtually map our kernel!" but, for now, again, none of those shenanigans.
Actually, before I move on, I think I was going about it the wrong way. See, what I tried to do was keep my existing kernel.c, and then just create a new kernelHigher.c,
but what might be better would be to create just a new prekernel.c and keep kernel.c (or, maybe even make a new kernel.c), and have the prekernel.c just do the bare
minimum to see if it even works (which is just setting up paging and whatnot), then jump to the higher half kernel. THAT might work, I hope, but I have no clue. 
Anyway, that is all for today. Today was a bad day and it really left a sour taste in my mouth. I know that no project will always have every day be a smash-hit home run,
or even have every day achieving something, but I really wanted to at least get this higher half kernel stuff out of the way. The issue might've actually been
related to the linker stuff, but I have no clue, because I have never studied linkers. Whatever, though.
That is all for today.
Bye.
Ok I lied, I am back (I am writing this around 2 hours later). I couldn't go to bed in good conscious knowing I did effectively nothing today, so I came back and thought 
about why it didn't work a lot, and I also looked at Queso Fuego's OS, along with other peoples OS. I noticed some things: yes, some people were doing what I was thinking,
which was using 2 linker scripts for the different kernel separations, good job me. But, when I examined how they compiled the linker scripts and what was in each script,
I noticed something:
The libraries that had global variables (usually the ones containing printf) were exclusively saved for the higher kernel memory page, while the lower kernel memory page
did the bare minimum, and acted as a trampoline (thanks OSDev wiki for that terminology: https://wiki.osdev.org/Higher_Half_Kernel). I should note, OSDev DOES have a wiki
providing a tutorial on higher half kernel, but it assumes the user is using GRUB, (which... Okay, that's fair, the bare bones tutorial has the user set up GRUB, so I
am not very surprised. By the way, I am not using GRUB, I am using a custom bootloader.). So, I have an idea now, which I have somewhat implemented. I will remove the 
pmm and vmm libraries from my kernel.c (duh), and I will have it act as the higher kernel, setting up everything, and housing everything. I know the VMM and PMM have
global variables, which is a little bit confusing, because OS's like Queso Fuego's AmateurOS have both the kernel and pre-kernel housing the VMM and PMM, but I think the
trick he, along with other OS devs used, to get around this is by saving the contents of the global variables to a global_address file, then, since the global_address file
is legal and will not be changed even amongst separate bin files, we can simply go into the higher half kernel, and set the global variables in there to align with 
whatever the pre-kernel had, thereby avoiding any issues. So, I just can't use something like stdio.h in both kernels, so I will save it for the higher half kernel 
exclusively. AAnyways, again, I have created the prekernel.c file, and I have housed within it everything that I think is necessary. Tomorrow, I will create malloc and 
whatnot, and then I will try to, once again, enter higher half kernel. I will be following Queso Fuego's video on it, mostly because I failed when I did it by myself: 
https://www.youtube.com/watch?v=pm2L0TxofQU. Ok, that is ACTUALLY all for today. Thank you, and Goodbye. (I just didn't want to sleep in a sour mood, I wanted to sleep 
convincing myself I did SOMETHING)
