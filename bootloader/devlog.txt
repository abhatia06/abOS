Hello, if you are reading this, this is a bit of like a personal diary I am keeping to help me, (and anyone else who 
stumbles upon this repository) understand what is going on in my code. They are sectioned into days, though I do not always
remember to update this txt file each day.

DAY 1
----------------------------------
Assembly code goes from top to bottom, (duh). Most CPUs begin by booting in 16 bits, then immediately switch to 32 or 64 
bits depending on the OS, for example, Windows is a 64 bit OS, so an intel cpu in a windows machine might boot in 16 bits,
then transition immediately to 64 bits. Since this assembly code is just for booting, we specify that we are coding in 16
bits.

Next, for the BIOS to know when the OS wants to boots, it reads starting from memory address 0x7C00 and checks if there is
any code there being run. If there is, great, it'll run it and, hopefully, boot up. Another thing is the BIOS requires that
the last two lines of the boot code contain the word 0xAA55 and times 510 - ($ - $$) db 0, which tells the assembler to pad 
the remaining addresses up to 0xAA55 with null.

Now, since assembly starts at the top and goes down to a HLT instruction, it enters the start function, which tells it to
unconditionally jump to main, which it does and, (to my knowledge), saves the return address to some register. 

In the main function, we fill the register AX with 0x0000. AX is a general purpose register that is used in the x86 ISA for 
numbers-based-things. So imagine multiplcation, division, but it can also be used to just hold numbers. We put 0 into the AX 
register so we can put 0 into the DS register, because the DS register cannot manually have integers inserted into it.

Next, we have SI point to MESSAGE. MESSAGE is a static variable in the assembly code and it points to, actually, only the 
byte with ASCII value 'H'. So, in actuality, MESSAGE -> 'H'
MESSAGE + 1 -> 'E'
...
up until 0, which we define as the sentinel. So technically, this is a big data segment (a DS, one could say!)

Also, we use the DS register because the DS registers main purpose is to store the addresses of static variables and, hey, our MESSAGE variable is in fact a static variable. Eventually we want to print out the contents of the MESSAGE variable, and we do this through the instruction LODSB, which implicitly uses [DS:SI] and stores the contents of [DS:SI] into the AL register before incrementing SI to point to the next letter in the segment of code, (the letter 'E').

Of course, we're getting ahead of ourselves right now though. The next thing we do is use MOV SS, AX and MOV SP, 0x7C00.

SS and SP are similar to DS and SI in that they both use memory segmentation and both registers are basically used in pairs. They are used in pairs to point to the beginning of a segment. So, the memory segment DS:SI points to the beginning of the message "Hello World!". While SS:SP point to the beginning of the stack. Since SS is also equal to 0, and SP is equal to 0x7C00, our stack begins at 0x7C00. Sounds dangerous, right? Maybe, but the space behind 0x7C00 is safe until a limit. That is usable memory space, and since stacks are LIFO, everytime we push variables or functions into the stack, it will not actually interfere with the bootloader code. 

Next up, we CALL the puts function (since we call it, that means we save the return address into the stack, I think). Puts pushes the CURRENT INFORMATION in the SI and AX registers (pointer to the letter 'H' and 0x0000 respectively) into the stack. Then, we go into the local function of puts, print_loop, which does the LODSB which loads [DS:SI] into the AL register. We then check to see if the value in AL is the sentinel, 0, which if it is, then we are finished. Otherwise, we must now interrupt the BIOS and tell it that we want to print to video. We do this through this code in particular:

MOV AH, 0x0E
INT 0X10

The MOV AH, 0x0E tells the bios we want to interrupt, and it prepares the BIOS
INT 0x10 is an interrupt code which tells the BIOS we want to display to video the contents in the AL register, (I THINK. I know the AL register is not actually its own register, but just the lower 8 bits of the AX register, so I don't fully know how it works...).

Once we've fully displayed "HELLO WORLD!" and we hit the sentinel, the JE instruction jumps us to the done local function, which pops the AX and SI registers from the stack (thereby returning them to their original values), and then returns by also popping the saved return address from the stack.

Afterwards, the next instruction is simply a HLT, which tells the assembler that we are done.



DAY 2 TO LIKE 4, I THINK?
----------------------------------
A lot has been added now, and I feel like I need to explain some new stuff to myself so I hopefully don't forget it all later. 

1. Real Mode vs. Protected Mode.
x86 processors all start in real mode, and real mode is SUPER limiting. It is locked to 16-bit, and the total memory we have
access to is (I believe) a measly 4 Kb. This is nothing, and if we really want to get into the big boy leagues, we need more
memory soon. So, generally, people will often switch to protected mode at the end of their bootloader. Some people are 
weird, (like one of the youtubers I was following, Nanobyte), and they instead first implement a FAT16 file system, but I
did not learn that yet, and I am not very far into the dinosaur book or comet book to really know about file allocation 
table yet, so I didn't touch it.
Switching to protected mode is actually very easy, even though my code might seem to complicate it. One of the registers,
CR0, has a PE (protection enabled) bit that, on boot, is set to 0. All we have to do, to switch to protected mode, is flip 
that bit to 1, and boom! Now we're in protected mode and we're coding in 32 bits with a LOT more memory. Of course, now that 
we're in protected mode, there are also some things we've lost. In day 1, we were able to print "Hello World!" through the 
use of BIOS interrupts. The instruction "INT 0x10" allowed us to signal to the BIOS that we wanted to display to video. Now 
that we're in protected mode, we no longer have such a luxury, but in return, we do have the ability to abandon assembly 
code altogether and, eventually, switch over to C, or C++, or Python, or whatever you wanna use, (I'm gonna use C because I 
also need to learn that language). The actual switching over to C is handled by our kernel, because we can do whatever we 
want in the kernel, basically. Our bootloader is limited to 512 bytes, which, once we implement the FAT16 file system, we 
will soon realize is not a lot.

2. Global Descriptor Table
This is the 2nd thing I did, and was one of the most confusing things ever, because it surprisingly is not super well 
documented (in my opinion). In order to put it very simply, and how I understand it, the GDT works like the following:
Each segment register (DS, ES, SS, CS) have two parts to them. One is the visible part, which you can access and play around
with with the MOV instruction in x86, and it also contains the "segment selector". The other is the hidden part, which 
contains what is known as the "segment descriptor". Whenever a new segment selector is loaded onto the visible part, the CPU
automatically copies the corresponding segment descriptor from the GDT (or LDT) and puts it into the hidden part. So, what 
is contained in this hidden part? Well, the hidden, segment descriptor, contains the description for the segment, such as:
How long is the segment?
What are the access rights for the segment? (Kernel only or can the user also access them?).
Where does this memory segment start?
etc.
This method all falls under memory segmentation which, supposedly, is an obsolete method of memory storage that has now been 
replaced by paging. But I CURRENTLY do not know what paging is, as it is irrelevant to me as of right now. Eventually, I 
will find out about it. 

3. Kernel 
I haven't written the kernel as of this writing, (hopefully I will have a very simple one that just says 'Hello World from 
Kernel!' by the end of today), however, as stated previously, we eventually want to switch to the Kernel because the kernel
allows us to do a lot more and code in C. 

First things first is establishing where the Kernel should be in memory. Generally, people say to put the Kernel at 0x100000 
from the beginning of the memory, and the reason for that is because from 0x0 to 0x500 is occuped. Then, there is 512 bytes 
reserved for the bootloader in the next 638 KB, (0x500 to 0x9fc00), and finally, from 0x9fc00 to 0x100000 is occupied too.
After that, it is all free space for our taking. So, we decide to put the kernel there.

But how do we actually load the kernel? Well, to do that, we can use a form of addressing to do a huge jump from somewhere 
in the bootloader all the way to 0x100000. The easiest form of addressing to do that is CHS, (Cylinder-Head-Sector). CHS 
addressing follows the way we read the HDD, or hard drive. See, the HDD is made up of a bunch of disks stacked on top of 
each other. To better explain this, I will be using polar coordinates. Imagine the cylinder being controlled by your r, 
(radius), or how far away you are from the center. The sectors are the thetha, or like a slice of pizza from a pizza pie. 
The head is what disk we are currently on, (there are usually multiple disks stacked on each other in a modern HDD). The 
bootloader begins in 
CYLINDER: 0, HEAD: 0, SECTOR: 1. 
Most tutorials told me, and will likely tell you, to just stay in the same cylinder and head, (CYLINDER: 0 and HEAD: 0), but
just move to SECTOR 2, because that is the closest available sector where we can store the kernel binary, (or the second 
bootloader if you want one), onto.

DAY 5
----------------------------------
Day 5 was a bit uneventful. I was really tired having worked on this project all day everyday so I didn't do a whole lot and
kind of just took a break. I did do two very important things, though
1. I fixed the kernel loading up! Now it actually says "Hello World from Kernel!" from the correct memory address, (0x100000)
I will now explain a bit about how this stuff works. Volatile memory, like the main memory (or RAM) is very, very easy and 
quick and easy for the CPU to access. Hence, the CPU requires that any instructions, code, etc. is to be loaded onto the 
main memory first before it is executed. Hence also why we load the kernel onto the memory, so that one, we can execute it 
and run it, but two, so that the CPU has quick and easy access to the kernel. So, you may ask, "why even have stuff in the 
disk in the first place if its so great for the CPU?" Well, for two reasons. The first reason is because it is volative, 
meaning that
once we shut down the PC, everything in the RAM dissapears. This is a big issue, and it means that we will have to restart 
everything everytime we open the system. But wait, that might bring up a question, "how does my system store my files and 
whatnot?". It does that by saving it to the disk. The disk is non-volatile, meaning it can be saved, AND it can store a LOT 
MORE information. But it has the issue where it requires more effort and interrupts from the CPU to access, and hence, is
much less efficient. 
So, "for the CPU to process data from disk, those data must first be transferred to main memory by CPU-generated I/O calls.
In the same way, instructions must be in memory for the CPU to execute them.", (from dinosaur book, 10th edition). 
However, there is a bit of a caveat. Since the bootloader begins in real mode, (16-bit), we cannot immediately load 
the kernel into where we want, (0x100000), because of the fact that 16-bit addressing simply cannot go that high 
(even with the segmentation technique I talked about earlier, the max it can go to is 0x10000). SO, to get around this, 
I made a second stage bootloader that the first bootloader jumps to once it enters protected mode, and then in the 
second stage bootloader, since we're in 32 bits now, we are able to move the contents in 0x10000 (where the kernel 
temporarily is) to 0x100000, and then jump to 0x100000.

2. I've finally entered C! 
I still need to learn more about this process, and how link loaders exactly work, as I mostly did this step through 
tutorials.
On a later day, (probably tomorrow, as I am reserving tomorrow for catch-up-study-day, I will come back and explain 
everything
in much greater detail, and probably even fix some mistakes in my explanations). But yeah! We've finally entered C, and this
is also where, unfortunately, most tutorials end. As all that is left to do now is dive into the more "complex" stuff, and
also design the kernel to OUR fitting. It's a bit worrying, but again, I will try whatever I can. If it doesn't work, then 
I'll try to make it later. That's all for today.

DAY 6
----------------------------------
Day 6 was quite uneventful. I didn't do much. However, I did start learning about memory paging, and realized: Oh shoot, I 
should've probably maybe enabled it.

Though I don't fully understand it, I do get its basics. Memory paging is basically a form of memory allocation, in which we
create a virtual address for each program and make it believe that it has the full range from 0x0 to 0xFFFFFFFF. Of course,
it doesn't actually have the full range. That's where we come in, and we can create and use a paging table to then say "ok,
program 1 is page 0. Page 0 is supposed to be mapped to 0x200000 in the phyiscal address". Thus, we get to control the way 
memory is allocated for each program. Now, why is this better than memory segmentation? I don't know, I still need to study
that. My guess is because each page is going to be the same in its size, whilst the coding and data segments are not equal 
in memory segmentation. I don't fully know, I still need to figure that out. 
Either way, today was boring. I didn't do anything because I was feeling a bit burnt out, and instead focussed on myself 
(played games, went to the gym pet cooper)


DAY 7
----------------------------------
Day 7 I finally got around towards figuring out what I wanna do next. One of my biggest worries with this project is that
I'll hit a wall and not know what to do next. I hope that doesn't happen. Today, though, I figured "why not create a print
function?" To do this, I mostly followed nanobytes tutorial that he had, but I differed from it a little bit. Nanobyte has
paging enabled AND is still in 16 bit real mode. I have neither of those things, so I had to figure out how to make the code
myself. Though it is incomplete, and probably VERY inefficient (I am literally calling stdio.c into kernel.c, which is NOT
good), it works! I've managed to print "Hello World from C!" onto the window. The way it works is abusing the stack. In my 
kernel.s we initialized a stack. Now, before I get into anything, I should preface I am using the GCC compiler. The GCC
compiler, by default, uses a calling convention known as CDECL. The way you save stuff in CDECL is irrelevant (at least for 
now). What's important is how it deals with the stack. When you call a function with CDECL enabled, and that function has 
parameters, the CPU will then first push the arguments into the stack from right to left. For example, say I have:

foo(x, y, z). Then, with CDECL calling convention, we will first push z, then y, then x into the stack. After all the 
arguments are pushed in, then we push in the return address into the stack. Then, the ESP register (the stack pointer 
register), will point to the "top" of the stack, which just so happens to be the return address. If we want to access the
arguments sent to us in the function we just called, we can then manipulate the ESP by adding or subtracting values. Since 
each "thing" in the stack takes up 4 bytes, if we add 4 to the ESP, we get x. Add 4 again, we get y, add 4 another time, and
we get z. This is how we can access arguments sent to us in a function. When all is said is done, we can then simply RET,
which will assume that the ESP still points to the return address (which it should), and then it'll return to the address
specified on the stack. Using this knowledge, I managed to print stuff onto the video! Woohoo!

Oh, I should specify how we print stuff onto video. We do this through VGA text mode. VGA has its own segment in the memory,
and that segment lives at address 0xB8000 and continues on for a while. The way it works is that each 0xB8000 stores the 
character, and 0xB8001 stores the background color, or other stuff, of the character at 0xB8000. By the way, 0xB8000 is the 
top left corner, and adding 2 increases the column to the next character position. So, if we store the register EAX with the
character, then that means AL (bottom 8 bits of EAX) has the character, and we just have to change AH (top 8 bits of EAX) to
be the background color and stuff (and we set it to 0x0F, which means WHITE ON BLACK). This allows us to print ONE CHARACTER.
But wait, what if we want to print MORE characters? Well, to do that, we should move to C. We can define a header that has 
C's putc and puts functions. Then, we can define a C file that uses the header, and then actually implements our putc and 
puts functions. Putc is easy, since all we have to do to display a character is just, well, put in a character into our
assembly code. Puts is a bit hard, but, since we know strings are simply just an array of characters, we can just send each
character one by one into the assembly code, and voila! We are printing strings! Woohoo!
Now all that's left is to include the c files and headers into the main kernel function, use puts("Hello World!") and now
we have an actually printing function! 

Tomorrow, I will enable paging and destroy everything. I will also try to make an actual printf() function in our main
kernel.c, so we can actually start printing for real in C, instead of fake printing in C.
----------------------------------
DAY 8 & 9
I got sick on these days, so I wasn't able to work on the project much. :(
----------------------------------
DAY 10
On Day 7 I decided I wanted to turn on paging.
I decided to NOT do that just yet, as I figured out there were some issues with my puts and putc functions. First, if I 
had a long enough message, that message would not wrap around. There was no overflow management happening, so I needed to fix
that. But by far the most GLARING issue was the fact that if I had multiple puts and putc functions, puts and putc would 
magically stop working. Even MORE confusing was that if I had another function in kernel.c that I wasn't even using, 
(like it was just EXISTING in kernel.c), puts and putc would ALSO stop working. Immediately I knew this had to be an issue
between two-three culprits.
The first culprit was that I assumed something incorrectly. Perhaps I used a register that had an implicit rule, or
I was just being unoptimized and it was messing something up. For this, I decided to clean up my x86_video_whatever it was
called function, and actually use the BP register, which is what a lot of people do whenever they're messing around with the
stack.
Speaking of the stack, this brings us to the second culprit. The stack. I assumed that, maybe, I was just being stupid and
I had actually not properly set up the stack, or I just forgot how the cdecl calling convention worked??? I dunno, but
I DID end up finding that I wasn't actually using the stack at all, and that my code just so happened to work. Originally I
was using the linker script 'ld' with the flag --oformat to directly create the .bin file by linking all the C and assembly
files. This turned out to be a BAD IDEA, because for some reason, "--oformat binary" sets the size of the .bss section to 
be 0, or just drops the .bss section. This is because the .bss section is not treated the same as .data or .text, as it 
is considered "unallocated memory", and has type SHT_NOBITS, (at least I think). A section of type SHT_NOBITS may be in 
the file, but occupies no space in the file, so we have to force it to see the .bss section. The linker ld uses the 
objcopy through the BFD library. The objcopy is the real culprit, as it does not include anything of type SHT_NOBITS to
copy over. So we convert it to an ELF type first, as ELF preserves the full memory layout, THEN we use objcopy to convert
to binary, (it wont think .bss is empty this time, because the ELF file contains it already). Also, it's a good idea to
use this anyway since, according to the OSDev wiki, generally binary files are "a flat binary with no formatting at all."
which is not great. (At least, I would assume it's not great, since we would like it to format to elf-i386 32-bit)
And finally, the third culprit. I don't understand the third culprit yet, and so I will not talk about it yet.
It is late, and I still have some glaring issues with puts and putc that I need to iron out before I can make printf and whatnot. 
The biggest issue has to do with reaching the edges. The window is 80x25. So, if I have two puts functions that are 40 characters each then,
you would think, I would reach the limit. But no, instead, it breaks, and nothing is displayed instead. Oh but wait, if I just have ONE puts
function with 80 characters, then it works just fine! But if I try to add clamps, then that one puts function fails too! What is even going on??
I'm going to try fixing this tomorrow, hopefully.

----------------------------------
DAY 11
Alright so it turns out none of the culrpits I talked about previously were the issue,  (well, I mean the .bss --oformat binary stuff was 
certainly an issue that I'm glad I fixed). Instead, the issue was that in my kernel file, I was specifying it with:
SECTION _text
instead of
SECTION .text
And my link loader doesn't know what _text is, as it only knows about .text, leading to the huge issues. That one character fix seemingly fixed 
everything, (or at least I am hoping it fixed everything. I can't seem to find any bugs at the moment..).
OK. New bug was discovered while I was implementing printf. The more I implemented printf, the more and more my strings were truncating. I knew 
that the issue HAD to be memory allocation, but I couldn't for the life of me figure out what the issue was. Eventually, while looking through
the boot code, and genuinely feeling like giving up and restarting this project from scratch and actually following nanobytes tutorial 100%, 
I found out in my boot2.s I was only copying the first 512 bytes of code over to 0x100000. That is nothing, so I changed it to 4096 and 
Everything started working again. 

Furthermore, today I implemented the functionality for printf. The OSDev wiki had a page on it, so I assumed I needed to do it. Also it is
quite nice to be able to actually print things stored in variables, so why not, right? Anyway. The implementation is only really half complete.
I followed Nanobyte's tutorial, but he doesn't make use of the va_list, (even though OSDev wiki tells me to), and he's in 16-bit real mode still
so I have to change some stuff. Unfortunately, though, the stuff I changed ended up not being great ideas and now the code doesn't function. 
For one, the "continue" I have doesn't increment fmt, so in some cases, (in literally evey case), I end up being stuck in an infinite loop in
printf. Also, the number to ASCII thing definitely does not work (my emulator keeps flickering). But oh well, this is all stuff I will do for
tomorrow.
Oh yes, as for how the implementation works. On paper, it is simple. We observe printf through a sort of finite state machine. Since strings are
just arrays, we will increment through the string starting from the far left and moving rightward towards the sentinel at the end. We begin at
state NORMAL. If we discover there exists no "%", and we are in no other state, then we simply print whatever character we are currently at
and move on. If we discover a '%', then we move to a state "LENGTH". Here, we once again increment and check the next character after the '%'.
If this next character is 'h', or 'l', that means we are thinking about short or long respectively. So we increment once again to see the thing
AFTER 'h' or 'l'. If there is another 'h' or 'l', that means we're in SHORT SHORT or LONG LONG. You can't be SHORT SHORT SHORT or LONG LONG LONG
so that means the next thing that comes must mean we are printing something out, so we move to the PRINT_STATE state, where we have a bunch of
cases for various commands, like %d, %i, %x, %o, etc. 
%d implies that we have to be printing numbers. Now, we cannot just prints numbers by themselves, as they are not characters, they are ints, and
ints are usually 32 bits (4 bytes), and chars are 8 bits (1 byte). So, we have to somehow make 32 bits into 8 bits and keep the same display 
value. How would we do that? Through the power of ASCII tables. ASCII tables allow us to take integers and print them as characters. So, we must
do that through a special function. That special function is printf_number, which takes a value and prints it into an ASCII value based on a 
specific base, (base 10, 16, or 8 depending on % code). It uses long division to repeatedly divide the numbers by the base to extract digits,
and then stores the corresponding digits in reverse order. Then we can just print it in reverse order and get the ASCII value for the number. 
If you want a better-ish explanation, I would recommend watching Nanobyte's printf implementation video.
Now back to printf. After it has processed this one character, it finishes, resets the state and length back to NORMAL, and then moves onto the
next letter.
Oh, by the way, our printf is technically wrong. The header is supposed to return an int according to the C++ documentation, but I don't care. 
This is my printf. So I made it return nothing, but the parameters are the same! (const char* fmt, ...). 
Anyway, tomorrow I'll fix printf and I'll do some more studying and whatnot. Then hopefully this week I can turn on memory paging to protect
my memory.

----------------------------------
DAY 12
Today was a bit of a nothing burger day in terms of new stuff. It was really just more debugging and studying. As of right now, I believe I have
EVERYTHING in my program working smoothly. Printf actually prints everything I want it to print, (including long long hexadecimals, which is
something I almost never see, but why not). I've also updated my x86_div64_32 algorithm, after realizing that my original algorithm was trying to
divide a 64 bit number by the base (base 10, 16, or 8), and then store it. This is stupid, and obviously won't work. So I went back to Nanobyte's
video, and I discovered a new "tutorial" channel, Queso Fuego, whom also talked about the "long division method" where they take the first 32
bits, divide them by the base, store the quotient and remainder, then take the last 32 bits, append the remainder, and divide it by the base, 
before storing everything once more. Adding the two will give you the actual division, just like if you were to try and divide the 64 bits in 
one go. This worked much better, (duh), and actually allowed me to use %llx, as before I was unable to use them with my implementation. 

As for studying, I just read more of the intel manual, OSDev wiki, and the dinosaur book (specifically chapter 2), to get a better understanding
of where I want to go next. I think the next plan of action is the following
1. More studying. I really need to read more of the dinosaur book, it is a very good book and teaches the concepts quite well. I want to have read
a large chunk of it by the time I'm done with v1.0 of this project.
2. Set up memory paging. I've been talking about doing this for a while, but I think I've held it off for long enough and I need to do it soon.
3. Set up TSS w/ ESP0 stack. I don't fully know what these are yet, but the OSDev wiki recommends them
3. Set up IDT. The IDT is not only necessary for entering user mode (ring 3), but also will be quite helpful in the future for debugging purposes.
4. Enter user mode. I'm quite excited for this, as there is a lot to do after I enter user mode still. 
5. This part I need to study some more, but I believe I need to:
  - implement various user programs (a simple clock, maybe)
  - implement system calls for said user programs (a system call interface)
  - set up a timer 
  - set up scheduling (so I can multitask and not force the user to have one thing at a time. Timer is also necessary for this, paging too)
After that, I should probably implement a file system, drivers, and other shenanigans that I need to study up on. (FAT32?)

----------------------------------
DAY 13
Today was very confusing, and that is why I will be dedicating tomorrow towards a study day. Perhaps even the rest of the week, I will dedicate
to being a study-remainder-week. I need to catch up more on the x86 ASM language, as I believe I am starting to reach the point where I'm not
fulling understanding the assembly code the tutorials are giving me.
The things I did understand, though, is the newly updated putc and whatnot with proper cursor handling. Beforehand I was relying on the assembly
code and then sending in a global variable, cursor_pos. This worked, but it didn't literally update the cursor position. Now, I use a volatile
8-bit memory pointer that uses 0xB8000 as a pointer itself (bleh, jumbo of words). As I've explained before, the VGA memory address works like
the following: 0xB8000 stores a character, and 0xB8001 stores the attributes of said character, (like background color, font-color, etc.). So,
by having a buffer integer pointer that points to an 8-bit integer, we are basically saying that the variable uint8_t* g_ScreenBuffer points to
the address 0xB8000, which we are able to then print directly to, and then we can move onto the next character by playing around with the pointer.

I also started working on setting up the IDT. The IDT is, of course, for interrupts. I believe I've mentioned in this dev-log that, once we enter
protected mode, we lose the ability to use interrupts (like INT 0x10, INT 0x13, both of which we used quite extensively. 0x10 is for video memory,
0x13 combined with AH 0x02 is for reading from a disk. Fun fact: INT 0x13 AH 0x03 is for WRITING to a disk, so that's fun). I never actually
explained WHY we lose the ability to use interrupts. That's because in real mode, to be able to use interrupts, we had the IVT (interrupt vector
table) already available to us, and so we could use its many functions. But, in protected mode, the CPU now requires the implementation of the
IDT, (interrupt descriptor table) for us to not only be able to use some interrupts (like being able to get keyboard input. This will be VERY 
important once we get into user mode), but also for the hardware to give us some interrupts (divide by 0 and whatnot). Now I still don't FULLY
understand the implementation of the IDT, so I will get back on this devlog tomorrow when I have more understanding, but to my knowledge, the
IDT is very similar to the GDT, in that when an interrupt is issued to the CPU, it will look to the GDT to figure out what ISR to call, (ISRs are
interrupt service routines, they will handle interrupts, so we would have an ISR for divide by 0, for example, that would process, and then return
function back to the CPU to continue reading memory). When an interrupt is handled, the CPU, of course, needs to stop what it is doing. But it
can't just DROP what it is doing, that would be very inconvenient, imagine having to download something, and you accidentally press a key, making
the download process just stop. So, the CPU needs a way to save the information of what it does, I believe it does this through the IDT too,
in particular, if an interrupt occurs on kernel mode, (which is good), then all it has to do is push the error code, if there is one, onto the
stack, the CS register, the ES register, and the EFLAGS, (if the interrupt is from user mode, it has to switch to kernel mode and also push a
lot more stuff onto the stack). The Intel manual is more specific, so I'd recommend checking it out (go to figure 6.7, it explains it quite well).
The IDT also has some other stuff, like the interrupt gate and trap gate. I, again, don't fully understand the IDT just yet, but to my 
understanding, and according to the OSDev wiki, the trap gate sucks and is useless, so don't implement it. Just stick to the 32-bit interrupt
gate. We use the interrupt gate because it clears the IE bit (interrupt enabled) in the EFLAGS register, so if one interrupt is being processed,
more interrupts cannot come and mess up the process. The trap gate doesn't do that. Furthermore, the severity of an exception/interrupt is 
organized from most severe (0) to least severe (256). There's a big ol' table on the Intel manual that gives us the list of interrupts, 
(table 6.1 in the manual). 
Furthermore, I started to implement the PIC. My thought process, as of right now, is to follow what the OSDev wiki has told me, which is, to
be able to implement and code in interrupts into your OS in 32-bit protected mode, we need to do the following: 
- Make space for the interrupt descriptor table
- Tell the CPU where that space is (see GDT Tutorial: lidt works the very same way as lgdt)
- Tell the PIC that you no longer want to use the BIOS defaults (see Programming the PIC chips)
- Write a couple of ISR handlers (see Interrupt Service Routines) for both IRQs and exceptions
- Put the addresses of the ISR handlers in the appropriate descriptors (in Interrupt Descriptor Table)
- Enable all supported interrupts in the IRQ mask (of the PIC)
Currently, I believe I've done steps 1, 2, and 3. BUT, before moving onto implementing ISR handlers, I have come across a bug that I believe has
to do with memory (I don't have any memory management techniques as of right now, so it makes sense for these issues to arise). I believe the 
issues are arising because I'm stupid and I don't understand the things I've implemented, so I will take a step back and try to understand
EVERYTHING I've implemented, and play around with it to see how they work. And genuinely this time. Yesterday was supposed to be a study day, 
but I ended up debugging instead. I will try not to debug tomorrow, instead, I'll play around with stuff and try to understand it. Also I've been
using a lot of tutorial code. This is not good. I will also try to change and make stuff mine.

----------------------------------
DAY 14

Today was a study day, but I also decided to do some debugging. Thankfully, the debugging was easy, because it was something I had seen before.
If I had too many functions, or if I had too many kprintf() function calls, or just too many lines, something would stop functioning, 
generally my kprintf(). So, I knew the issue had to be memory related. Low and behold, after like 10 minutes of figuring out what the issue was,
I figured out that I was copying only 8 sectors from the disk to the memory in my boot.s, when my kernel was taking up 9 sectors. SO, I just 
changed it to 9 sectors, and everything works now. I really need to make a dynamic memory allocator for that.

As for the IDT and PIC, as those were the main things I studied today, I think I finally understand them. Interrupts 0 - 31 are occupied by the
CPU, and are directly thrown by the CPU itself, so our PIC doesn't actually manage it. PIC is a piece of circuitry that ENABLES interrupt-based 
I/O to occur, otherwise, we would be forced to use polling. Basically, what happens is that the I/O has some lines connected to it and the various
external hardware, like the mouse, keyboard, etc.. A PIC usually has 8 IRQ lines, (IRQ 0 thru 7), and each line corresponds to a different 
external hardware sending an interrupt signal. Now, when the PIC recieves the signal it does some things to it. First, the PIC_DATA port contains
an 8-bit binary that is basically a mask for the various IRQ lines. (So, it'd be like 00000000 on default). If we don't want keyboard interrupts
to interrupt our CPU, we can mask the IRQ line that the keyboard hardware is associated with (for example, lets say keyboard is associated with
IRQ line 7, I don't think it actually is, but I'm too lazy to check right now). Then, we flip the 7th bit to 1 and we get: 10000000. So, if
the keyboard ever sends a signal through IRQ_LINE_7 now, the PIC will see that the PIC_DATA port's 8-bit binary mask has the 7th bit flipped
to 1, and so we now ignore any signal coming from IRQ Line 7, or the keyboard. That's masking. The priority resolver, which is the next thing
the PIC does, sees how to assign the interrupts if MULTIPLE IRQ lines have sent signals. The general scheme is the following: the timer is the
most important, followed by keyboard, followed by the cascade to slave, etc.. That's the priority resolver. Finally, the ISR is a register that
basically is the security guard at a club. It checks to see if an interrupt is being processed (i.e., has the CPU sent an EOI signal back to
the PIC yet?), and if it is being processed, then it tells the interrupts in line to wait. Once the CPU sends an EOI signal back, then we can send
the next interrupt to the CPU. The transfering of the interrupt to the CPU is controlled by a variety of things, like the data bus buffer, and
the read/write logic thing, and that's all kind of un-important to really understand the PIC right now. There is another thing, and it has to do
with the existence of the slave PIC's. You see, I said the PIC's have 8 IRQ lines. That's not a lot, and the IDT, supposedly, can manage 256 
descriptors for different interrupts. If the first 31 are to be controlled by the CPU, then the interrupts 32-255 (external hardware interrupts)
surely can not be managed by 8 IRQ line alone, correct? Correct. They can't. That's why we can have up to 8 master-slave PIC's, but for now, 
I only have one other PIC, or slave PIC, allowing us to have 15 IRQs. Yes, 15. Not 16. That's because the master and slave PICs still have to
communicate with each other, so IRQ2 is used to communicate with the slave PIC. If the slave PIC recieves an interrupt (say we get an interrupt
on IRQ 14 or something, clearly out of the master PIC's range), then the slave PIC can send the signal, after it has done all the masking and 
whatnot in its OWN registers, to the master pic through IRQ2, where the master PIC can then send it to the CPU. So, basically, the way it works
as of right now, with the slave pic included is:

One of the PICs recieve a signal from external hardware via the IRQ lines (0 - 7 for master, 8 - 15 for slave). 
The PICs manage said signal, and see if it should even be sent. 
PICs check to see if the CPU has sent an EOI, (is it processing an interrupt right now?)
If not, then the slave PIC will send the INT to the master PIC through IRQ2, which will then send the INT to the CPU
the CPU, if it allows, will send back an INTA signal, which is just the CPU acknowledging the interrupt, and asking for the interrupt number, or
vector, or whatever
If the master PIC found the interrupt, it will then calculate the interrupt number based off of the IDT:
(interrupt number = IRQ_Line + base_offset)
And then it'll send it to the CPU.
If the slave PIC found the interrupt, the master PIC will instruct the slave PIC to calculate the interrupt number, again based off of the IDT,
and then it'll send it back to the master PIC, who will send it off to the CPU.

I believe that is basically how these interrupts work.
Oh, I noted that interrupt number = IRQ_Line + base_offset.
The IRQ_Lines are from 0 - 7 and 8 - 15, and base_offset is generally going to be 0x20 and above, because anything lower is being used the IVT,
or the CPU itself reserves those offsets, (or at least, I placed it as 0x20. The OSDev wiki doesn't specify, but I used 0x20, because the
IDT states that the external hardware interrupt starts from 0x20 and goes onward, so we should probably specify it to that for the vector offset).

That's basically how the PIC works. As for the IDT, we haven't implemented any ISR's yet, nor are we in user mode, so it isn't a huge issue,
but there is the issue regarding how the CPU should switch into an ISR. Generally, there are two types of interrupts, precise and imprecise 
interrupts. I will be really only dealing with precise interrupts, because imprecise interrupts seem impossible to manage. Basically,
precise interrupts assume and require that every instruction up to the instruction the CPU is currently at has been 100% executed, and every
instruction AFTER the instruction the CPU is currently at hasn't even been touched. This allows for easy flow between the CPU doing the actual
process and handling the interrupt. Of course, Tanenbaum also says there are some issues with this, as there is no guarantee, and also you CAN
have some interrupts be imprecise, (like a divide by zero interrupt can be imprecise, or a trap gate interrupt can be imprecise, because you
aren't really gonna be going back to the process after these interrupts), so I'll probably take that into heart when developing the ISRs. Also,
I've grossly simplified the PIC, or at least somewhat simplified it, because there are actually a lot more registers it has at play, but the 
basic concept of the PIC I think I've managed to explain quite well. For more information, I would highly recommend checking this out:
https://pdos.csail.mit.edu/6.828/2014/readings/hardware/8259A.pdf
Very good PDF that explains the PIC and its inner mechanisms. 
That's all for today. The weekends I have decided to make into rest days, but I am quite restless about this project, so I'll probably work on
something anyway, or at least I'll study.

----------------------------------
DAY 15 & 16

As stated above, I've made the weekends into rest days. I didn't do anything these days except hang out with friends and stuff

----------------------------------
DAY 17

Today was an ok day. Somehow, due to the fact that I hadn't touched the code for two days, it felt as though I was re-learning everything,
(not really). 
Today, though, I did learn some new stuff. For one, I have a better understanding of the IDT and PIC code, largely thanks to the studying I did
on Friday, and also largely thanks to the fact that I had started writing ISRs today! I have created a stub for ALL 256 interrupts available, 
though, I have disabled the external hardware interrupts (the ones that come from the PIC via IRQ lines), due to the fact that I haven't
implemented any of them at all, and they all require the use of the EOI method I've made to signal to the PIC that we're done processing that
interrupt.
However, I DID start on implementing the keyboard handler! We now are able to actually interract with the operating system and type stuff!
Of course, the stuff we type isn't actually worth anything, as we're just typing the ASCII representation of the keys we're inputting. I believe
now would be a good time to explain keyboard interrupts, along with more information regarding interrupts. 
Firstly, I will begin with interrupts. I've already explained a great deal about them with my day 14 explanation on PIC, but I will explain WHY
we use interrupts. 
Interrupts is NECESSARY for a CPU to work to its fullest potential. Without them, our OS would be moving at snails pace. Why is that? Because, 
well, take a very basic example for now: typing "Hello World"
Say, for example, you are a super fast typer, and you can type at an astonishing 300 WPM. While to you, and to literally every living species, 
300 WPM might seem fast, to the computer that is operating at microsecond speeds, 300 WPM is a snails pace. So, say we weren't using the interrupt
method, and instead, we were using the polling method. There would be quite a lot of issues. The polling method is relatively simple to implement,
and is actually what, (at least I think), quite a few real mode (16-bit) OS's use for user I/O. Basically, if we ever want the user to type 
something, instead of using interrupts, we can have the CPU drop everything it's doing and constantly nag the keyboard hardware. We keep asking
the keyboard, and its data ports, "hey, is there anything here yet?" over and over, every CPU cycle, until the keyboard eventually says "yes,
there is something in my data port now". Then, we ask the keyboard, "Well, can I take the stuff out of the data port?". We do this because,
although there is stuff in the data port, it isn't necessarily ours for taking immediately. We have to ask keyboard for permission to take it.
Eventually, the keyboard says "yeah, ok, take it now". Then we can take the letter, process it, display it, do whatever we want with it, 
throw it away, then go back to asking the keyboard for the next letter. In theory, this SOUNDS ok, because you might imagine a scenario where the
user is asked to put in a command. Then, you would imagine that the CPU would want to have its attention focused towards you, right? Well, not
exactly. The issue with polling is that it restricts the CPU to being able to do nothing except wait for the user to type at what feels like
a snails pace. Imagine being the Flash in speed-time and having to watch a snail crawl a kilometer. That's probably what the CPU would feel like.
See, we NEVER want the CPU to be doing nothing, because the CPU can ALWAYS be doing something, like other processes. That's where interrupts come
in. Instead of having the CPU sit and wait for the user to type stuff, we can have the CPU instead go and do other stuff while it's waiting. THEN,
when the keyboard is ready (which is signaled to the CPU via the PIC/APIC), the CPU has two options: either 1, it says "hold on a second, I am
doing a HIGHER PRIORITY task/doing a different interrupt", OR, it can say "alright, give me the interrupt". This can save immeasurable time,
and it allows for the CPU to effectively multitask multiple things, and that is why it is so desired.
Next up, the keyboard handler. I'm too lazy to explain it. So, instead, I will link to a very good website I found that explained it quite nicely:
https://www.basicinputoutput.com/2024/11/the-keyboard-controller-interface.html

----------------------------------
DAY 18
Today was a more productive day than yesterday. I managed to set up the keyboard handler to actually display the characters the user is typing, AND I've managed to
make it so the user can use stuff like caps lock, shift characters, etc. AND, I've managed to make it so I can use the stuff the user types (this will be very useful for
when I switch to user mode and create system calls/programs). 
The way I've managed to do this is because, again, how the keyboard handler works. I was lazy and didn't explain it yesterday, so I'll explain it a little bit today, or
at least, I'll explain the parts that are important to us. First and foremost, whenever the user presses a key, there is an external piece of hardware on the computer 
that puts in a SCANCODE into the port 0x60. Notice how I said scancode and not the hex code, or ASCII code of the characters. The scan code is... Weird.. Thankfully, scan
codes are similar to hex and ASCII codes, where each character corresponds to its own unique character. So "A" = 0x1E. To see all the scan codes and their mapping, go
to the OSDev wiki: https://wiki.osdev.org/PS/2_Keyboard. Anyway, to fix this, all we need is an array that maps the scan codes to their respective keys, (which is exactly
what we do). The way that I do it might not be the most elegant or nicest looking, but I don't care. It works. As for actually handling the inputs, it's pretty simple. 
We simply define an input buffer array that accepts a certain number of characters (I used 256, it doesn't really matter though). We also create various boolean values, 
particularly the "input_ready" boolean value, that basically tells us that the user has created a message for us to use. We will specify that the message is only available
to be used IF the user has pressed the "enter" key. Since a string of characters, when it is read, must have the \0 at the end of it as a sentinel, whenever the user
inputs a new character, or deletes a character, we will ALWAYS have the current index pointer for the array be pointing to the \0 sentinel at the end of the string.
Example:             this is where our pointer is 
                                  V
"H, e, l, l, o, , W, o, r, l, d, \0, (NULL), (NULL), (NULL), ..."

If we add another character, we simply override the \0 to be the newly pressed character, then we increment the pointer (so it's pointing to nothing), and then place
a \0 there. We do a similar thing but in reverse for whenever the user presses backspace or wants to delete a character. We decrement the pointer, and then override 
whatever character was there with \0. 

Now that's the keyboard stuff. That is IRQ1. As I've stated before, there are multiple IRQ lines, and one of the most IMPORTANT IRQ lines is IRQ0. I have yet to fully
implement IRQ0, and I have yet to finally understand the external thing connected to IRQ0, but I will explain everything I understand right now. 
IRQ0 is connected the PIT. The PIT, though, isn't only connected to IRQ0. The PIT, (known as the Programmable Interval Timer), is simply a timer. It has multiple uses,
primarily for the system to keep track of the time, enable multi-tasking/scheduling (whatever you wanna call it), and to stop one process from taking too much time/hang
for too long. It's relatively simple in the way it works. It's simply an oscillator, a prescaler and three independent frequency dividers. The oscillator is what creates
the frequency. The PIT is, historically, known to have a default frequency of 1.193182 MHz. The reasoning for this frequency is due to history, but just know, this is
the frequency we're working with. The purpose of a frequency divider is to achieve a slower frequency, and we can do it relatively simple in software by using a counter
that counts down from a tick. If we have a frequency divider of 3, then we basically can have a counter that counts down every tick from the 1.193182 mhz frequency 
until it reaches 0, where it will send a signal to SOMETHING. As we said, the PIT has three independent frequency dividers. These independent frequency dividers are more
commonly known as the PITs channels, and each channel does something different and, thus, we can also specify the program to not only have different frequencies in each
channel, but also execute/do different things in each channel, (duh). "Each PIT channel also has a "gate input" pin which can be used to control whether the input signal
(the 1.193182MHz one) gets to the channel or not. For PIT channels 0 and 1, the associated gate input pin is not connected to anything. The PIT channel 2 gate is
controlled by IO port 0x61, bit 0." (stole this from the OSDev wiki because they explained this really well). Now to explain what each channel is contected to and what
each channel does. 
Channel 0 is directly connected to IRQ0. Therefore, that makes it incredibly good for setting up a system clock. Typically during boot, the BIOS sets channel 0 with a
count of 65535, (which is the frequency divisor), thus having the frequency be 18.2065 Hz (or about 54.9254 ms).
Channel 1 is useless. Literally. It used to be used for refreshing the DRAM (Dynamic Random Access Memory) or RAM (Random Access Memory), but on later machines, the
refreshing of the DRAM and RAM is now done by dedicated hardware, and now channel 1 is utterly useless and sometimes not even allowed to be touched.
Channel 2 is directly connected to the PC Speaker, so the frequency of the output determines the frequency of of the sound produced by the speaker. I won't be explaining
the PC Speaker here, because I haven't read about it yet, but if you want more information, here it is: https://wiki.osdev.org/PC_Speaker. 
Next up are the ports and the stuff accepted by the PIT. Instead of explaining it in my own words, I think the OSDev wiki does an excellent job in this scenario, so
I will just copy and paste it here:
"The PIT chip uses the following I/O ports:
I/O port     Usage
0x40         Channel 0 data port (read/write)
0x41         Channel 1 data port (read/write)
0x42         Channel 2 data port (read/write)
0x43         Mode/Command register (write only, a read is ignored)"

"The Mode/Command register at I/O address 0x43 contains the following:              (AGAIN, THIS IS FOR 0x43)
Bits         Usage
7 and 6      Select channel :
                0 0 = Channel 0
                0 1 = Channel 1
                1 0 = Channel 2
                1 1 = Read-back command (8254 only)
5 and 4      Access mode :
                0 0 = Latch count value command
                0 1 = Access mode: lobyte only
                1 0 = Access mode: hibyte only
                1 1 = Access mode: lobyte/hibyte
3 to 1       Operating mode :
                0 0 0 = Mode 0 (interrupt on terminal count)
                0 0 1 = Mode 1 (hardware re-triggerable one-shot)
                0 1 0 = Mode 2 (rate generator)
                0 1 1 = Mode 3 (square wave generator)
                1 0 0 = Mode 4 (software triggered strobe)
                1 0 1 = Mode 5 (hardware triggered strobe)
                1 1 0 = Mode 2 (rate generator, same as 010b)
                1 1 1 = Mode 3 (square wave generator, same as 011b)
0            BCD/Binary mode: 0 = 16-bit binary, 1 = four-digit BCD"

The stuff you put into a channel port, (0x40, 0x41, or 0x42), the number you put will be the DIVISOR. Basically, if you put 1000, the frequency in the channel, or
the independent frequency divisor will be (1.193182Mhz/divisor).

To create an actual tick/clock thing, I still don't know how it works, lol. I will get back tomorrow to explain how to make an actual clock, I think.

----------------------------------
DAY 19
Today was a bit of a bad day, because I ended up sleeping super late yesterday, waking up super late today, and then thus, not getting a whole lot
of work done. Despite that, though, I did manage to do SOME things today, though I don't feel very accomplished. For one, we finished the IDT. Well, sort of. 
Not really. See, I finished implementing the PIT, and so now, the CPU will be sent an interrupt by the PIT through channel 0 every millisecond or so, (it's a little bit
more than a millisecond, but it's basically a millisecond). For now, this millisecond interruption is utterly useless, and all we do with it is increment a 64 bit 
(long long unsigned) tick counter that starts counting up from 0 as soon as the kernel is loaded into memory, (or almost as soon as the kernel is loaded into memory. 
See, my kernel is very disorganized, and the IRQ0 line is only unmasked until quite a bit later, but who cares! It's basically from as soon as the kernel is loaded
into memory). So, now that the IDT is (basically) finished, (if we ever need more interrupts for the various IRQ lines, or more in-depth interrupt handlers for any other
interrupt, we can always implement them later. It's not as though they are all necessary to have), we can move onto the next, and one of the scariest things about OS
development: memory allocation. I did a LITTLE BIT of studying on memory allocation prior to getting into this, so I know a little bit about how I'm going to do this. 
I will basically be planning it like so:
1. I will implement a physical memory manager. This is the part that I understand, I believe, the 2nd-most, or most about. A physical memory manager is quite important,
as once we get into having processes floating around in our memory, we would like to be able to manage each process, so that we do not have one process's information
overriding the information of another process. Furthermore, memory is SMALL. I believe I talked about this earlier, but RAM is very quick and easy for the CPU to access,
however, the downside is that it is 1. volatile, meaning its contents are lost when the PC shuts down, and 2. it is SMALL. For a 32-bit CPU, (which is what we're dealing 
with), the memory only has 4GB, which, if you have ever looked at your storage, or looked at any application ever, you will realize "oh wow, that is literally nothing". 
Indeed, if you have ever actually looked at your VRAM, and RAM, you will also realize you actually don't have a lot of space in memory, (for example, I have 16GB of RAM,
which is nothing! But I've played huge games that have been over 60+ GB. How do we play with all of these?). The answer to this is with the MMU, or Memory Management Unit.
What is the MMU? Well, as the name implies, it's a component of many computers that handles memory translation, protection, and other stuff related to memory in an 
architecture. What is memory translation and protection? I'm so glad you asked. Memory protection is the physical memory manager, basically. As I stated previously, 
when we have multiple processes on the memory, we don't want one process to override a part of another process when we load it into memory, and thus, we need a way to 
handle allocating an deallocating memory for each process. This is, again, where the physical memory manager comes into play. I have decided to take on the very simple
(although very inefficient) bitmap approach to creating a physical memory manager. Basically, the way that my plan will work is the following:
1. The memory will be divided into 4KB chunks. This is because paging does it the same way. 4KB is nice for a couple of reasons, but before I explain any of those reasons,
I should probably explain the bitmap. The bitmap is simply a very long array in which each index of the array corresponds to a page/block/chunk in the memory. So, for
a very simple example, imagine that the memory is 10 bytes. If we decide to make every page 2 bytes, then we will have a 5-length bitmap array/list, and each index of
the bitmap will correspond to its own unique 2-byte page in the physical address space. Simple, right? There are a couple issues with this, though. First and foremost, 
the size of the pages. Lets say our memory is the standard 32-bit size, 4GB, and we decide to make each page 1 byte. Then, we will have an array of size 4000000000, which
isn't feasible, because, although we can use 32-bit or 64-bit integers to mark the indicies, we still need this bitmap to be on the memory, and our bitmap is size
4 billion, that means our bitmap is the ENTIRE MEMORY, and so we can't actually do this at all, (it isn't feasible). I'm sure you've seen the issue now, if we make our
pages/chunks too small, then our bitmap consumes a large portion of the memory. But, I hear you say, what if we just make it super big? That still has its own issues. 
First, lets say we decide to make the chunks 10 megabytes. Then, our bitmap is super tiny, and doesn't consume a lot of memory. Great! Now, we load in a program that is
a mere 500 bytes into the memory. That 500 bytes goes into the first 10 megabyte chunk and, oh no, would you look at that, the rest of the 9+ megabytes become unusable, 
because the bitmap has 1 bit that corresponds to each chunk. 0 means that nothing is in that chunk, and 1 means something is in that chunk. So, if our chunks are TOO large,
then we end up wasting a lot of memory and have nothing in them. That is why we need a nice middleground, and it is mostly believed the middleground is 4 kilobytes, 
because Linus Trovalds (creator of Linux) used 4 kilobytes for his paging, and so why not, right? You might have looked at my incomplete physical_memory_manager.h and
noticed the other defined thing and asked what that is. That is simply there because generally the physical memory is split into bytes. So, we have that there to say that
one byte will manage 8 different chunks, because 8 bits = 1 byte, and 1 bit manages 1 chunk. Furthermore, as I stated previously, the bit map must exist on the memory.
Of course, that means we want the bitmap to never be touched, because if it ever is touched, that means our PC will likely explode (not really, exaggeration, but it will
be VERY bad). So where would be a good place to put? Well, our kernel is untouched, and we like to put at 0x100000, right? The user space is generally higher up, and sits
at at 0x200000, (or even higher, I haven't really studied the user space yet). So, why not put the bit map as close as we can to the end of memory? This will mean that
we will have to have a LOT of programs running on the memory concurrently for it to actually mess up the bitmap, therefore giving us good leeway. So, to figure out how
much memory we have, we use some fancy schmancy stuff (particularly INT 0x15, EAX 0xE820. For more information, look at the OSDev wiki: 
https://wiki.osdev.org/Detecting_Memory_(x86)), and load it into memory in a safe spot. Then, we can read that information in the kernel, and display it onto the screen
so that we can actually SEE the reserved and available memory. There is also one extra benefit for doing this. Once we implement a genuine physical memory manager, then
we can actually see how many pages exist in our memory, see if it lines up, allocate some new pages/deallocate some pages, and once again see if it lines up. So,
doing this step is just good for debugging (and is why I did it). Anyway, once we have a bitmap, we must simply start the chunking process, create a way to allocate/free
memory chunks, create a way to FIND memory contiguous memory chunks for larger processes (bigger than 4 kilobytes), and then boom! We have a physical memory manager.
Next up is the virtual memory manager. The virtual memory manager is also for paging, (as paging allows us to create virtual memory addresses). Basically, the virtual
address space is not real, but we give each process the illusion that it has access to the full physical address space, when it really doesn't. Then, we can simply 
manage each process, assigning them to a differnet page (or, I guess, frame. It's called pages for virtual memory and frames in physical memory, I believe). I believe
this is how the virtual memory manager works, but admittedly, I haven't studied it yet, so I am mostly spitballing here. After we have a physical and virtual memory manager
I believe we will be safe to turn on paging. We can turn on paging whenever we want, but I decided to make memory managers first. As far as I know, virtual memory only
exists when paging is turned on. It doesn't exist before then, but I may be wrong about this, as, again, I haven't studied this. Anyway, oh yeah, as I was saying 
for my plan:
1. Implement physical memory manager
2. Implement virtual memory manager (will be useless without paging, I THINK?)
3. Turn on paging
And then boom! Our kernel now has memory management! There are other things we can do after we have both memory management AND interrupts (particularly the PIT), like
setting a scheduler, but that is complicated and I want to stay as a single-task/single-thread kernel for now. Later on, if I feel confident, I might make a multi-thread
system. Multi-threads are scary, because they can completely mess up how we manage interrupts (according to Tanenbaum, at least), and how memory might be managed too,
but they have the benefit of being far more efficient. The best way I can explain threads is with this example, (which I HOPE I am getting correct, because if not, then
I will come back later and fix this explanation):

void print_A() {
    while (1) {
        putchar('A');
    }
}

void print_B() {
    while (1) {
        putchar('B');
    }
}

This entire piece of code is a process. But, the process includes two threads, print_A, and print_B. Each thread has its own stack, register set, (duh), thread ID, and PC
and much more. Furthermore, if there exists more threads in a process (which there are), then it will share, with the other thread, its code section, data section, and 
other OS resources. A single thread system will basically say "ok, I see print_A() is first, so I will follow that thread, complete it, then come back and see if there
exists any other threads". But, a multi-thread system, (along with some functions in the threads, like maybe a yield() function), can jump between two threads whilst also
saving the state that each thread originally was in. So, in a multi-thread system, you could feasibly have the code above print something like "ABABAB", (for the sake of
this example, assume the two threads are being called 3 times each), while a single-thread system can only ever print "AAABBB". I hope I got threads right, it is also
a big confusing to me, because a lot of explanations literally just use the definition, "basic unit of CPU utilization", which I think is a stupid definition, because to
a noob like me, it doesn't really make sense. Again, if I am wrong, I will come back and edit my explanation of threads so that future me (and anyone else reading this),
doesn't get confused/get the wrong idea. That is all for today. Tomorrow I will finish the physical memory manager, and hopefully start on the virtual memory manager. 

----------------------------------
DAY 20

Today I did quite a bit, though it might not seem like a lot. Today I started on the development of my memory manager, and I began by working on the physical memory manager
and I've actually (I hope) finished the physical memory manager. The way our physical memory manager works is through a bitmap, (I think I mentioned this yesterday).
While a bitmap has glaring issues, and a lot of people seem to use a buddy-allocator or a stack instead to manage physical memory, I found those to be a lot more
complicated to not only understand, but also implement, so I stuck with the bitmap approach. In fact, my approach is so bad that, even amongst bitmap physical memory
mappers, mine is by far one of the most inefficient, because when looking free chunks, it scans through EVERY bit, instead of trying to do some optimization (like by
scanning 32-bits first, or something. The OSDev wiki talks about it). But I don't care. All I care about is getting it to work first, then I can optimize it to my hearts
content later on, when I feel like it. Anyway, to really understand the physical memory manager, I had to do one other thing before that, and that was to get the 
information regarding the physical memory. There is one really good way to do that, which the OSDev wiki outlines, and it is through INT 0x15, EAX 0xE820. For information
on how this works, I would just suggest looking at the OSDev wiki: https://wiki.osdev.org/Detecting_Memory_(x86), because I don't fully understand it, and I don't really
care (not yet at least). But anyway, all we did with this method is store the information regarding physical memory at a safe address, (I picked 0x9000. The OSDev wiki
uses 0x8000. I can't use that because that's where my second-stage bootlader is). Then, all we have to do is read through 0x9000, and print to the screen the information
regarding the physical memory. INT 0x15, EAX 0xE820 is actually very useful because it also tells us what parts of the memory are reserved or available to us, which
is nice. Anyway, through this I learned that I'm actually temporarily loading my kernel at a reserved section of the memory! This is not good at all, and I should
honestly switch it, but I have yet to encounter a mistake/bug, sooooooo... If it ain't broke, don't fix it. There was actually one bug I was encountering when I was working
on the physical memory manager, and it was a very very odd one. You see, randomly, whenever I tried to run my code via the qemu emulator command, in SOME cases, my code
would run just fine and it'd be all fine and dandy. In other cases, as soon as it finished loading in the physical memory information, it would experience an interrupt
(interrupt number 8). What's even MORE confusing is that it had an error code! Yes, I know that interrupt 8 is supposed to have an error code, but according to OSDev wiki
the error code should be 0. This one was NOT 0. Also, again, it was happening randomly. Eventually, I figured out it was because whenever I initialized my IDT, I was
turning on interrupts and giving interrupts free reign to do whatever they liked before I could actually disable the IRQ interrupts. So, the fix for this was to simply
move the PIC_disable to be the first command run by the kernel, so that even when I do initialize the IDT, nothing funky happens with interrupts just yet until I say
they're allowed to do whatever they like. Also, in more exciting news, the physical memory manager was one of the things I've managed to (mostly) implement myself! I feel
quite proud, and I feel as though I am becoming more and more confident in my ability to code in C. Anyway, enough with the random talk, I should probably talk about my
future plans. So far, here is my gameplan: I begin by calling initialize_pmm(), which will set the entire bitmap to be 1's. This will effectively state, 
"hey, the entire memory is in usage/reserved". But in reality, it is not. Then, in the kernel, based on the available entries in the SMAP (from INT 0x15, EAX E820), 
I will call initialize_memory_region with their base addresses and lengths (all of which is given by the SMAP). Then, I will manually go in and deinitialize some of 
the memory from 0x100000 (a few KB, maybe a full MB), because I want that to be reserved for the kernel. I will also deinitialize some of the memory from 0x7EE0000,
because that is where the bitmap is, and I don't want it to ever override it. Finally comes the creation of malloc, or the memory file in general. My plan for memory is 
that I will have two functions, at least. One function will find the size/length of a process/program/file. How I will find the size, I don't know yet. I'll have to look
at some tutorials, maybe. After that, I will have a memory allocation function, (kmalloc), that will put a given program/file/process into memory based on the base_address
that is going to be provided when I use allocate_blocks(num_blocks). And then, boom! I think that'll be everything. I'll need to make a de-allocation method too, that will
also free memory of the process. Then that should be it.. I should have a genuine physical memory manager then.
Buuuttt, this was all the stuff I typed in my physical_memory_manager.c, which I thought was going to be my plan back then. This is  no longer my plan now, (though it is
still very similar). Tomorrow I was planning on making the memory allocation stuff, but then I decided "hold on, I'm gonna need a virtual memory manager soon anyway, and
the entire reason I made my PMM like this, using 4KB chunks, is because of paging. I should probably enable paging tomorrow and develop the VMM before going into malloc".
Indeed, this is also something the OSDev wiki, and some tutorials/videos I've been watching, agree with it. The OSDev wiki claims that malloc should be one of the last 
things you implement for a memory manager, and so I will do the same. Also, I believe I've come to realize the POINT of a virtual memory manager. See, today, after I
finished implementing the physical memory manager, I decided to then follow my initial plan, which was setting the entire bitmap to be 1's (or reserved). Then, I went
and changed the parts of of the bitmap that memory said were supposed to be available, (as stated by the SMAP). I noticed some nice things. First, we have quite a lot
of memory to work with, almost 32,639 available memory blocks! (Remember, each block is 4KB, so this is quite a lot). Of course, we are going to be decrementing from
some of those blocks, as those blocks still take into account a few things, like our kernel, the bitmap itself, and some other spaces that we would like to reserve
for the kernel to use. But regardless, we have quite a lot of space. I also went and thought about virtual addresses, and I came to realize something. See, available
memory isn't contiguous in the physical memory. There are reserved chunks, available chunks, then reserved, then available, then reserved, reserved, reserved. So, say that
a lot of the available chunks have been filled with processes, and there is a little bit each available chunk left, (say 12 KB in total, but 6KB in 1 chunk, and 6KB in
another chunk). Then, based on my PREDICTION OF VIRTUAL MEMORY, (again, I need to study it), I am guessing that if we try to load in a 12 KB process, virtual memory
management will be able to split up the process and map them into the physical memory (thanks to PMM), but since the process believes the illusion of the virtual memory,
it won't have any issues with this, and it will run as if it were contiguously mapped to the memory. If this is indeed how virtual memory is intended to be used then,
great, that sounds like it's quite useful and important. If it's not, then I really need to figure out what the point of virtual memory is.
Anyway, that's all for today. Tomorrow will be FINALLY turning on paging, (I've been putting it off for SOO long), and implementing the VMM, (I do sincerely believe the 
PMM is easier to implement, but I am hoping I'm wrong, and I'm hoping it turns out that the VMM is much easier to implement than the PMM). After I've done the PMM, we
might make the heap memory allocator. I gotta see, but I will eventually need a way to actually take processes from the disk and put it onto memory based on the specified
address and size of it, (it was easy in real mode- just use INT 0x13 and CHS addressing, or LAS if you're weird, but we don't have that luxury now). Oh, I almost forgot
to mention too. I decided I wanted to get a taste of programming commands. I haven't studied how to do that just yet, so I decided to make a really simple one: basically,
the user gets stuck into a while loop that, obviously, always loops and uses the readLine() function I implemented to read what the user wrote. Then, I wanted to compare
what the user wrote to a specific phrase, and if they say that, it'll print out what we want! But, oh wait, I don't have <string.h>, which is apart of the C standard 
library. So, I had to implement string.h and string.c. It was quite simple, so there is no issues there (simply have a pointer to each character in the string, and since
characters can be checked if they're equal via ==, we can just check each character, and increment each character to their next character IF they are equal. If they are
either not equal, or if they land upon their sentinel, then we terminate. Then, again, since characters are just fake integers, we can subtract their difference. If
they are the same (i.e., they are both the sentinel), they will return 0, (just like the actual strcmp), otherwise, it will return the difference of the first non-equal
characters in both strings, (just like strcmp). Anyway, we used this, and the length function (quite simple, just have a counter that increments for each character in the
string that isnt the sentinel), to make our first "program!". (It's not really a program), but it works! If the user types printmem, it'll showcase the physical
memory! 
Anyway, that's all for today.
Ok, bye.


----------------------------------
DAY 21
Today was not a great day. I didn't finish implementing virtual memory manager, but I did come to understand the point of paging, and virtual memory as a whole. 
Tomorrow, even though it is supposed to be a weekend, I will finish implementing virtual memory management, so I'm not super behind.

I will also add more info (what I learned today) to this devlog entry tomorrow.

It is now DAY 22, and I will be finishing the devlog today.
On day 21, I mostly learned stuff regarding virtual memory and paging, particularly, I learned about the concepts and I had started work on it by following the broken
thorns tutorial, along with the OSDev tutorial. And it turns out, my suspicions were correct regarding virtual memory. It's there to make management of memory and 
processes easier, (duh), but more importantly, it is there to make the management of processes easier. So, one of the biggest issues regarding memory is loading stuff
in from the disk. This takes a lot of time, and it is inefficient for the CPU to be constantly accessing the disk. That is where the power of virtual memory can come in.
Every process can be loaded into their own virtual address, (or, more specifically, each process can have its own page directory, or 4GB of virtual address space). If
the CPU is not using a process, and the OS deems the process to be wasting memory/believes the CPU is in need of memory, it can de-allocate some of the process's pages
from the physical RAM and put it back into the hard disk, before setting the present bit in the page table entry to 0. IF, for some reason, the CPU is in need of that
page again, it will FIRST check the flags and see the "present bit" in the page table entry. If the page table entry says the page is NOT present (present bit is 0),
then the CPU will send a page fault exception (or interrupt 14) to the OS, which it can then handle with its own page fault handler. Now, this might seem complicated,
and to me it was, but I believe it becomes much more clearer with an example.

So, when the computer first boots and everything, and a process is FIRST EXECUTED, the first thing that happens is that it is loaded from the disk into memory by the OS.
The OS then provides the process its own virtual address, by mapping its physical address to some random virtual address, (lets say, I don't know, 0x8000000). Now,
for the sake of this example, and to understand virtual addressing better, lets say this process is fairly large, taking up multiple pages (remember, each page is
4KB, or 4096 bytes). Without the virtual memory manager, if the CPU wanted to run the process, it would have to load in the entire process into memory before executing it,
which is obviously quite wasteful, because the CPU doesn't need the ENTIRE process at once. It first needs the first page, then the second page, then the third page, etc.
until it finishes the process. Basically, if you've ever played a game before, do you remember seeing that "invisible bridge" thing, where you cannot see the bridge,
but as soon as you, the player, start walking over the bridge, the bridge would become visible right under your feet? This is basically what we want to do in the future.
If a process is gigantic, like thousands of pages, (say it is a 5GB process, but we only have 4GB of RAM), we can basically create the illusion of having more RAM than 
we actually do by using this "invisible bridge" illusion. Parts of the process will be first loaded in, (for this example, assume physical memory 0x1000 for simplicity's
sake). Then it will be mapped to a virtual address (assume 0x0 in the virtual address). It will execute that first page, and get to the ending of the
first page, which is around 0x2000 or whatever, (or 0x1000 in virtual memory). The CPU will check the present bit and see the next page is not available, so it will
throw a page fault exception, or interrupt 14, to which the OS will handle. The OS will first see if there is available free memory, and if there isn't, it must then
de-allocate some of the memory. Assume the CPU has completely finished handling the first page, so the OS will then de-allocate that first page, and it MIGHT swap the
contents of the the first page from the physical RAM into the disk. DO NOTE, the OS will only do this if another one of the flags for the page entry was set, that flag
being the DIRTY FLAG, (which means the page was modified). If the page was NOT modified, and it was 100% processed by the CPU, then the OS doesn't care, and it simply
discards the first page entirely. Anyway, the first page will be de-allocated by the CPU, and then the CPU will put the 2nd page where the 1st page originally, (0x1000). 
BUT, it this time maps the 2nd page to virtual address 0x1000, therefore making the process have the illusion that it is still running on contiguous memory, even though it
clearly is not. This is end-goal of virtual memory, and it is one of the reasons for why it is so important. Other important reasons include memory protection, and 
ensuring that no processes override each others data, memory isolation, and whatnot. But, this is the one thing that made really go "oh, that makes sense for why virtual
memory is so seemingly important". 

Now that is the motivation for virtual memory, I will now explain the basics of how virtual memory works, or at least the implementation of it.
Virtual memory is really only applicable when we have paging. Paging is what enables virtual memory to exist in the first place, and a virtual memory manager, without
paging enabled, is completely useless and is just taking up precious memory, and readability, away from your code. Before we implement a virtual memory manager, we first
need a physical memory manager, and I have already explained the motivation and the implementation of a PMM beforehand. The reason we need a PMM is because virtual memory
manages pages, and each page is 4096 bytes, or 4KB. This should sound familiar, because we also just so happen to divide our memory into 4KB blocks in the PMM. The way
paging works is that we have 4GB of physical memory, which is not a lot. As stated previously, we want to create the illusion of having more memory than we actually do,
so we achieve this via the usage of page tables and page directories. Each page table has 1024 entries, and each table entry points to a page, (each entry is a 32-bit 
integer). The entries in the table also point to a physical FRAME, which is just the term for a block in physical memory. Now, I mentione dearlier that pages are 4KB,
and our physical frames are also 4KB. Why 4KB? Historical reasons. It's just considered the standard in most systems today, so we stick to the modern conventions. 
Anyway, each page is 4KB. Page tables have 1024 entries, so they have 1024 pages. 1024 * 4KB is 4MB. Now, we have a page directory, which is just an array of page tables
so 1024 * 4MB = 4GB. If 4GB sounds familiar, it should. 4GB is the maximum amount of physical memory a 32-bit system has. Therefore, we have just created 4GB of virtual
memory. We can create more of these directories, and we WILL, and I will explain why later (I did briefly talk about it in the paragraph above). Anyway, back to 
page tables. Each entry is a 32-bit integer, and the 32 bits are split up: 
Bits 31-12 are the address. They point to the physical frame address
The reset of the bits are simply flags that the CPU uses. Some important bits include: the dirty bit, and the present bit. The present bit is what the CPU uses to see if
a page is present IN the physical memory right now. If it is not, it throws a page fault exception (interrupt 14). If it is, then it continues. The dirty bit is for 
the OS. If the OS deems a page necessary to be relinquished, (as it has been 100% processed), then it will check the dirty bit to see if it has been modified. If it has
been modified, the OS will assume it will be important in the future, so it will save it to the disk. Otherwise, it will merely throw it away and forget about it.
Here are the page table bits:
Bit 0 (P): Present flag
0: Page is not in memory
1: Page is present (in memory)
Bit 1 (R/W): Read/Write flag
0: Page is read only
1: Page is writable
Bit 2 (U/S):User mode/Supervisor mode flag
0: Page is kernel (supervisor) mode
1: Page is user mode. Cannot read or write supervisor pages
Bits 3-4 (RSVD): Reserved by Intel
Bit 5 (A): Access flag. Set by processor
0: Page has not been accessed
1: Page has been accessed
Bit 6 (D): Dirty flag. Set by processor
0: Page has not been written to
1: Page has been written to
Bits 7-8 (RSVD): Reserved
Bits 9-11 (AVAIL): Available for use
Bits 12-31 (FRAME): Frame address

Page directory also stores 32-bit integers, and they have similar meanings to the page table entries. However, instead of having bits 31-12 point to the frame address,
they just point to the page table instead:
Bit 0 (P): Present flag
0: Page is not in memory
1: Page is present (in memory)
Bit 1 (R/W): Read/Write flag
0: Page is read only
1: Page is writable
Bit 2 (U/S):User mode/Supervisor mode flag
0: Page is kernel (supervisor) mode
1: Page is user mode. Cannot read or write supervisor pages
Bit 3 (PWT):Write-through flag
0: Write back caching is enabled
1: Write through caching is enabled
Bit 4 (PCD):Cache disabled
0: Page table will not be cached
1: Page table will be cached
Bit 5 (A): Access flag. Set by processor
0: Page has not been accessed
1: Page has been accessed
Bit 6 (D): Reserved by Intel
Bit 7 (PS): Page Size
0: 4 KB pages
1: 4 MB pages
Bit 8 (G): Global Page (Ignored)
Bits 9-11 (AVAIL): Available for use
Bits 12-31 (FRAME): Page Table Base address

When paging is enabled, memory addresses follow a certain format based on the page directory, table, and whatnot. We want every memory address to reference the directory,
then the table in the directory, so that the memory addresses are not simply identity-mapped (identity mapping is simply when virtual addresses = physical addresses),
and instead point to their respsective frame addresses.
SO, memory addresses will follow THIS format, (example of 0xC0000000):

1100000000         0000000000        000000000000 (this is 0xC0000000 in binary)
 
AAAAAAAAAA         BBBBBBBBBB        CCCCCCCCCCCC
directory index    page table index  offset into page

As you can see, 0xC0000000 points to the 768th table, 1st page, and has a 0x0 offset into the page (I forgot to mention, but every page is 4KB aligned, duh).
So, if we want 0xC0000000 to point to 0x100000, we simply go into the 768th page table in whatever directory, and have the 768th table house the frame address
0x100000, and boom! Now 0x100000 maps to 0xC0000000. 
For more information, (and a better explanation, honestly), go to the brokenthorns guide on virtual memory management and paging:
http://www.brokenthorn.com/Resources/OSDev18.html

Ok, that is all. I am writing this on Sunday, so I will be coming back the next day to work lol

----------------------------------
DAY 22 & 23 

Look at DAY 21 to see what I did on these days.

----------------------------------
DAY 24

Today was quite a bad day in terms of progress.
By that, I mean I didn't progress at all. What I tried to do today, since I had finished memory management, was try and set up the next big thing. That being, I tried
to set up a higher kernel. I thought this was going to be relatively easy, as my plan was simple: 
Have one link loader present for the lower kernel, (which will be setting up the PMM and VMM, maybe even the IDT). Then, once paging is enabled, jump to 0xC0000000.
We'll have another link loader present for the higher kernel at 0xC0000000, which will have the same libraries and it'll all work fine and dandy.
It did not work fine and dandy. My idea was solid, we were able to jump to 0xC0000000 via the usage of another link loader being physically at 0x100000, and then having
the lower kernel (physically at 0x50000) jump to 0xC0000000, that was then mapped to 0x100000 at the lower memory kernel, but for some reason, and I have NO idea why,
all of the libraries (like stdio), were completely unfunctional. After hours and hours of testing and checking, I realized the issue was due to the global variables
I was using in stdio, but I had to use global variables, especially for stuff like the cursor position, because how else am I supposed to keep track of it?? Can I use
macros for that?.. Maybe.. I dunno actually... But, regardless, it was odd why I couldn't use global variables, because it worked just fine in the lower kernel memory, 
so what the hell was the issue now?? Again, I still don't know what the issue is. I checked literally everything, and I do mean everything. I checked the .rodata section,
it was properly loaded in, same with the .data section, same with .bss. I even went as far as to make another assembly file that would zero out the bss section, and set
up another stack, but nothing worked. Even MORE odd was, when I tried debugging in gdb, was that the higher kernel was NOT at 0xC0000000 exactly, it was at 0xC000200.
WHY? 0xC0000000 is aligned to 4KB, there is no reason for that offset to exist, especially because I didn't specify it in the link loader. What the hell is going on??
Why is this happening??
And after going insane for almost the entire day, I came to the conclusion to revert everything (except for the existence of the files), and go back to where we started
for today. Tomorrow, I will shift focus towards trying to implement a heap memory allocation thing (malloc), and then probably, maybe, hopefully, (I am really hoping),
enter into user mode finally.... I did find a video by Queso Fuego that talks about shifting kernel into higher memory but... I don't know, I'm quite worried. I'll
give it one last try tomorrow, and if it doesn't work, for whatever reason, then that's it. I'll just keep the kernel un-divided, existing in 0x100000 FOR NOW. Later on
I might try to actually have it separated and so we can actually say "aha! We use 0xC0000000 to virtually map our kernel!" but, for now, again, none of those shenanigans.
Actually, before I move on, I think I was going about it the wrong way. See, what I tried to do was keep my existing kernel.c, and then just create a new kernelHigher.c,
but what might be better would be to create just a new prekernel.c and keep kernel.c (or, maybe even make a new kernel.c), and have the prekernel.c just do the bare
minimum to see if it even works (which is just setting up paging and whatnot), then jump to the higher half kernel. THAT might work, I hope, but I have no clue. 
Anyway, that is all for today. Today was a bad day and it really left a sour taste in my mouth. I know that no project will always have every day be a smash-hit home run,
or even have every day achieving something, but I really wanted to at least get this higher half kernel stuff out of the way. The issue might've actually been
related to the linker stuff, but I have no clue, because I have never studied linkers. Whatever, though.
That is all for today.
Bye.
Ok I lied, I am back (I am writing this around 2 hours later). I couldn't go to bed in good conscious knowing I did effectively nothing today, so I came back and thought 
about why it didn't work a lot, and I also looked at Queso Fuego's OS, along with other peoples OS. I noticed some things: yes, some people were doing what I was thinking,
which was using 2 linker scripts for the different kernel separations, good job me. But, when I examined how they compiled the linker scripts and what was in each script,
I noticed something:
The libraries that had global variables (usually the ones containing printf) were exclusively saved for the higher kernel memory page, while the lower kernel memory page
did the bare minimum, and acted as a trampoline (thanks OSDev wiki for that terminology: https://wiki.osdev.org/Higher_Half_Kernel). I should note, OSDev DOES have a wiki
providing a tutorial on higher half kernel, but it assumes the user is using GRUB, (which... Okay, that's fair, the bare bones tutorial has the user set up GRUB, so I
am not very surprised. By the way, I am not using GRUB, I am using a custom bootloader.). So, I have an idea now, which I have somewhat implemented. I will remove the 
pmm and vmm libraries from my kernel.c (duh), and I will have it act as the higher kernel, setting up everything, and housing everything. I know the VMM and PMM have
global variables, which is a little bit confusing, because OS's like Queso Fuego's AmateurOS have both the kernel and pre-kernel housing the VMM and PMM, but I think the
trick he, along with other OS devs used, to get around this is by saving the contents of the global variables to a global_address file, then, since the global_address file
is legal and will not be changed even amongst separate bin files, we can simply go into the higher half kernel, and set the global variables in there to align with 
whatever the pre-kernel had, thereby avoiding any issues. So, I just can't use something like stdio.h in both kernels, so I will save it for the higher half kernel 
exclusively. AAnyways, again, I have created the prekernel.c file, and I have housed within it everything that I think is necessary. Tomorrow, I will create malloc and 
whatnot, and then I will try to, once again, enter higher half kernel. I will be following Queso Fuego's video on it, mostly because I failed when I did it by myself: 
https://www.youtube.com/watch?v=pm2L0TxofQU. Ok, that is ACTUALLY all for today. Thank you, and Goodbye. (I just didn't want to sleep in a sour mood, I wanted to sleep 
convincing myself I did SOMETHING)

----------------------------------
DAY 25
Alright, so, I told myself I was going to abandon tring to do the higher-half kernel stuff, but for some reason, I was physically unable to forget about it. I just felt
the need to do it, so I spent today trying to figure it out. And it turns out, I believe my prediction from yesterday was right. The issues were arising due to global
variables being funky, and two linkers sharing the same compiled libraries (stdio.c), which was causing issues with the global variables within the compiled libraries. Of
course, it is fine if two linkers both share something like stdint, because that library has no global variables, only macros. This was the key. I figured out that I could
make use of libraries like the PMM and VMM in both linkers, with one compiled version, as long as I align the global variables in both versions to be the same, (or at least
align the higher kernel to have the same global variable data when it jumped to the higher kernel). So, I created a file called "global_addresses.h" that has a bunch of
macros, which I can edit in the prekernel whenever I modify a global variable, and then, as soon as I enter the higher kernel C file (after setting up the stack and 
clearing out the BSS and whatnot), then I can simply set the global variables to be whatever they were assigned. I am, like, PRETTY sure this works. I tested it out
and it did seem to correctly update the macro in global_address, and then the kernel.c (or higher half kernel) did correctly manage to grab the stuff from global_address
and put it into its global variables, so I am pretty sure it works, and that the higher half kernel is stable.. I hope.
Of course, during this process I've realized a few things that I kind of want to implement in the future, just not for this project right now. One of those things is a
file manage system, like the FAT32. I won't make a file management system on my own, since I believe OSDev considers that a "master-level difficulty" thing to do, so I'll
just implement the FAT32 file system and use that. But, I won't do it just right now for this project. I will likely do it in the future after I have "finished", (finished
as in I've done what I wanted to do, which was make a monolithic kernel that goes to user mode and allows the user to do certain processes/use certain programs via
system calls and whatnot). Which, after that, I will come back and optimize the code, (there is a HUGE unoptimization happening with my pmm initialization, apparently. This
unoptimization wasn't happening when I had the kernel as one, for some reason.. I can't figure out why it's happening now, but oh well), along with doing stuff that I was
too scared to do, like making the C standard library, implementing FAT32, and MAAYYBBEEE getting into networking, creating a GUI, and MAAYYBEEE switching to 64-bit UEFI
stuff. But that is a big maybe. I do think I am approaching the end-goal of my project, as I believe the next step might honestly be entering user-mode now that I have
a functioning GDT, IDT, and memory manager. I might also dip my toes in scheduling, and then back out if the water is too cold (if I find it too hard to implement, or 
something). 
I have come back a few hours after posting the stuff above, and have come to decide that I need to take a step back. I am going to spend tomorrow really just testing
stuff out, seeing how stuff works, really understanding my code, and then also studying about user mode, along with better understanding the GDT. See, as I have mentioned
before, I have finished setting up the IDT, GDT, and a memory manager, meaning, according to OSDev at least, I am now officially ready to enter user mode for real. I have
already studied user mode a little bit, and I understand the grave importance of syscalls, and why virtual memory is so important for stuff like scheduling, or context
switching. And that is where the issue lies. I don't fully understand scheduling, neither do I understand how to implement system calls, nor do I understand what the 
importance of the TSS is. I believe my lack of understanding for the TSS comes from the fact that I actually don't know all too much about the GDT or LDT, (I don't even
have an LDT... What's the point of it??). I understand the IDT, (very simply put it's the 32-bit protected mode, or long mode, version of the IVT), but I don't truly get
the GDT or LDT. So, I've decided. Tomorrow will be a study day where I will go back, study up on the GDT, understand user mode better, maybe even study scheduling/
multitasking, understand context switching, and understand the TSS. I saw an OSDev forum post talking about the TSS, and they also talked about context switching there.
I need to understand the connection between the two. So, yeah, I have a pretty decent gameplan going forward. 
Tomorrow, study day, maybe develop a LITTLE bit.
Day after, try get a little bit into entering user mode. If I do manage to enter user mode, great
Friday, saturday, sunday will be off-days. I'm going to be hanging out with friends for 4th of July and whatever (maybe I'll do SOME work on Saturday)
Then monday, I will either enter user mode, OR, if I have already managed to enter user mode, I will start implementing syscalls, (like open and exit. I dunno, haven't 
studied syscalls enough). Tuesday, I will keep developing syscalls, then after I am satisfied, I might start writing programs the user can use. Then, I'll take a step back
and add the FAT32 file system, so that I can (maybe) create a framework for multitasking and whatnot, (basically, having a file system can make it easier for me allow for
page swapping, which is something some schedulers use, and I too would like to maybe make use of it). I will also finish implementing the page fault handler hopefully,
(currently it just says "page fault exception" then stops. I want it to be perform a page swap if a fault occurs). Then, after all that, I will be "finished" with the OS,
or at least I will have achieved what I wanted to achieve, and slow down the grind to spend the rest of the summer either making some other stuff or hanging out with
friends. I will still continue working on the OS, and work towards making it even better, (there are some huge unoptimizations I am likely doing). And, if I feel confident
enough, who knows, I might try to go beyond and make a GUI or something.
That is all for today, bye.

----------------------------------
DAY 26
Hello, I am writing this actually on DAY 28, (or Saturday) because I forgot to do it the day of. 
Anyway, I didn't do a whole lot of work, really, I just studied a little bit and found the exact root of the issue that was not allowing me to enter user mode properly.
The reason why the page fault occuring was, of course, due to the the fact that:
1. The pages I was mapping to did NOT have the user_present bit in the PTE set to 1, so, when in user mode, I had no privilege to actually access the pages at the
specified virtual addresses and 
2. Though the pages were set to "present", there wasn't actually anything there, so that's an issue too.
But the previous 2 aren't even real issues, the real issue was the following:
3. Another linker issue with global variables! Wooho!
See, I forgot about what I typed in day 25, so I might've said this, but, I actually did a little bit more testing and found out there is a critical issue occuring with
my VMM in the higher half kernel. Whenever I mapped more than one page, (ex: map_page(0x500000, 0x500000) and then in the next line, map_page(0x600000, 0x600000)), then
everything would fall apart. My IDT sometimes wouldn't even load, some exception handlers would fail, (like the keyboard interrupt handler), among other things. I have
since figured out the issue is most definitely with global variables, and I have been re-writing my VMM and PMM ever so slowly to make use of less and less global 
variables, so that I don't have to shove all of these into the global_addresses.h and then move them all over. I know the issue is with the linker global variable stuff
because I moved the mapping of pages to the pre-kernel, and it worked just fine--no issues at all were present. The  only issue that happened next were the issues I
specified in the first two parts, which can be fixed via simply
1. Making a file loader, (or sticking to the CHS addressing stuff, which is very tedious)
2. Set the pages to be ser present
And then boom! I'll be operating in user mode. After that, I simply need to make a bunch of syscalls, the two most important of which being sysexit and sysenter, (I think
that's what they're called), which basically just allow for the entering and exiting of the kernel mode/user mode. After that, it's as simple as making a bunch of other
systemcalls for various other programs/commands (like a sysprint, maybe, that calls like my kprintf based on what the user typed... Which should be pretty easy, as I 
already have my keyboard handler to put the stuff typed into an input buffer. All I have to do is to make it possible for the user mode to also have access to these
interrupts, and voila! It works! I'll have a print command, I guess). But yeah, that's the game plan. I may or may not make a file system, BUT, I might be forced to 
if I really wanna call my kernel a monolithic kernel. A monolithic kernel is one that has:
1. IDT stuff
2. Memory management stuff
3. File system stuff
4. System calls stuff
5. scheduler/dispatcher stuff (I haven't studied these all too well. I still need to read about them)
So yeah, that's the plan. On day 27 I actually didn't do anything, as it was July 4, and I decided to celebrate it with my friends. And now it's Saturday, so I may or
may not do stuff. Dunno, I'll see. That's all for today, goodbye

----------------------------------
DAY 30
Hello, 
I am again  writing this late (DAY 31), but, we have officially reached day 30! Soon enough I have been technically working on this project for over a month now! Awesome!
The end is in sight, though. I managed to fix the issues that I talked about on DAY 26, that being the IDT breakking apart, and the fact that I could not map multiple 
pages. I actually found the root cause of both the issues, for the IDT breaking apart, it wasn't actually a global variable issue, which I came to realization after I
started playing around more and more with the higher half kernel, and noticed only certain keys working. Also, when I say "my IDT broke apart", that's wrong. What was
actually breaking was just my keyboard handler, but, my keyboard handler was breaking due to memory issues. See, what was happening was a sector issue and reading files
into memory issue. I load in the higher half kernel at sector 3, (or 4, I forget), and then I load in the prekernel AFTER the higher half kernel. The higher half kernel
is 21 sectors large, so it was taking up sectors 3-24, (including sector 24). I was then loading my prekernel INTO sector 24, therefore overriding the last 512 bytes, or
1 sector, of information in the higher half kernel. This overriding was then effecting the .bss section, because the .bss section is the last part of the bin file. So,
the fix was simple. Just move things over, make sure I copy the right amount of sectors, and boom! My keyboard handler was working again. Quite a simple fix, and honestly,
I have had so many issues related to memory in this project, that it is becoming easier and easier to spot and understand how to fix these memory issues (thank goodness).
But, yeah, the keyboard handler breaking stuff was not a global variable issue. But, I did decide to re-write my VMM anyway just for niceness sakes. Anyway, the second
issue was my map_page function causing a triple fault if I used two or more calls to it. This is not entirely right. With more testing I figured out that it was having
an issue if I every subsequent map_page call did not follow the rules of the previous map_page call. What does this mean? Let me give you a very specific example, that
just so happened to actually be my issue. Say I map a page with frame address <8MB. Then, every subsequent call must also map a page with frame address <8MB. (0-8MB). 
I could also have first mapped a page with frame address between 8-12MB, or 12-16MB, etc., but then every subsequent call must've also mapped a frame address between those
4MB intervals. "But wait," I hear you say, "4MB sounds awfully familiar". Indeed it is, that just so happens to be how large our page table virtual addresses are, as they
are big size 1024 arrays with 4KB pages in each entry, therefore making them 4MB. So, my first idea was "Oh, we're getting stuck in a page table for whatever reason".
This turned out to be... Not true? Somehow? The issue was actually, for some reason, the fact that I was not de-initializing the first 12KB of physical memory, (we store
some important information in those first 12KB), and that I was also not deinitializing the memory map region. Which.. Ok, that's fair, but then it's odd that my lack
of de-initializing was causing this issue. After all, the PMM and VMM don't actually put anything into memory, I don't have a malloc or file system just yet. So, what's
happening? I don't know. I think the 4-day break I took kind of messed up my brain a little bit, so I have to get back into the groove of things, but it is odd as to why
this issue was occuring. Also, I don't even know for a fact if I fixed the issue, I just THINK I fixed the issue, because all my tests seemed to work. Soo... Whatever.
Anyway, onto the next thing, and that was entering user mode. We OFFICIALLY managed to enter user mode on day 30, and it was relatively simple. I had already set up my
TSS, (which, again, I don't know why my TSS works, I just know it does for whatever reason. Still need to study up on why it worked, when it clearly doesn't follow the
expected rules for the limit, base, and whatever the TSS should have). All I had to do was to create a dummy function in my higher kernel, which will temporarily act
as the user_mode region, and then I also had to create a custom stack for the user mode. To do that, I just needed to set the PTE_USER and PDE_USER bits to be 1 for the
higher kernel region (0xC0000000), because that is also where our dummy user mode function is. And furthermore, I needed to map like 2-3 pages for the user stack, which
I randomly picked to be like virtual address 0xBFFFF000, (I also had to set the user bit to be 1 here too). And then, we just needed the inline assembly function to do its
magic that switches to user mode, (Again, have to read up on this), and then boom! We're in user mode. And we can verify by using a privileged instruction, like 
__asm__ volatile("cli");, and we can see that sweet sweet general protection fault with error code 0x0, which will tell us "hey! We're running a ring 0 privileged command 
from ring 3! Not allowed!". Now, we can start implementing system calls, and I slowly did. See, system calls are actually quite easy to implement. All we do is we create
a separate function that is going to be the MAIN SYSTEM CALL HANDLER, and we set it to be tied to vector 0x80. Why vector 0x80? I dunno, I saw other people use it, so I
used it too. Then, that system call handler will read whatever integer is in the EAX register, and compare it to a big array that we have that houses each function call.
So, for example, say that we have syscall0 and syscall1 as functions inside a void* syscalls[2] array. Then, the syscall handler will read whatever the user put into
EAX before calling INT 0x80, see if it is 0 or 1, then call syscall0 if it's 0 or syscall1 if it's 1, or just nothing if it's >= the length of the array. So, that's
that. Then, all we do in the higher half kernel is just set the descriptor for vector 0x80 to point to the syscallhandler, and assign the flags 0xEE, which just tells
the CPU that the interrupt handler is: present, for DPL 3 (ring 3), and is a 32-bit interrupt gate. And that's it. We can do a 32-bit trap gate for it too, but, no..
I don't think it's a good idea to have a syscall handler be a trap gate. And, yeah, that's pretty much all we did. I should probably explain system calls now that 
we've gotten to them. System calls are incredibly important because, well, as I just explained, the user in user mode can't really do any privileged instructions, like
cli, hlt, or whatever. This is an issue, because what if the user wants to run a program? Well, we can't really give user access to the physical memory, that'll defeat
the entire purpose of privilege, as privilege really only exists for security and for protection of the physical memory. So, we use system calls. System calls is basically
the way for the user and kernel to communicate with each other without giving the user access to the privilege that the kernel has. Say the user wants to load a program,
instead of giving user access to the memory, the user instead executes a system call (again, INT 0x80 with a specific value in the EAX register). But, it's not usually
just one system call. See, the CPU first has to switch from ring 3 to ring 0, and it does that THROUGH the segment selector (which is also defined in the descriptor
for the IDT entry). See, as I explained earlier in some other day (I hope I did, at least), each IDT entry has a relatively complex descriptor, similar to the GDT, that
requires a couple things. Most generally, you have the offset, segment selector, a few reserved bits, the flags, and the higher offset for 32-bit. The segment selector
is what tells the CPU to perform a switch from ring 3 to ring 0 or not. If the segment selector for the IDT entry is pointing to the ring 0 code segment, (which will be
0x08 in the GDT), then the CPU will perform a switch from ring 3 to ring 0 in order to execute that syscall. Basically, the general structure is, imagine a big wall
exists between ring 3 and ring 0. In that wall exists a booth that allows for things to travel between each side. Ring 3 wants to talk to Ring 0, so it executes a syscall
through int 0x80. The booth checks the interrupt, and, say for this example, the segment selector does NOT point to the ring 0 CS, but instead points to the ring 3 
CS. The booth will then deny the syscall from entering the kernel, and instead allow the syscall handler to execute in ring 3, with the ring 3 privilege rules. Knowing 
when and when not to do a privilege switch is a bit annoying, and honestly, I don't fully care about it just yet, so for now, I will have every syscall be performing
a privilege switch into ring 0, (so every idt descriptor entry will have segment selector 0x08). Anyway, now assume that its segment selector does point to 0x08. Then,
the booth will say, "ok", and allow the syscall to execute the interrupt in kernel mode, and do all its shenanigans there, before once again going back to entering 
ring 3. This is important. The syscall does NOT stay in ring 0. It must return back to ring 3, so it is effectively performing 2 context switches, and passing the
big wall, or the booth, twice. That is the general gist of how syscalls work. Syscalls can get more complicated, as each syscall handler can call another syscall handler,
making a big chain of syscall handlers. In reality, this is actually how some systems work, as there is no one syscall to perform anything. Instead, each syscall handler
is short, (also like each IDT handler), in order to, obviously, not allow other interrupts from interrupting an interrupt (this would be quite bad). I have seen a lot
of times from people to keep interrupt handlers short, so I will be likely doing that for syscall stuff (unfortunately, though, I feel like my keyboard handler is a bit
large... But whatever, no issues have arised just yet). Anyway, that was all for today. Tomorrow (or well, today, I guess), I will be finishing up some syscalls, most
likely the sysopen and sysexit, or whatever people wanna call them. After that, I might have to get a file system, which then means I'll have to implement malloc, 
and then I can get into writing programs, make a shell, and then.. That might just be it for what I wanted to achieve for now. 

----------------------------------
DAY 31
Last time I said "and then.. That might just be it for what I wanted to achieve for now", making it sound very simple, as if I have nothing left, but it is not. I still
have a lot of complicated stuff left to go. For one, the entire reason I am implementing malloc, (or specifically kmalloc honestly) is so that I can have dynamic 
allocation of memory for my programs. Now, for that, I need a file system. I was already planning on making a file system already, because I do not want to say "My OS 
still boots from a floppy disk!", (TECHNICALLY IT DOESN'T. I AM USING THE FLAG -HDA WHEN I USE QEMU, SO I AM TECHNICALLY USING A HARD DISK, BUT ALSO, I AM NOT.. IT'S KIND
OF CHEATING). I need to study up on file systems, like a lot, and I might look at Tanenbaum's MINIX for inspiration, and I will most definitely be following along with
Queso Fuego's filesystem, OR, I might follow along with OSTEP and implement the vsfs, (Very Simple File System). I don't know. I don't know anything about file systems
just yet, so I don't really wanna jump the gun. I am a bit confused, though. I read a little (just the bare minimum) about file systems, so I have a SORT OF basic idea of
how it works. Basically, if we want to be able to start reading from a hard disk, we need to use ATA PIO, (well not NEED to, but it is recommended I guess). ATA PIO is
just a way of communicating with the hard drive to extract information from it and put it into the memory. It's actually really useful for also setting up a file system,
as all we need to do is just exploit the ports available in the ATA PIO hardware, (I believe they are ports 0x1F0 through 0x1F7 and 0x170 through 0x177 for each bus). We
can still access these ports, so we simply just have to create a read/write system that accesses these ports and, obviously, reads/writes to the sectors. I guess, my
current question is "why is making a file system so seemingly complex, when you can simply just read/write from these ports?". Like, upon initialization, I can just
load in my file system into some place in memory, then, I can have the file system read through the disk, and I can tell it, like "hey, sectors 3-20 is for this program"
and then I can save its information into the file system, like its size in sectors, AND the sector location of each thing. Then, if the kernel/user want to load in a file,
they will call upon the filesystem to check if there actually does exist a file at the sector specified, and if it does, then the file system can COPY the file, and 
however large it is, into memory, (I am guessing I can do a bit of a bitmap approach thing. So like, big array that houses 32-bit integers. First like, I dunno, 5 bits
can be for location, the rest of the bits can be for size. This unfortunately limits how many files can exist in the disk, but whatever). Then, if we don't want it
anymore, we can just tell the PMM/VMM to unmap the pages/set the bits for the frames to be 0, so that it can be overridden. This seems like a relatively easy to implement
file system, no? Of course, you don't really get access to naming things, and I don't think you really have the ability to make directories, or any hierarchy system, so
yeah. But, I don't really care about all that complicated jargon just yet for this project. This project is mostly to learn about low level programming, so I might
actually go with the approach I talked about here.

I am coming back to this to rant a little bit about my worries right now. I'm starting to hit a wall with my understanding of the code, and I'm also starting to hit a wall
with my understanding of what I am supposed to do next, (well, not really. I KNOW I need to make a dynamic memory allocator, and to do that I need a malloc and file
system, so that my programs do not eternally live on memory). I'm getting quite worried that I don't have a solid understanding of everything, so I have decided to make
the executive decision of temporarily halting progress on the OS. I don't want my entire code to be built off of stuff everyone else wrote, (which it isn't entirely. 
There are parts of each code that I wrote myself a little bit, but I still don't want to be this reliant on tutorials and whatnot). I am going to finish reading OSTEP,
and I might even finish reading Modern Operating Systems by Tanenbaum. I want to have a really solid understanding of OS's and how they work before I continue, because
I am getting near the end, I don't want to rush the end. So, for the rest of the week, (WEDNESDAY, THURSDAY, FRIDAY, or days 32, 33, 34) will be just me studying and 
reading OSTEP, and putting whatever I learned into there.

I tried to implement malloc today, but I didn't understand how malloc really worked, how to implement it, so I need to really learn about it. I don't want to just copy
and paste someone elses code, because that defeats the purpose of learning to do it by myself. So, it is back to the notebook for me to learn about EVERYTHING, including
things I may have already learned (like paging and whatnot) so I can have a more fundamental grasp on those topics. While I am learning more about OS, I might also decide
to refine my understanding of C, because, as I have stated before in a previous day, I am starting to reach a point where my understanding of what is happening in the code
is being worse and worse. Single pointers and whatnot are easy enough, but double pointers is when the confusion starts, so I would like to have a better grasp of C.
Anyway, that is all for today. Today might've seemed like a relatively bad day, but, that's fine. I have re-organized and I have a better plan for the future.


----------------------------------
DAY 32

As I said before, I decided to make the rest of the week into a study week, and hopefully finish reading OSTEP so that I can have a better understanding of what I want
to do next and how to implement the stuff. Though the stuff I read I had already read about from other textbooks and the OSDev wiki, I did still learn about some stuff.
Below are my notes from the first few chapters in OSTEP.

VIRTUALIZATION

The main crux of the problem is how to make processes believe
that they have access to the entire physical memory, when,
in reality, we only have one physical memory available to
house all the processes. We need to give each process the
illusion they are operating in virtual memory in order to
Create the illusion that we have more physical memory than
we really do
Allow for the fragmentation of processes/code, so that
they do not require running in contiguous memory, (i.e.
the first 4KB of a process runs in block 2, then the next
4KB of a process runs in block 6, but it believes it is
running contiguously in virtual memory)

TIME SHARING
The CPU is a single-task processor, which means that it
executes each instruction one at a time. But, modern OS's
make it seem as though the CPU is a parallel processor. They
achieve this through time sharing, which basically means that
each process gets a little bit of process time, before the
CPU jumps to another process. If the CPU does this fast
enough, it can create the illusion of parallelism and
multitasking.

CONTEXT SWITCHING
Context switching is a bit tricky. If a process is running on the
CPU already then, by definition, the OS is not running. Therefore, we
have no way of actually performing a context switch, because a process
is hogging the CPU (assuming the CPU only has 1 core, and thus, can
only do 1 process at a time). So, how can we actually perform a
context switch?
The first approach to fixing this problem is a cooperative approach
in which the OS trusts the process. As we've stated before, with
system calls, processes in user mode are effectively forced to perform
hundreds, if not thousands, of system calls. So, when a system call
is performed, the CPU switches back to ring 0, therefore reviving the
OS and giving it control again. Then, once the OS has control again,
it can now perform a context switch to other processes if it sees fit.
But, there are issues with this approach. The most glaring issue is
that it is too trusting on the process. What if the user programmer
is an awful programmer and codes in an infinite loop? Well, now the
issue becomes that the OS is never revived, and we are now eternally
stuck in the process.
This is where the non-cooperative approach comes in to fix that. The
non-cooperative approach uses the PIT built into the computer. The
PIT is similar to the clock, but it can be programmed to have a
different system clock for processes, (which is generally set to
be 1 millisecond). So, what we can do is that every process is on a
timer. If it either does not finish executing the process in its
acquitted time (perhaps 30 ticks, or 30 milliseconds), then a
timer interrupt will be performed by the PIT, giving control to the
OS once more. Afterwards, based on what the scheduler says, a context
switch can be performed, in which the OS saves a bunch of registers
for the currently executing process onto the kernel stack, and restore
a few registers for the soon-to-be executing process. There is an
issue that can still arise with this solution: what if another
interrupt occurs DURING a system call interrupt handler? --

SCHEDULING
The scheduler is in charge of managing what processes the CPU should
give control to next. Generally, we want the scheduler to be as
optimized as possible, and be picking only the most optimal of
solutions in order to keep the average turnaround time of the
processes as low as possible. Another thing the scheduler should keep
track of is "fairness", but just like how "granularity" and "space"
for a bitmap approach to a PMM are at odds with each other, so is
"fairness" and "optimization". If we optimize performance, we may
prevent a few processes from running, thus decreasing fairness. As of
right now, I don't really see a point to fairness. Anyway, there are a
few approaches you can go to implementing a scheduler, with the most
basic of them being
FIFO
FIFO is arguably the most basic approach. The first process that comes
into the process list is the first one executed, then the next one,
then the next, etc. etc.. There are a few issues with this, though.
It works well enough if each process is running for the same amount of
time, and thus, also take the same amount of time to execute, (assume
10 ms). So, if we have 3 processes, the cumulative average turnaround
time will be (10 + 20 + 30)/3 or 20 ms. The issue comes, though, when
we DON'T assume that every process takes the same amount of time to
finish. Assume the first process takes a whopping 100 ms, but the
subsequent two processes take a mere 10 ms. Well, the cumulative
turnaround time is now (100 + 110 + 120)/3 or 110 ms. This is
quite slow, but, personally, I feel like you COULD optimize it
feasibly by not forcing each process to run fully. How about, instead,
forcing each process to run for at most 10 ms, before switching to a
different process. So, A runs for 10 ms, then B for 10ms, then C for
10 ms. But B and C only needed 10ms to finish, so they're done now,
so now we run A for 90 ms. So, our average cumulative turnaround time is
(10 + 20 + 30 + 120)/3 or 60 ms! Is that not a huge improvement?

SJF
Alternatively to FIFO, we have SJF, or Shortest Job First. This should be
pretty obvious as to how it works. Take the same example with the three 
jobs, A = 100 ms, B = 10 ms, C = 10 ms. If we use SJF, then we get:
(10 + 10 + 120) / 3 or 50 ms. Which is a crazy improvement. But, there are
issues with this one too. We've been assuming that all the processes arrive
into the process list at the same time, but this is simply not true in the 
real world. If we do not assume this, then what if, for example, A
is the first process to arrive, then B, then C? Well, then our SJF 
algorithm for our scheduler achieves nothing, because we are forced to 
process A first, (100 ms) followed by B, (10 ms), then  C, (10 ms). So, we
achieved nothing. (Also, I should state now. Turnaround time is 
completion time - arrival time). So, assume that A arrives first at 0ms
and takes 100 ms to complete. Then B arrives at 10ms, and takes 10 ms to
complete, then C arrives also at 10 ms, and also takes 10 ms to complete.
So our average turnaround time is (100 + (110 - 10) + (120 - 10))/3, or
103.333 seconds, which is, again, slow. 

STCF
The STCF algorithm is another algorithm for the scheduler, (also known as
Shortest Time to Completion First). What this does is it uses the pre-emptive
approach to scheduling. It is willing to pause a process IF the remaining time
it takes to complete the process is longer than the time it would take to complete
the newly appeared process. So, with our previous example, it executes A for 10 ms,
then switches to B, finishes executing B, switches to C, finishes executing C, then 
executes A for the rest of the time unless another process comes by. This gives us
a turnaround time of 50 ms:
((120 - 0) + (20 - 10) + (30 - 10))/3. 

Next up is response time. We define response time to be the time from when a process
firs arrives into the process list compared to the time it is first schedule. So,
Response time = first schedule - arrival
As you may expect, the three algorithms shown before are not great for response time,
as they require running the previous processes in their entirety before the process
you may care about actually runs. To fix this, we have the

RR
The RR algorithm, or Round Robin algorithm throws all the previous algorithms out the
window and, like what I suggested in for the FIFO scheduler, only gives each process
a set amount of time to run, before switching no matter what. So, if we give each process
only 10 ms, then we go from A, to B, to C, back to A. This also gives us quite a nice 
response time, much better than what STCF or SJF can perform, (duh). There is a worry to
RR, though. We have also been assuming that context switching takes no performance, but it does,
inherently. We don't want the time slices of RR to be too short, (microseconds, or even 1 ms), 
because then the cost of context switches will become quite expensive as we are going to be doing
them constantly. Furthermore, though the response time of RR is great, the turnaround time of
RR is not. It is actually quite bad for turnaround time. 

But there is still one fundamental issue regarding the scheduler that we haven't talked about.
That being, we have been assuming that the OS knows the length and time required to execute
each process. In reality, this is impossible, and would require the OS to be omniscient. 

This is where we get to the most common scheduling algorithm, and the one that I am 
90% sure literally everyone uses:
MLFQ
MLFQ, or the Multi-Level Feedback Queue optimizes the turnaround time, and it tries
to optimize the response time too. The way that the MLFQ works is that it houses a 
distinct number of queues, each with a different priority level. At any given time, if
a process is ready to run and is in the ready state, it will exist in one of the queues.
The MLFQ then uses priorities to decide which process to run based on which priority queue
it exists in. The MLFQ assigns priorities to processes based on "observed behavior". If a 
The MLFQ will then try to predict future behavior, and then assign priority based off of that.
Furthermore, the MLFQ will give each process in a given priority queue an "allotment time", 
which is just the amount of time a given process can spend at a given priority queue. If a
process uses up that time, its priority will be reduced. So say a process is 100ms long, and
allotment time is 10ms. It enters into the highest priority, runs for 10ms, then goes down 1
priority, runs for another 10ms, repeats until it is either at the lowest priority, or it is 
finished.
So, here are the rules for MLFQ:
1.	If Priority(A) > Priority(B). A runs but B does not
2.	If Priority(A) = Priority(B). A and B run via RR.
3.	When a process enters the system, it is first placed at the highest priority queue
4.	If a process uses up its allotment, its priority will be reduced and allotment reset 
5.	If a process gives up the CPU (via I/O operation or other) before allotment is up, it
stays at the same priority level (allotment is reset)
But... There are a couple issues with this approach still (we didn't implement MLFQ right).
The biggest issue is starvation. If we have multiple processes that have I/O stuff, then
the longer processes at the lower levels will likely never run. Furthermore, someone could
cheat the scheduler by simply writing a program that, at the very end of their allotment,
would do a very simple I/O call, and then hog the CPU forever, (infinite loop, but at the 
very end, do an I/O call). One way we can  solve the issue of starvation is through the 
following: After a time S, set the priority level of everything to the topmost priority. 
What should S be? Who knows. 
As for the issue of someone cheating the scheduler, we can change rules 4 and 5 to be 
combined and simply state: if a process uses up its allotment time, regardless of how
many I/O calls it makes, then we can reduce its priority. We can optimize MLFQ even
further, by having each queue have its own varying time slice (maybe queue 0 is
10 ms, queue 1 20 ms, etc.). So, here are the actual rules:
1.	If Priority(A) > Priority(B). A runs but B does not
2.	If Priority(A) = Priority(B). A and B run via RR.
3.	When a process enters the system, it is first placed at the highest priority queue
4.	Once a process uses up its allotment time, its priority is reduced
5.	After some time period S, move all processes in the system to the
topmost queue.
And that's MLFQ. It's quite complicated, so I'll probably just implement RR
with like a circular-linked list when I get to scheduling. 

There are other types of schedulers, like lottery schedulers, but those 
prioritize response time over turnaround time. In due time, I will come
back here to talk about multiprocessor scheduling,

PROCESSES
In its most basic form, a process is simply a "running
program". Each process is separated by varying machine states,
as the CPU, obviously, has no idea what a process's
name is. The machine state comprises of the address space
of the process, the registers (duh), and the stack and
frame pointers. Think back to CS252, where each process
was basically its own function. That is basically what
a process is, kind of. A processes APIs must have the
following, (remember APIs are like user stuff. Not ABI):
CREATE: A create function to create new processes
DESTROY: A destroy function to delete existing processes
WAIT: A wait function to suspend processes for a given
period
STATUS: A function which returns the status of a processes'
execution to the user (register info, how long it has run
for, etc.)
Remember that processes cannot simply be executed from the
disk. They must be loaded into memory first before being
executed. The CPU can very efficiently communicate with
volatile memory, caches, or registers, but it is incredibly
inefficient and slow for the CPU to communicate with the disk for
executing code. So, generally, most OS's will load programs from
disk into memory. However, they may do it differently. My OS will
likely load processes/programs eagerly, which means all at once,
while most modern OS's load programs/processes lazily, or only
loading the necessary things, and then, requiring a page fault to
load in the next required thing, (page swapping, or invisible
bridge technique).
SOME THINGS TO CONSIDER:
It seems as though each process has its own runtime stack, and
don't necessarily rely on the kernel/user stack.
When calling a program with specific parameters, the OS will
load the parameters into the stack. Afterwards, the program
can access said parameters via manipulating the ESP, (or more
commonly the EBP).
Processes also need a heap. Very simply, the heap is where the
literal objects live, whilst the stacks simply point to where
on the heap the objects are. Remember, since we are in C, we do
not have access to a garbage collector of some sort, so we need
to implement a malloc(), and have each process manually
allocate memory and de-allocate memory for the heap of each
program. (We will also need a kmalloc for the
kernel in the future when we implement a file system, as we
want the file system directory to be growing dynamically, not
be fixed)
Furthermore, there exist three different states a process can exist
in:
RUNNING: Obviously, the process is running and being executed
READY: The process is ready to run is currently in line
BLOCKED: The process is unable to run, and is currently waiting
for something, (like an I/O request/interrupt).
When a process switches from ready->running, it has been scheduled,
when a process switches from running->ready, it has been
de-scheduled.
So, imagine process 0 is running first, then it becomes blocked.
So, the OS decides that it should run process 1, and schedules it.
Then, process 0 becomes ready, so now the OS comes to a decision
that is decided by the scheduler. Does it continue running process
1, and finish it, or does it go back to finish process 0? There
are more states, and how many states a process can have is based
on each OS, (xv7 has 3 extra states besides running, ready, blocked). Like for example, an "initial state" (basically, 0%
execution on the process), and a "finished state" (100% execution,
but it has not been cleared from memory just yet). Of course,
if you have differing states for each process, you need a way to
manage them all too. That is where a process list comes into play,
where each entry in the list contains information about a specific
process.

RINGS
One of the ways we can execute processes is via a Direct Execution
Protocol, which simply creates an entry for process in the process
list, allocates memory dynamically for the program (malloc),
loads the program into memory from disk (which it does via
a file system thing, which we get into later), sets up the stack
with argc and argv (the arguments for a program), saves register
values, and then jumps to the beginning of the program, where it
executes it, then pops the register values, returns, frees memory
of process, and removes the process from the process list.
There are a few caveats with this, though, as the OS will have
no control over the actual execution of the process. We have no
way of dictating whether the process is following any rules or not,
(like if it's accessing physical memory, when it really shouldn't).
To get around this, the CPU has different privilege level rings
that it automatically keeps track of. User processes obviously
run on user mode (ring 3), while the kernel runs on kernel mode,
or supervisor mode, (ring 0). The way these privileges work is that
0 has the most privileges, whilst 3 has the least privileges. If
the user in user mode tries to execute a privileged instruction,
the CPU will automatically raise a "General Protection Fault"
or INTERRUPT 13, which the OS can then manage (usually it just
kills the process). But what if the user would like to access the
disk and read from it? Well, again, we don't want to give the user
direct access to this, so instead, we give the user access to
various system calls that momentarily switch the operating mode
from user mode to kernel mode, read from disk, and then switch
immediately back to user mode and to the calling user program,
which it does via a "return-from-interrupt" instruction, (iretd,
since syscalls will generally be considered an interrupt).

IDT
This is all related to the IDT. We take great pains in ensuring that
each interrupt, or trap, is associated with a certain handler on boot-time, which is when the system is in kernel mode. We create
a descriptor table, that assigns each interrupts vector, so that
when an interrupt occurs, the CPU can simply go to the descriptor
table, match the associated vector to its handler, and then jump
to the handler. We do the same with system calls, but specify that
the user mode can use system call interrupts by using certain flags,
(by setting the 2 DPL bits to be 3, or 11). Of course, we cannot
have a million different vectors for each system call, so we have
one vector for the main system call handler, then we read a register
that the user sends in, and assign which system call the user wants
based on the value they selected (ex: EAX register has 1, so we call
syscall_1(). EAX register has 2, so we call syscall_2(), etc.)

----------------------------------
DAY 33
Again, more studying. Didn't get through as many chapters as I'd like. I might actually do some studying over the weekend to make up for it.

Everything before this has been on virtualizing the CPU. The next few
notes will be on virtualizing the memory in order to isolate and keep
processes from overriding another process, (also for other various reasons
like to emulate more memory then we actually have). 

SEGMENTATION

One of the methods for creating a virtual address space for each process is to
use the methods of base and bound registers. Basically, we specify bounds for 
the virtual address space, and provide a base register. This base register, when
added with the virtual address each memory address generates (ex: memory address
1KB, base address 16KB) produces the physical address, so: phys_addr = virt_addr + base.
But, this is super simplistic, and has a couple issues, because we are assuming
quite a few things.
1.	The user space processes are being loaded contiguously into physical memory
2.	The user space processes are smaller than the available memory
These are very laughable assumptions, and no process actually works like this. 
Furthermore, even if we do make these assumptions, there are still quite a few 
issues, one being that each process takes up a large amount of space, even if the
program itself is very small. This is because the heap and the stack will allocate
and reserve a large amount of memory, even if all that memory does not end up being
used. So, all of this, and putting the entire address space of each process into the 
memory can create problems very fast, so we have invented differing ways to go about 
fixing these problems. The oldest and now outdated approach (that some people still use) 
is segmentation. The idea of segmentation is simple. Instead of having one base and bounds 
pair in our MMU, why not have a base and bounds pair per logical segment of the address
space? We have three different logically-different segments (in our examples, at least. 
In reality, we have a few more, but whatever). Those segments are code, stack, and heap.
(x86 calls them code segment, data segment, stack segment, extra segment, and general 
purpose segments, CS, DS, SS, ES, GS, FS). There are a few ways to achieve segmentation. 
Take the following example with an example address space:

---------------------- 0x0
|		     |
|    Program Code    | 
|		     |
|		     |
|		     |
---------------------- 2KB
|	FREE	     |
|    	FREE         |  
|	HEAP	     |
|	FREE	     |
|	FREE	     |
----------------------7KB
|	FREE	     |
|       FREE	     |
|	FREE	     |
|	FREE	     |
|	STACK	     |
----------------------16KB

Ok, this is a bad example because I drew it poorly, but the idea is still there. The way
we can achieve segmentation with this code here is by looking at the top two MSB (if in
little endian or whatever). Imagine a 14-bit virtual address, then for the Heap, which
starts at virtual address 4200, (hex 0x1068) has binary form 0001 0000 0110 1000. Again,
we only care about the 14-bits, so really it's: 01 0000 0110 1000. If we set the first two
bits (01) to be the separator for segmentation, specify that 01 is for the heap, 00 is for
code, and 10 perhaps is for the stack, then we realize that this address is for the heap. 
So, we ignore the first two bits and look at the offset, which is 0000 0110 1000, or 0x68
in hexadecimal, 104 in decimal or base 10. So, the offset is 104, which we can add to the
base register physical address (which, for example, assume is 34KB) to get the desired 
physical address for this virtual address. So, 4200 -> 34,920. As for checking if an offset
is legal, we can simply compare the offset to the bounds, and if the offset is greater, we throw
an exception. There are issues with this approach, though. That issue being that it limits our
virtual address space to a maximum size that is, evidently, less than the total memory.
Another thing we can do with segmentation is specify which way we want the physical address
for the virtual address space to be placed, (duh). So, for example, for the code segment and
heap segment, we have been specifying they grow up, by adding the offset to the base. But,
for the stack segment, we can specify that it must grow down, by subtracting the base from the
offset. To do this, we must once again obtain the offset normally, (for example, consider
virtual address 15KB, or 0x3C00). 15KB is 11 1100 0000 0000. The first 2 bits tell us this is
a stack segment, and that we should grow downwards. So, we are left with 1100 0000 0000, which is
roughly 3KB, (0xC00, decimal 3072). Now, for growing downwards, we subtract this from the total size
a segment can be. In this example, since we are working with 12 bits, the max size a segment can be
is almost 4KB. So, 3KB - 4KB, gives us a negative offset of -1KB. Then we add this negative offset
to the base (28KB for example) to get 27KB. 
(I should've stated. Assume the base for code segment is 32kb, heap 34kb, stack 28kb). If we would
like to share segments between each process, we can assign an extra bit for protection (read-write-execute).
Again, there are numerous issues with segmentation present. Here are a few:
1.	If the OS wishes to perform a context switch, it'll have to save all the segment register values and restore
them, which it can do, but it's still a lot.
2.	We now need to manage malloc() a bit differently. With each call to malloc, we now need to, on top of
allocating more memory for the heap, also update the segment size registers to be bigger so that the OS
can know of the size of the heap
3.	Most importantly, segmentation has issues with fragmentation. Imagine we allocate multiple processes of 
varying sizes. We then free some processes in the middle and at the end or beginning. We now have 24KB free,
and we wish to load in a 20KB process. The issue is, though, the OS doesn't allow it, because, though we have
24KB free, those 24KB are split across multiple smaller chunks. The 20KB process wants a full contiguous
20KB of memory, which we simply do not have access to. We can go around this by having the OS periodically
compact memory, but this has a lot of overheard for the CPU, and consumes a lot of resources. There are other
methods, but there is no "perfect method" for addressing the issue of fragmentation in segmentation, whilst 
also keeping the processing time near 0. 

MALLOC
Malloc is an interesting thing in C, as it is in charge of managing the heap physical memory of processes, 
(or getting a certain number of bytes reserved in kernel memory with kmalloc). The header for the malloc 
function is void* malloc(size_t size), where size_t size is obviously the size of bytes the process wishes to
reserve. Generally, the idea that I have for my malloc implementation is the following:
1.	Each process will begin by calling malloc_init(), and malloc_init will reserve a large chunk of memory for
the heap, (maybe like 8kb or something). Each call to malloc will then simply store something into this
already-reserved piece of memory, so as to limit the number of syscalls, (malloc isn't even supposed to be
a syscall. It's just doing syscalls behind the scenes, like sbrk, which I will get into).
2.	If, during a malloc library call, malloc sees that it has run out of memory space, then it will perform a
brk or sbrk syscall to reserve MORE memory. 
3.	Each process exists in its own directory, obviously, (we are using paging), so we will have the same malloc
exist in every directory, along with the kernel, but malloc will ONLY reserve memory in a certain part of the
memory, (like, for example, 8MB or 4MB or whatever). Since we are also making use of a pmm, then we have no
need to worry about overriding. What we DO have to worry about is fragmentation, which we can kind of do a 
band-aid fix and get around by having each syscall to brk or sbrk reserve the same amount of memory. Of course,
sbrk and brk do not work like this, but I do not want to worry about implementing malloc_split, or a compact
algorithm. 
4.	Once malloc has actually found a piece of memory to ""reserve"", (it will, again, just look at the chunks of
memory it has access to, which will be available on a linked list. Malloc will be a linked list, I forgot to 
specify that, with a bunch of virtual addresses that point to reserved pages), then it will simply return
that virtual address, and then pop it out of the linked list all the way until it reaches the end, where it 
will then get more memory
5.	If the process is done, then the addresses or memory reserved by malloc_init or sbrk/brk will be freed in
the pmm. 
There are, of course, issues with this implementation idea that I'm thinking of. First off is "internal fragmentation".
Basically, I am reserving large pieces of memory that may not all be used. This is, in fact, a type of fragmentation, but
I don't really care about it just yet. In the future I might. Furthermore, I talked about how, in order to reduce 
external fragmentation, I'll have each call of brk and sbrk reserve the same amount of memory, (so that even if like, in
the middle, some memory was freed, any and all calls to sbrk/brk will then reserve that memory to make it seem contiguous).
Again, this is a very band-aid approach. In reality, I should use some of the algorithms discussed below:

SPLITTING
So, as I said before, my band-aid approach  isn't fantastic. Consider this:

0		10		20		30
-------------------------------------------------
|	10B	|	10B	|	10B	|
|		|	USED	|		|
-------------------------------------------------

So, our malloc linked list might look like this right now for available spaces:
head -> (start addr: 0, len 10) -> (start addr: 20, len 10) -> null

But if we free the middle part now, and we add it in all willy nilly

0		10		20		30
-------------------------------------------------
|	10B	|	10B	|	10B	|
|		|	FREED	|		|
-------------------------------------------------

Our malloc linked list now becomes:
head -> (start addr: 10, len 10) -> (start addr: 0, len 10) -> (start addr: 20, len 10) -> null
Clearly, this is an issue, because we are perpetuating fragmentation. Furthermore, the issue that could
arise from my idea is the following, (go back to the first scenario where the middle one is used)
I have an object that's 20 bytes. My malloc does a syscall to sbrk or brk to get 10B of memory, because that is what
it is hard wired to get. It gets 10B, returns back, annndd it doesn't work. It's not enough. So, I am forced to get another 
10B via the same syscall (inefficient). But, there are issues to this too, because the next 10B I get won't result in 20B that
is contiguous. This is quite the issue, so my band aid approach might actually not work and I might be forced to do something
more complicated (ughhh). 
This is where splitting & coalescing come in. Again, go back to the first scenario. If the user decides they want only 1 byte, 
then returning the full 10 bytes would be quite a waste, no? So what splitting does is simply split the 10 bytes into 9 byte and
1 byte, and return the 1 byte. Simple enough.
Coalescing also solves the issue I explained with the freeing of the middle part all willy nilly. What coalescing does is it
checks the neighboring chunks to see if they can form a contiguous block. If they can, then they'll merge into one, making this
linked list:

head -> (start addr: 0, len 30) -> null


FREE in MALLOC
One of the issues I might encounter when implementing malloc is free(). The header for free is the following:
void free(void* ptr). Basically, it takes in a pointer to the starting address of the chunk the user wishes to free from memory.
But, how can we get the size of the chunk with just the starting address? To accomplish this, we must store a bit of extra info
(the size of each block and starting addr) in a header block which is kept in memory, usually right before handing out the
chunked memory via malloc. So, we can simply create a struct like so:

typedef struct {
	uint32_t size;
	uint32_t whateverelselol;
} header_t;

And then in free, 
header_t* hptr  (header_t*)ptr - 1; 
So basically, right before the starting address, there will be header information, which we can access to check the size of the
chunk of memory. This might also change up our other code a bit, because now, if the user wants to malloc N bytes, we can't just
give them N bytes, we need N bytes + the size of the header. 

Now, a linked list also has to follow the rules of a linked list, meaning we will also have to make a node_t for the linked list.
So, 
typedef struct node {
	uint8_t size;
	struct node next;
} node_t;

For a diagram on how this entire thing works, I would highly recommend looking at this page again:
https://pages.cs.wisc.edu/~remzi/OSTEP/vm-freespace.pdf (page 10, 9, 8)

PAGING
For some reason, the authors decided to make this part of the textbook, where they had an introduction to paging
and what not, super complex for relatively no reason, but I do believe I understand the point they're trying to make.
Basically, as I have stated before, paging is quite simple to understand really. We partition the physical memory into chunks,
(usually 4KB), and then also create pages that will exist in these partitioned pieces of memory. Each page is going to be an 
entry in a page table that keeps track of the pages. Each page also points to its own physical frame, (or basically tells us
where the physical frame is). So, already, we are experiencing quite a few issues regarding paging that are not small by any 
means.
1.	In order for data to understand where it is supposed to exist, it must first involve a memory access into the page table
to find which table entry (or page) it exists in. It must then do ANOTHER memory access to actually understand where in the
physical memory the data must actually exist in. (Or, more generally, for each virtual memory access it does at least this)
2.	The page table can get quite large if we try to partition the entire physical memory, so not great. 
Basically, paging can get quite big and slow very easily and very quickly.  
That is where the TLB comes in to make things faster.
The TLB is a hardware cache that houses a bunch of popular virtual-to-physical address translations, thus making the translations
from the virtual memory to the physical memory (when paging is enabled) much faster. In the x86, this is enabled by default, I
think, and are handled transparently by hardware. It is only if the page directory/table is not present in-core will the
OS be notified by a page fault. The TLB is mostly transparent, but we can still do stuff with it, (though I don't see 
why you would want to, really).

TLB
Basically, what we do then, when paging is enabled, is first look at the virtual addresses' first few bits to find the page table
entry it corresponds with, which we will call it the Virtual Page Number, or VPN. (This is a horrendous name, but whatever).
Then, we consult the TLB first, and see if the TLB contains a translation for this VPN to its correct frame. If it does, great,
we use that frame, and we skip all the jargon of having to do multiple memory accesses through the table, then the page. If not,
then unfortunately we have to go through it. (Though, as I said in the x86, it will actually give us a page fault instead). If
we manage to find a translation in the TLB, it is called a TLB hit. Otherwise, it is called a TLB miss. When a TLB miss occurs,
the CPU accesses the page table to find the translation, and if it manages to find it, cool, it puts the translation into the TLB
for future references, recalls itself and looks through TLB again, TLB hit, and then carries on. Each TLB entry is quite simple.
For example a TLB entry might look like so:
VPN | FRAME | flags
Apparently, the TLB is a fully-associative cache, which means that the CPU searches the entries in the TLB in parallel to see
if there is a match. The flags are similar to the flags in a ptable, a present bit, protection bits, dirty bit, etc..
There is an issue with the TLB, though, and it has to do with context switches. The TLB only contains virtual-physical 
translations for the currently running process. Take for example process 1. Process 1 has its 10th entry in the page table
point to a page with a specific page frame. Process 2 ALSO has its 10 entry in the page table point to a DIFFERENT page with a
specific frame. This creates an issue in the TLB, because it has two different VPNs, (entry #10) that have different pages. One
simple approach to fix this is to just flush the TLB on context switches, (which is what we do), which empties it out before
switching to a different process. There IS some inefficiency with this, which is the fact that we now have to incur a bunch of
TLB misses for said process at the beginning of the process, but, honestly, that's fine. To reduce some overhead, some hardware
systems provide an address space identifier (ASID) field in the TLB, which is sort of like a process identifier (PID)



----------------------------------
DAY 34

I was being lazy today, and only managed to finish the virtualization section. I'll study some more over the weekend to make up for it, and hopefully finish
OSTEP by... Tuesday? Here are the notes:

Today I want to finish up the virtualization section of OSTEP, and begin Concurrency. 

PAGE TABLES (AGAIN)
As was stated previously, there are some issues with page tables. If each page is 4KB, and we want to create
an illusion for the ENTIRE memory, then, knowing that in a 32-bit address, we have access to at most 4GB of 
memory, that would require our very simple linear page table (if we're using an array approach) to have a
million entries, because 4GB/4KB = 1,000,000. This is a gigantic array, and we'd rather not have this. So, this
section of the notes will discuss some ideas to fix this, (though, we already know of the solution. Page 
directories).

One solution is to just make the pages bigger. Instead of 4KB pages, make them, I dunno, 32KB. This is also an
issue because it will lead to internal fragmentation, where we are wasting a gigantic section of pages, (remember
if a page is in use, we mark it as in use. We don't care if it's only  using 1-2 bytes of the page). We would
generally like to reduce internal fragmentation as best as we can. 

The hybrid approach uses both segmentation and paging to try and solve this issue. Basically, we divide the
32-bit address even further, and have the first few bits point to a segment (like how segmentation works, lol).
This means that each process will now have three different page tables associated with it (because we have
three different segments).
However, this approach has issues. Namely, using segmentation means we can now have variable length pages, 
(because the heap can be larger and whatnot). Therefore, we once again experience external fragmentation, which
sucks, so we avoid this.

Now we arrive to multi-level page tables. This one doesn't rely on segmentation, instead, we turn the linear
page table into something similar to that of a tree, (and it is the solution I talk about, with the directories
stuff). Basically, the way it works is that the page directory points to a page table, and a page directory entry
is only set to be valid if there exists at least one page on the page table that is valid, or present. The page
directory points to the physical address of a page table, and then the same 32-bit address that is used to access
the specific page table can be used again in said page table to access the specific page, and the physical address
associated with the page. I talked about paging a while ago, so I will just put this again here:

AAAAAAAAAA         BBBBBBBBBB        CCCCCCCCCCCC
directory index    page table index  offset into page

This is for each 32-bit virtual address, or memory reference. We are able to split it like this, because each
directory and table only holds 1024 values, so we don't really need the full 32 bits to know where in the memory
we want to access. Just enough to go through the full 1024 entries, and enough to offset anywhere in a 4KB range,
(2^12 is 4KB, 2^10 is 1024). In total, the page directory index, (first 10 bits) and the page table index (next
10 bits) are both the VPN, or Virtual Page Number. There are other things like inverted page tables, but I don't
care about those just yet, so I didn't really bother with studying them. 

SWAPPING
Now we get into swapping. Or, the "invisible bridge technique" as I like to call it. As I've mentioned in 
previous days, there exists a memory hierarchy, with volatile and non-volatile memory. The CPU reallllyy likes 
the volatile memory, because they're the easiest, fastest, and most resource-efficient for the CPU to access,
(think registers, caches, and of course, the RAM). There also exists non-volatile memory, and that non-volatile
memory is MUUUCHHH larger than volatile memory (just.. Huge), but are also much less resource-efficient and slower
for the CPU to access. So, when we run programs, we'd like to have the programs/processes exist in volatile memory
(or RAM), and when we AREN'T dealing with a process, we'd like it to be in the disk, (non-volatile memory), where
we don't have to deal with it. 

I talked about the invisible bridge technique as more of an explanation, but I didn't really explain how it'd 
work. Thankfully, this chapter on OSTEP (https://pages.cs.wisc.edu/~remzi/OSTEP/vm-beyondphys.pdf), and the one
right after it (swapping: policies), do quite a nice job of explaining swapping and how we might be able to 
conceptualize and, later, implement them.
As stated on a previous day, the reason we'd like swapping is mostly for multiprogramming purposes, and memory
management, (Duh). If we have multiple large processes running at once, we'd like them to, obviously, run 
at the same time, but we don't want them to be clashing and overriding each other's memory. This is, again, where
the invisible bridge technique, or swapping, come into play (I am the only person that calls it the invisible
bridge technique. Nobody else probably does this, I don't think). 
First off, we need to define a "swap space" on the disk. Basically, we need to reserve some space in the disk 
for moving pages back and forth. Furthermore, the OS will need to remember the disk address of a given page, so
that it knows where to swap a page to in the disk. When a page is in the disk and not in the memory, it is 
obviously not present, so its present bit is set to 0, and therefore, it also doesn't exist in the TLB. In
x86, this will generate a page fault by the CPU, which we can then handle. But, how does the OS know where
to find the desired page?
Well, this is done by the handler, obviously. When the handler is called, it has access to the CR2 register, which
holds the faulting virtual address. The OS can then translate this faulting virtual address, see what page is
causing the exception in the PT, (find the corresponding PTE), and then issues a request to the disk to fetch
the page from memory, before returning control over to the process once more. The OS will also update the page
table and mark the page as present, update the PFN field of the page-table entry, along with other things. When
it gives control back to the process, another TLB miss is raised, but, as we know, the TLB will be serviced 
and  it will update its array to now hold the virtual-physical translation for that newly added page, (or,
we could just manually flush the TLB and whatnot in the page fault handler). Remember, I/O is expensive, and
accessing the disk is reaalllyy inefficient. So generally, while this current process will exist in the blocked
state, other processes will not, so the OS is free to run other processes while it waits for the I/O of accessing
the disk. 
What happens when the page is full? Well now the OS needs to handle replacing useless pages. If a page has been
processed 100%, the OS should either throw it away (if its dirty bit is 0), or store it into the disk (if the
dirty bit is 1). To know when to start evicting pages from memory, OS's have a "high watermark" and a "low
watermark". When the OS notices there are fewer pages available than what the "low watermark" states, a thread
that is responsible for freeing pages runs, (how it does and what pages it decides to free is quite complex, 
and it doesn't really talk about it for some reason), until there are enough pages available, (what the
high watermark specifies). This thread, called the "swap daemon" or "page daemon" then goes to sleep and waits
for the OS to call it again. 

Now, how do we decide which pages to remove? Well, there are a few methods. To analyze each method, the book
uses a variable called the Average Memory Access Time (AMAT), which is computed as:
AMAT = (Time to access memory) + (Probability of not finding the page in TLB * Cost of accessing the disk).
(THIS ENTIRE EXAMPLE WAS RIPPED STRAIGHT OFF OF OSTEP. I AM JUST "RETYPING" IN MY OWN WORDS, BECAUSE THIS
EXAMPLE WAS HONESTLY REALLY GOOD)
The AMAT is quite important. Take for example a tiny address space of 4KB with 256-byte pages. The first 4 bits
of our 12-bit address space (it's 12 bit because, remember, 2^12 = 4KB) will be for the page table entry index, 
(2^4 = 16, 256 * 16 = 4KB). Now assume that the process accesses only 10 of the pages, and virtual page 3 is
NOT in memory just yet, however, all 9 other pages are. This gives us:
hit hit miss hit hit hit hit hit hit hit
So, our probability of not finding a page in TLB is 1/10, or 10%. If we assume the time to access memory is 
100 nanoseconds, and the cost of accessing the disk is 10 milliseconds, just our example alone gives us:
100ns + (0.1 * 10ms), or about 1 millisecond. Now, assume our hit rate is 99.9%, (or a miss rate of 0.001, or
0.1%), then our AMAT suddenly becomes 10.1 MICROseconds, or 100x faster. Clearly, by having even a few more misses
occur will cause our AMAT to be exponentially higher, causing the overall OS to slow down with a bunch of programs
running.  So, how can we avoid as many misses as possible?

OPTIMAL REPLACEMENT POLICY
Apparently this policy is quite difficult to implement, so I will 100% not be doing this one lol.
This policy is quite simple. If you need more pages, just throw out the ones that are going to be needed the
furthest time from now. There is a really good example on the OSSTEP chapter that I cannot really show, because
they use a diagram, but it's a good one that I recommend checking out: (PAGE 3-4)
(https://pages.cs.wisc.edu/~remzi/OSTEP/vm-beyondphys-policy.pdf)
The hit rate of this policy is 85.7%, (if we ignore the initial misses that would occur by the TLB, since the OS
has just initialized). 

FIFO
But the optimal replacement policy is hard to implement. Instead, we can use it as a comparison standard. The 
closer a method is to the optimal replacement policy, the better we can say that the method is, because that
replacement policy is the peak optimal one. This is a relatively simple policy. Each page is placed into a 
queue, (obviously. A queue is FIFO), and if a page fault occurs, and we need more pages, then we remove the 
first added page. So, for example, say we go like this:
0 -> 1 -> 2 -> 0 -> 1 -> 3 -> 0
Ignoring the first 3 misses, we only ever replace the TLB cache (if we assume it can only hold the info of 3
pages) when we want to access page 3. But then, there's an issue. The first page we added was page 0, and 
accessing page 3 would remove page 0. But, the next page call IS page 0, meaning we have another miss, and we 
now have to remove page 1.
This is quite inefficient, and apparently, with the example OSSTEP gives us, it has a hit rate of 57.1%, which 
is quite bad.

RANDOM
(Honestly I might implement either this or random, because they seem to be the easiest to implement lol)
Instead of picking the first page, or a specific page, we just randomly swap a page into the disk when we 
are running out of pages, and we need more pages. There really isn't much more to be said about random, it is
entirely dependent on how lucky your OS is going to be. There is an issue with random and fifo, where that it 
might accidentally unmap a page that housed some important information, like underlying data structures that the
rest of the program uses, or global variables, or whatever, (remember, the process believes it is operating
under contiguous memory, even when it might not. A process, and its first 4KB might house all the data structures,
global variables, and objects, as one might do in an actual program). So we would rather NOT free those pages. 

LRU & LFU
LRU, which is known as "Least Recently Used", basically checks to see what pages have been recently used when it
needs access to more pages. It looks for the least recently used page, and then kicks it out. There is also the
LFU algorithm, which is "Least Frequently Used". This one checks to see what pages are used MOST INFREQUENTLY,
as in we do the least number of accesses/calls to that page. Then, if we need more pages, we just kick out the
pages used least frequently, as we can assume it doesn't house any underlying data structures, or anything
important, (because if it DID house data structures, it would likely be called very frequently, or very recently
very often, no?). Again, for LRU in action, I'd recommend checking out page 7 of the same chapter. 

The book goes into explaining the differences in each example in various real-world environments. For example,
if we compare these methods, RANDOM, FIFO, LRU, in a "no-locality workload", which means we are effectively 
calling random pages over a set number of times, then it doesn't matter what method you use, as they will
all perform relatively the exact same.

The next example is with an 80-20 workload, which means that 80% of the references go to 20% of the pages, 
(the "hot" pages, or pages we access relatively frequently). The remaining 20% of the references are for the 
remaining 80% of the pages (the "cold" pages, or pages we access infrequently). In this example, we actually
start seeing differences, and we can clearly see LRU performing better than RANDOM and FIFO, (again, look
at the chapter for this one. It's a really good chapter.).

The final workload is the "looping sequence" workload, where we call 50 pages from 0-50, then we loop again.
In this case, we see the worst-case scenario for both the LRU and FIFO. Since FIFO kicks out the first page that
was put into the TLB cache, it sees a 0% hit rate. LRU is similar, and also experiences a 0% hit rate. Of course,
this scenario is veerrryy unlikely, so we don't reaalllyy have to worry about it all too much.

Implementing each of these is also its own beast, (well, particularly LRU). LRU requires us keeping track of a 
list and dynamically updating said list for each page access that the process may perform. FIFO is much more
simple, as we just create a queue, and for each page fault that occurs, the page fault handler will add the
newly-added page to the end of the queue, and, if it needs pages, it will remove the first added page, (duh),
from the queue. Random is even easier, it'll just add the page, and, if it needs more pages, it'll just, again,
randomly remove a page from the TLB cache.

Also, as I've said, we need to update the list dynamically for EACH MEMORY CALL in an LRU model, which is quite
inefficient. We can speed this up with hardware support (the clock, or the PIT!). For each page access, instead
of dynamically updating the list, which is quite inefficient, we can just update a time field somewhere in memory
that corresponds to each page, (so a separate array that corresponds to a page in the page table. Another table
for pages, OR, we can just make the page tables also store time). Then, when replacing a page, the OS could
scan all the time fields in the system to find the least-recently-used page. 
Again, as was the issue with the previous paging thing, having a big ol' array is also an issue, as we will
have to scan and sift through a lot of entries, (perhaps we create another table and entry type structure, but 
just for the time? Might also be inefficient, though). 
We CAN approximate this, though, by instead utilizing a "use bit". Whenever a page is referenced, the hardware
sets the "use bit" to 1. It will never clear the bit, though, as that is the responsibility of the OS. The OS
can use the "use bit" via a "clock hand algorithm". Basically, imagine all currently existing pages exist
in a circular list. A clock hand points to some arbitrary page, and the OS then reads the use bit for that page.
If the use bit for that page is 1, then the OS goes "Oh, this is NOT a good page to replace just yet", so it
sets the pages use bit to 0, and goes onto the next page. It does this until it finds a page with use bit 0,
meaning it likely has not been used recently, and thus, the OS is free to replace it. Personally, I feel like
you can make use of the PIT for this one. The hardware sets the use bit of the page, and then, somehow, (this
is likely going to be complicated, maybe?), the OS starts a timer tick for the page. Once the timer runs out, 
and the page has NOT been called before then, then the pages use bit is set to 0. Then, when the OS wants to clear
a page, it just looks through and, again, tries to find one that's 0. This is the same as the "clock hand" 
algorithm, but it's more dynamic, I guess. 
Also, the dirty bit is quite important. The dirty bit just tells the OS that the page has been modified, and if
it has been modified, it might be useful in the future, so we should store this page. If not, then we can throw
that page away forever, and not have to access I/O (and waste precious resources). So, if the clock hand
finds a page that has its use bit set to 0 AND is not dirty, then the OS has free reign to just throw that
page away. 
Finally, there is also the issue of thrashing. What happens when the memory is over-subscribed, and the memory
demands of the set of running processes is too much? Well, I am not going to deal with this, (sorry! I am not
that good of a coder!), but there are a few solutions
1.	Admission control. Basically, we can decide NOT to run a subset of processes for a process, and only run the
"working sets", (the pages that seem to be used most actively), and keep those in the memory to make at least
SOME progress in completing this process.
2.	The other case, which, if I DO decide to implement, or am forced to, I will be using this one. This case is
to basically go to an out-of-memory exception handler that will look for the most memory-intensive task, and
kill it. While this does fix the problem, it might accidentally kill something very important, so we'd rather
NOT do this.




----------------------------------
DAY 35

I finished the virtualization and concurrency pillars. Only one pillar left (theres security too but lol)

CONCURRENCY

I did a little bit of the notes for this on Saturday, (not really notes, I just did a slight bit of the reading).
Today I hope to finish up concurrency at the very least, and get into persistence for next week. 

LOCKS
A lock is conceptually quite simple. Why we want locks is due to the Producer-Consumer problem created by Dijkstra
(if you've taken ANY CS data structures class, that name should sound either scary, or familiar. Remember 
Dijkstra's algorithm on a graph?). The problem is quite simple. Basically, you have a producer that adds data
to a finite sized buffer, and a consumer that removes data from said finite sized buffer. These two, the producer
and consumer, must not interact with the buffer at the same time, the producer should not place anything into
the buffer if it is full, and the consumer should not remove anything from the buffer if it is empty. If we
have MULTIPLE producers and/or consumers running without coordination or proper management, we can either
overfill the buffer, underrun the buffer, or corrupt shared data. (The dialogue chapter gives an example. Imagine
5 producers produce 5 peaches at the same time. Now, there are consumers that must each randomly pick a peach
and then go for it at the same time. This can create issues, as a consumer might go for a peach that has already
been taken. OR, the producer could produce a peach on top of another peach, or whatever. I kind of changed
their example, but it's close enough). Locks help manage this by making it to the scheduler is BASICALLY forced
to only allow only one thread to run, (if you use one lock, that is). This basically achieves atomicity, as from
the perspective of the entire computer as a whole, it is running that thread all at once without being interrupted
(though, it still CAN be interrupted by interrupts. Just, if the scheduler decides to preempt and try to run a 
different thread, if that thread tries to access that lock, it will wait until that lock is freed). 
So, one thread can hold a lock at a time, and any code between lock() and unlock() are effectively running 
atomically. 
Lock() checks to see if a lock is available on a lock variable (like mutex). If it is unavailable, it just
pauses the current thread. If it IS available, it gives this specific thread the lock.
Unlock() is similar, but only the owner of the lock can call it. Once the owner calls it, the lock is free 
again, and now seeks any other threads waiting for this lock.
We are not locked to having one lock. For example, the POSIX library allows for lock() to actually accept a
variable, implying that it can have MULTIPLE locks, and therefore, increase concurrency, (fine-grained approach).

There are a bunch of different implementation for locks. And so, we have three different criteria:
Does it effectively prevent multiple threads from entering a "critical section"? 
Does each thread that wants a lock get a fair shot of acquiring a lock once it's free?
How good/bad are the time overheads for using this specific lock implementation?
You'll notice that these are quite similar to the requirements we had for when we were talking about scheduler
implementations. That's because they are quite literally the same, lol.

I believe this is also where I should explain race conditions. Race conditions are just simply a bug that occurs when there
is no locking, or any form  of guarding multiple threads from effectively running at the same time and modifying the
same data. For example, take a simple counter. We can encounter a race condition bug by having two threads
modify the counter variable. They can modify it by taking the value from the counter, incrementing it, then
storing it into the counter. Now, how a race condition could occur could be by having
thread A grab the value from counter, increment it, and before it can store the value into counter,
thread B interrupts thread A, grabs the value from counter, (still 0), increments it, and stores it into counter (1)
then thread A finishes, and stores the value into counter (still 1). 
Therefore, both these threads ran, yet counter has only incremented once. This is a race condition bug, and it is a
runtime bug.

CONTROLLING INTERRUPTS
This is a super simple one, and it might be honestly what I do. This solution is quite simple. If we want the
next few pieces of code to run effectively atomically, we should just... Disable the interrupts, no? After all,
schedulers do work based off of interrupts, and so does everything else. Then, once we're done with executing
the critical section of a thread, we can just enable interrupts again. There are quite a few downsides to this,
though. First, the book states that this requires that any calling thread must be able to perform a privileged
operation, and thus trust that this facility is not abused. (Which.. Ok, but, why not just make it a syscall
then?). Second, this approach does not work on multiple processors. It doesn't matter if interrupts are enabled
or disabled, as each other processor can have different threads running, and each thread can therefore enter
the critical section, as our lock mechanisms don't really stop any other thread from executing, it just disables
interrupts. 
Furthermore, disabling interrupts for a long period of time is risky, as it can lead to interrupts becoming lost.

Well, ok, we can then go about solving one of the problems (the multi processor one) by simply having a flag.
typedef struct lock {
	int flag;
} lock_t;

void init(lock_t* mutex) {
	mutex->flag = 0;
}

So when we begin accepting processes, we initialize the lock for a variable, mutex. Then, when a thread is
entering its critical section, and it asks for the lock in mutex, then we can simply do

while(mutex->flag == 1) { 
	// wait here
}

//  free, so we set the flag to 1 to specify it is being used
mutex->flag = 1; 

and for unlock, it is simple as:
mutex->flag = 0;


BUUUTTT, there is a BIG issue with this approach. Imagine process 1 calls lock() with flag = 0. It gets to the 
while loop, sees the condition fails, and goes past the while loop. But right before it can set the flag to be
1, it gets interrupted by the scheduler and the scheduler goes to a separate thread. Now, here, that thread
calls lock() as well, fails the condition, and sets the flag to 1. Then, that thread finishes, and the scheduler
goes back to the other thread that was in lock(). Now, since it had already finished processing the while loop,
it goes and it sets its flag to 1 too. Now, we have an issue. We have one variable, mutex, with flag = 1 in
multiple threads, implying multiple threads have the same lock. This is an issue.

Thankfully, modern CPUs now provide a bit of hardware support, known as an "atomic exchange" instruction. Or,
sometimes also called the test-and-set instruction.

So, instead of having a while loop in the lock() function that does while(mutex->flag == 1), we can just do
while(TestAndSet(&lock->flag, 1) == 1) {
	// do nothing
}

and that's it. The test and set instruction returns the old value pointed by "old_ptr" and simultaneously updates
said value to "new". This sequence is performed atomically, and thus, we can perform a TEST (test the old value
returned by old_ptr), and a SET (set the new value to be whatever "new" is) in 1 line. So, for example, say
the flag is set to 0. When a thread calls lock(), it will acquire the old flag (value of 0), test it, see that
it isn't 1, and then atomically set the flag to be 1, indicating that this process now holds the lock. If
another thread already has the lock, it will acquire the old value (1), see it is 1, then spin and wait for the
thread to free the lock. So, now we have an actually working lock implementation that provides mutual exclusion.

There is also compare-and-swap, which just tests whether the value at the address specified by the ptr is equal to
expected. If so, update memory location pointed to by the ptr with the new value. If not, do nothing. In either case,
return the original value at the memory location, thus allowing the code to compare and swap to know whether it succeeded
or not. 

There is also also LL and SC, or Load Linked and Store Conditional. The x86 architecture does NOT have this, so I will not
be going over it, (at least, I don't think it has this, because OSTEP never claims it has this. It says that MIPS, Alpha,
PowerPC, and ARM have it

And finally, there is Fetch-And-Add. It atomically takes a value at an address, increments it, and returns it. Again, atomically. (x86 has this).
With fetch-and-add, we can now implement the ticket lock implementation for locks. When a thread wishes to acquire a lock, it first does the
atomic fetch-and-add on the ticket value. That value is now that thread's "ticket number", or "turn number". 
lock->turn, which is a global value, is used to determine which thread's number is chosen right now. If the threads turn value is equal to 
lock->turn, it is that threads turn to enter its critical section. This implementation is incredibly fair, and, based on what the book states, it
ensures progress for all threads. Once a thread is assigned a ticket, it will be scheduled at some point in the future. Again, the OSTEP book gives
a nice implementation idea of it:

 typedef struct __lock_t {
2 int ticket;
3 int turn;
4 } lock_t;
5
6 void lock_init(lock_t *lock) {
7 lock->ticket = 0;
8 lock->turn = 0;
9 }
10
11 void lock(lock_t *lock) {
12 int myturn = FetchAndAdd(&lock->ticket);
13 while (lock->turn != myturn)
14 ; // spin
15 }
16
17 void unlock(lock_t *lock) {
18 lock->turn = lock->turn + 1;
19 }

(COPY AND PASTED FROM OSTEP)


Now, hardware locks are simple and nice, and I'll probably be using them, but the biggest issue is performance with all of them. That performance
comes from the spinning part. See, imagine two threads. Thread 1 gets the lock and gets interrupted by the scheduler, which goes to thread 2. Thread
2 tries to acquire the lock, sees its not available, and then spins. And then it keeps spinning until the PIT, or the scheduler timer interrupts it
and goes back to thread 1 where it continues this cycle until thread 1 releases the lock (and therefore is finished). This is quite inefficient, sure,
but I will still be using these hardware locks lol
But, how can we develop a lock that doesn't needlessly waste time spinning on the CPU?

A simple way to do fix this issue is by implementing a yield() syscall that simply switches the currently running thread from the "running state" to the
"ready state", (remember, we talked about these states when discussing schedulers). There are still issues with this, (of course there are!).
The primary one being that the scheduler won't immediately go back to the thread that was just being processed (obviously, if we're using one of
those efficient scheduler approaches that has a queue and whatnot). So, it'll go to another thread, and it'll repeat this run-and-yield thing 
as many times as it can until it eventually reaches the thread with the lock. This is still inefficient, but, more efficient than the spinning.

Another problem with the hardware locks we discussed, (except for the ticket lock), is that they leave things up to chance. See, not every thread
is created equal (unfortunately). The scheduler will determine what thread to run next, and if the scheduler makes a bad choice, we'll have
wasted resources/time. We can get around this issue by using a simple queue. If nobody wants the thread, then we can free the lock. But if someone
else wants the thread (i.e., the queue is not empty), then we can just set the lock to be enabled for that specific thread, or the thread that is
next in the queue. 

This can be solved with this setup, again by OSTEP:
void lock(lock_t *m) {
14 while (TestAndSet(&m->guard, 1) == 1)
15 ; //acquire guard lock by spinning
16 if (m->flag == 0) {
17 m->flag = 1; // lock is acquired
18 m->guard = 0;
19 } else {
20 queue_add(m->q, gettid());
21 m->guard = 0;
22 park();
23 }
24 }

(I only posted the lock implementation, nothing else. Check page 16 yourself: https://pages.cs.wisc.edu/~remzi/OSTEP/threads-locks.pdf)

Basically, we have a guard as a mutual exclusive thing for the lock itself. It ensures that no other threads can modify the queue where waiting
threads sleep, or edit the lock structure itself, etc.. Only one thread is allowed to have access to the lock structure, and that thread is also
the one executing the critical section. Once that thread is done, de-queue other threads from the queue, set guard to 0, thereby
allowing them access to the critical section next. If, in that implementation, we do not set guard to be 0 BEFORE calling park(), then we can get
quite a horrendous bug. Basically, imagine we have two threads. Thread A has access to the lock right now, and thread B tries get access to the lock,
but the lock is in use, (m->flags == 1), so it enters the else statement. It goes there, adds itself to the queue, and goes to sleep by calling
park(). But, it did not set the guard lock to be 0. Remember, thread A, when it called lock(), went into the first if statement, and set the guard
to be 0, thereby freeing it. Thread B interrupted Thread A, and thus was able to acquire the guard lock. But thread B, again, went to sleep before it
frees the guard lock. So, when thread A goes to unlock, which is the following:

26 void unlock(lock_t *m) {
27 while (TestAndSet(&m->guard, 1) == 1)
28 ; //acquire guard lock by spinning
29 if (queue_empty(m->q))
30 m->flag = 0; // let go of lock; no one wants it
31 else
32 unpark(queue_remove(m->q)); // hold lock
33 // (for next thread!)
34 m->guard = 0;
35 }
(AGAIN, FROM OSTEP)

It will get stuck in that while loop, because it is constantly checking to see if the guard lock is 0 yet. It isn't, because thread B is asleep and
still holds onto the guard lock (it is 1), so thread A gets stuck, and nothing ever happens. 
But, there is still an issue to this. See, this code is NOT atomic, so imagine this scenario:
Thread A holds the lock
Thread B wants the lock, sees it is not available, goes to the else statement, adds itself to the queue, releases the guard lock, and just before
it parks itself
An interrupt occurs and the scheduler goes to thread A, who finishes up, and goes to unlock(), sees the queue isn't empty (thread B), and calls
unpark on thread B, (wake up), and also removes it from the queue. 
We then go back to thread B after thread A is finished, which then calls park() and goes to sleep for (potentially) forever

This is quite a big issue, and so we can fix this via another syscall known as setpark(). It indicates a thread is about to park. If another thread
interrupts before it actually parks, and calls unpark before park is actually called, then the next park call returns immediately instead of putting the
the park to sleep. 

There are other types of locks too, like the futex lock, and the two-phase lock, which I am very interested in as of right now. I might go back to them
maybe, because I've heard the Linux kernel, (well, haven't heard. The book literally says Linux uses these, lol), uses these.

THREAD SAFE
Adding locks to data structures to make it usable by threads make its thread safe. 
We can very easily achieve precise data structures, (like counting) via jut adding locks in each thread, like so:
11 void increment(counter_t *c) {
12 Pthread_mutex_lock(&c->lock);
13 c->value++;
14 Pthread_mutex_unlock(&c->lock);
15 }

But, the time taken increases a lot with more threads using this data structure, so it's not great. (4 threads each updating the counter a million times
takes over 5 seconds). SO, to fix this, we use an approach known as an approximate counter. The idea for approximate counting is easy enough, if a
thread wishes to increment the counter, it increments its local counter. Then, local values are periodically transferred to the global value by
acquiring the global lock and incrementing it by the local counter's value. Then, the local counter is reset to 0. Of course, this is only if you're
worrying about multiple CPU cores. If you just have one core, you can just use one counter and then use the precise locker, it won't actually be that
intensive at all, (like use the thing above), because threads will be working in parallel, (or pseudo-parallel). 

The next chapter talks about various data structures and how to make them thread safe. I skipped this chapter for right now, as I'll probably just
come back to it when I implement multithreading and whatnot. 

I may just be getting tired of doing all this reading, but this is starting to become quite complicated as time goes by. Basically, in some of our
programs we want to wait for other threads to finish executing before continuing on, as the execution of those threads, the changes they may make to
the local variables/global variables used by other threads could be vital. Thus, we run into condition variables. They're just an explicit queue that
threads can put themselves into when something like a condition has not been fulfilled. For this, there exist a couple of programs, explicitly, 
the wait() and signal() functions. The wait() call just puts the thread to sleep, while the signal() call is basically executed when a thread is
"done", or has made changes in a part of the program, and wants to wake a sleeping thread, (which OSTEP wraps around thr_join and thr_exit). To use
both the wait() and signal() functions well, we employ the use of a done variable. When a thread finishes, it executes thr_exit(), which will, 
atomically, set done = 1, and calls signal to wake up a sleeping thread, which will wake up the sleeping parent thread, and then it'll continue
execution. You might ask what the point of the done variable is, then, if we can just use signal? Well, threads do not always execute in the way we
want them to. Perhaps the child thread could execute first, which would therefore call signal(), but nothing is sleeping, so it just returns. 
Eventually, the parent thread will be called, which will enter wait() and sleep forever, not knowing the child thread finished long before it started.
We need that done variable to ensure that the parent thread can KNOW that the child thread finished for these types of scenarios. We also need
these to be running atomically, because if the parent thread, for example, calls join(), then, when it is about to go to sleep, there is a chance
it could be interrupted, causing the child thread to run and finish, go back to the parent thread, and then cause the parent thread to fall asleep
forever.

Surprisingly, the next chapter starts talking about the Producer-Consumer Problem. (I swear it talked it about earlier, but whatever I guess).
So, I'm not wrong when I say the Producer-Consumer Problem is associated with locks. It is. But it is a little bit MORE than that too. You can't JUST
have locks with it, as you also need condition variables. The reason why just locks fail is because, as I believe I specified, we need don't want the
producers to produce anything when the buffer is full, and we don't want the consumers to consume anything when the buffer is empty. So, we can just
use simple condition variables to do this. And, if you're only using one thread of producers and one thread of consumers, you can just get away with
have one condition variable (cond) and an associated lock, (mutex). But, if you have more than one thread, things fall apart. As I've mentioned before,
we have guard locks to help stop race condition bugs from occurring, but then other bugs can occur based off of it too. So, for example, imagine
three different threads. Thread 1 is a producer thread, thread 2 is a consumer thread, and thread 3 is another consumer thread. Say thread 3 runs
first, everything works just fine for now, as thread 2 checks the buffer, the condition variable BASICALLY tells it is empty (not really), so 
thread 2 falls asleep. Then thread 1 starts running, the condition variable BASICALLY tells it is empty (not really), so it produces a value and
puts it into the buffer, and then signals to thread 2 (for the sake of this example), that the buffer is ready. But, thread 3 was never asleep,
and it was always in the ready state, waiting to strike, so it sneaks in and consumes the value. Thread 2 then comes back (remember, it was asleep
waiting for the condition variable. That variable now passed), so it re-acquires the lock, and tries to run get(), but nothing is there, as thread 3 
consumed the data. Therefore, signaling a thread only wakes them up, and it only hints that the state of whatever they wanted to process has changed,
not that it may actually have changed (from their perspective). This interpretation of the signal() function is known as Mesa Semantics, while the 
exact opposite is referred to as Hoare semantics. A simple fix to this issue that was just described is to use a while loop instead of a conditional.
Instead of checking for if(done == 0) { wait(); }, just do while(done == 0) { wait(); }. BUT, there is still a bug with this approach.
Again, imagine the three threads (thread 1 producer, thread 2 consumer, thread 3 consumer). Thread 2 and 3 go first and both go to sleep. Thread 1
acquires the lock, produces a value, signals thread 2 to wake up, then goes to sleep. Thread 2 wakes up, acquires the lock, consumes the data, then,
it must now signal one of the two threads to wake up. For the problem so far, we have been assuming that the queue for sleeping threads has been 
set up so that it is FIFO. But, not all wait queues are based on that, primarily because having a FIFO wait queue could ruin fairness, and also allow
for malicious threads to wreak havoc. So, based on some wait queue implementations, we have come across a problem, as thread 2 could feasibly,
accidentally, wake up thread 3, which if it does, then then thread 3 will wake up, see that there's nothing, go to sleep, then all three threads
will be asleep, leading to an error. 
So, again, there is a simple solution to this, which is to use TWO conditional variables instead of just one. So, we will have two condition codes:
empty and fill. (Remember, originally we just have the one condition variable, cond, that just checked for if the buffer was filled or empty. 
Also remember, though, that this conditional variable wasn't explicitly a conditional variable. We weren't waiting for a signal for cond to be sent,
we just had a general lock, mutex.). By splitting the one condition code into two, we can now avoid the second problem by design, as we can simply set
the condition code for empty to be 1, or vice versa, thereby avoiding the issue of accidentally causing all 3 threads to fall asleep altogether.
There is also one other, smaller issue that can be seen with a better example, (one with memory allocation). Assume there are 0 bytes free, and
thread 1 asks for 100 bytes, sees there are none, goes to sleep. Thread 2 asks for 10 bytes, sees there are none, goes to sleep. Thread 3 now
comes around frees 50 bytes. Now, how can thread 3 know to free thread 2? It should realistically only free thread 2, and keep thread 1 asleep.
An easily solution is to replace the signal with a broadcast, so that it wakes up ALL sleeping signals that are waiting for the condition. This will
have a negative performance issue, as the threads that can't be processed will wake up, see they don't have enough, and go back to sleep, but it 
does fix the issue.

SEMAPHORES
Dijkstra, surprisingly, contributed a bunch to the concurrency scene of Computer Systems, or OS Development. So much so that he and his colleagues
invented a primitive type called the semaphore. The semaphore is a SINGLE primitive for all things related to synchronization, that includes 
using them as BOTH locks and conditional variables. 

According to OSTEP, a semaphore is an object with an integer value that the programmer can manipulate with POSIX-standard routines sem_wait() and
sem_post(). There is also sem_init, which of course, initializes a semaphore.
For example:	
sem_t s;
sem_init(&s, 0, 1);
The first argument is, of course, the semaphore structure that we just made, (a pointer to it), 0 means that this semaphore can be shared between all
threads in the same process, and the 1 initializes the semaphore to have a value 1.
The way the semaphore works is relatively simple, I think. 
Here are the pseudo codes for the functions, defined by OSTEP:
1 int sem_wait(sem_t *s) {
2 decrement the value of semaphore s by one
3 wait if value of semaphore s is negative
4 }
5
6 int sem_post(sem_t *s) {
7 increment the value of semaphore s by one
8 if there are one or more threads waiting, wake one
9 }
(ASSUME SEMAPHORES RUN ATOMATICALLY SO AS TO AVOID ANY RACE CONDITIONS)

In a binary lock setting, say we initialize the semaphore like the way we did above. Then, we call sem_wait, which decrements the value of the
semaphore to 0. Then, it will wait only if the value is NOT greater than or equal to 0. Since the value is 0, sem_wait() will return and the calling 
thread will continue. (You may ask: why do we pick 1? Well, we want only this one thread to be able to run exclusively, right? So we set it to 1,
wait decrements it for this thread only, then any future calls to wait will decrement it into the negatives, causing those threads to wait. 

In an ordering setting, (or just conditional variables, really), we can make it so that a parent thread executes first, reaches a point where it 
needs a child thread, executes the child thread, then goes back and finishes up the parent thread. We can achieve this with semaphores, (duh),
by just setting the initial value to be 0 instead of 1 this time. By having it be 0, we can call sem_wait() in the parent thread, (thereby
making the parent thread fall asleep), switch to the child thread, which can call sem_post (on the specific semaphore that was for the parent thread),
which will then allow the parent thread to continue execution. (The alternative method also works, where an interrupt immediately occurs after
making the child thread)

Next up is the producer-consumer problem that we discussed last time. 
If we decide to make two semaphores, one for the empty conditional variable, and one for full, it'll work if we have a very simple buffer of size 1.
But, if we have a buffer of size > 1, then we run into an issue, particularly with our put and get that lead to a race condition. Suppose there are 
two producers, one producer is about to assign a value to the first buffer entry, but it gets interrupted, and the second producer assigns a value there
instead. Then, we go back to the first producer which then overrides that first value and puts in a different value. Now, we've just lost the old data!
That's quite bad. You may ask, "hey, isn't this the same code from the conditional variables chapter, though?" You're right. But, it worked there
because we had mutual exclusion happening, which means only one thread really could run at a time in the producers and consumer section (mutex). Here,
we don't have that, so we need ANOTHER semaphore for mutex, and it'll work all fine and dandy now.

READER-WRITER LOCKS
Some programs don't actually manipulate/insert into anything, and they'd only like to READ from a given section/thread/data structure, or whatever.
Thus, we'd like programs to be able to read whenever they'd please, (concurrently), and not be stopped by mutex, or any other locks. Thus, we'd
like reader-writer locks. We can achieve this via acquire_readlock and acquire_writelock. The way OSTEP does it is quite simple. The first reader ever
will effectively initialize the readlock. It'll increase the readers variable, which keeps track of how many readers are in the data structure, and
it'll acquire a writelock too, while also freeing the general lock. Now, anyone else is free to acquire a readlock, but only the first reader has the
write lock too. We do this because, while there are readers in the data structure, we DON'T want to edit it, as that can cause some messy behavior to
occur, especially if they're using the data in the data structures for their own use. Thus, we must wait. There are issues with this, like writelock
threads COULD starve, but I don't care.

OTHER STUFF
Next up is the dining philosopher's problem, once again posed and solved by Dijkstra (my goodness, this guy was cracked). Basically, there are 5
philosophers sitting in a round table. They each have 1 fork, so there are 5 forks in total. To eat, they need 2 forks (left and right), but they
also think sometimes, and thus, need 0 forks.
The solution is kind of simple. Basically, if a philosopher wishes to eat, we must see if they have a left fork. If they do, then grab it. Then, check
if they have a right fork. If they do, then grab it. If neither exist, or if one exists and not the other, then wait for the philosopher using the fork
to put it down and be done with it. There is an issue, though, with this simple solution. If philosopher 1 grabs the left, then philosopher 2 grabs
the left, then philosopher 3 grabs the left, then philosopher 4 grabs the left, then philosopher 5 grabs the left, we'll have gone in a full circle,
and thus, all philosophers are waiting for the right forks, but none are available. Thus, we must simply force one of the philosophers, at random,
to grab the right fork FIRST, THEN the left fork.

There is also thread throttling. This is simple enough. If you only want a maximum number of threads doing one thing at once, then you can just make
a semaphore with the value being the maximum number of threads you want being able to do this one thing, implement it into your code, (put a sem_wait
and sem_post around the region), and then that's it. 

APPARENTLY, It is hard to make conditional variables out of semaphores, which... I guess I can see? (You're losing the signal and broadcast stuff), 
dunno though, I need to do some testing

The next chapter was on bugs. I don't think its super important to take notes on all the bugs that occur, but they are nice to know.
Basically, almost all concurrency bugs fall into two categories (and also subcategories IN those categories). Deadlock and non-deadlock bugs.
Deadlock bugs are basically when multiple threads need to interract with each other in some way, but are all waiting for the other threads to do 
something else. Thus, they've entered a deadlock (good example is the philosopher problem. All of the philosophers were waiting for the right fork
to be available, thus, they were in a deadlock). Non-deadlock are just bugs that aren't related to deadlocking, and most non-deadlock bugs are just
atomicity or order violation bugs.

The conditions for NO deadlocks, according to OSTEP are the following:
1.	Mutual exclusion: Threads claim control over resources that they need, or the CPU in general
2.	Hold-and-wait: Threads hold onto resources they need, while also waiting for other resources or conditional variables
3.	No preemption: Resources cannot be forcibly removed from threads that are holding them
4.	Circular wait: There exists a circular chain of threads such that each thread holds one or more resources that are being requested by the 
next thread in the chain
If ANY of these conditions are met, a deadlock CAN NOT occur.

Circular wait:
You can obtain this via ordering. We talked about this in semaphores, but basically, if there exist multiple threads, then you can enforce ordering
by acquiring thread 1 before thread 2. But, most systems have more than 2 threads, so we go for partial ordering. But, ordering is just a convention,
not a requirement, so a sloppy programmer can still cause a deadlock. (Though, again, my OS is going to be mostly for fun. The programming is going
to be by me, and though I am a sloppy programmer, I should remember the conventions... I hope).

Hold-and-wait:
Basically, a thread will acquire all locks at once, atomically, at once. Each thread will first grab the prevention lock, which guarantees no untimely
switches can occur in the midst of lock acquistion. But, this has some issues. First off, we lose some concurrency, as we are forcing each thread
to grab all the locks immediately, which is not very good.

No preemption:
Locks are held until their respective unlock is called, so if we want MULTIPLE locks, we get into some trouble, as we are waiting for a lock while
hogging another lock that another thread might want. So, some libraries give us more flexible options that either grab the lock, or return an error code
indicating that the lock is held, and you can try again if you'd like. Thus, if we see that the lock is not available, we can just free the already held
lock, thereby avoiding deadlock. But, we can run into another issue, known as livelock. In a very unlikely scenario, two threads could be repeatedly
trying to acquire both locks, so they're stuck in a loop while staying alive. There ARE solutions to livelock too, like adding a completely random
delay between each loop, which heavily decreases the odds of a livelock occuring like this.

Mutual exclusion:
One way to avoid deadlocking is to avoid using mutual exclusion altogether. This might seem odd, as we have stated that we need mutual exclusion
to run critical sections in our data structures, or other stuff. So what can we do? Well, since we know that a bunch of ISA's, including x86,
have built in atomic instructions, we can abuse them to create lock-free data structures. For example, we can make the counter data structure
with compare-and-swap, and even a few list data structure algorithms, like insertion, with the compare-and-swap atomic instruction.

Or, instead we can do deadlock avoidance via scheduling. If we have knowledge of which threads will grab which locks during execution, we can schedule
the threads in a way no deadlocks can occur. 

OR, we can just have a deadlock detector that runs in the background. If a deadlock DOES occur, then it just restarts the system, or it can take a more
complex approach to recover from a deadlock. 

IMPORTANT STUFF FOR FUTURE IMPLEMENTATION
SINCE I am using GCC, I have access to the <stdatomic.h> library, which gives us atomic_flag_test_and_set(atomic_flag* mutex), __builtin_ia32_pause(),
and atomic_flag_clear(atomic_flag* mutex);
https://wiki.osdev.org/Synchronization_Primitives
I definitely need to read up on concurrency again. This is not a pillar that I think I can read once and I'm done. This pillar was quite heavy.


----------------------------------
DAY 36

I didn't do a whole lot of studying today, and ended up not getting as far as I wanted to. I think it happened because I ended up studying over the weekend, and hanged out with my friends, so I was quite tired
today. But, whatever, I still got through 3 chapters, so it's fine. I can just re-read chapters in the future if I need to.

PERSISTENCE
We are now finally at the 3rd and final pillar of OSTEP, and the journey of reading OSTEP is drawing near. In total, I believe I will have spent an
entire week reading this textbook. Now, onto the notes:

I/O
This pillar begins quite simple and just repeating stuff we already know. It begins by describing some of the computer architecture, and explaining
polling and interrupts. Where it actually starts to explain new stuff is around page 7, where it explains a problem that we run into, even with
interrupts, regarding wasting precious clock cycles:

With programmable I/O, (PIOs), to transfer a large chunk of data to a device, the CPU is once again burdened with a rather trivial task, and is 
being wasted doing it, when it could be doing another process or something. The example OSTEP gives is below:

Suppose the CPU runs continuously, and now it wants to transfer something to the disk. It initiates the I/O, which must copy the data from memory to the
disk explicitly, one word at a time. So, the CPU is being hogged by this I/O operation, until it is done copying data from the memory, and then proceeds
to put it into the disk, where the CPU then regains control and is able to do whatever it wants. So, to get past this problem, we use a Direct Memory
Access, or DMA. The OS basically programs the DMA engine by telling it where the data lives, how large it is, and what device to send it to. Then,
the DMA is able to handle the copying of the data from memory with very little CPU intervention, allowing the CPU to also be doing whatever it wants
even during programmable I/O, or PIO.

Also, the way the OS actually communicates with external hardware, (at least in the x86), is via an in and out instruction (which we've programmed).
Basically, if we want to talk to a certain external hardware, we simply read/write to a port that the external hardware is connected to that processes
our operation (read/write). For example, port 0x60 is the read/write port for the keyboard, so when the user types a letter, the letter exists in port
0x60, and we must just grab it from that port to do stuff with it.

Next up are device drivers. A device driver allows the OS to know in detail about how a device works, and what other specific interactions it can 
perform with a device. Having device drivers allow for abstraction in an OS, which allows us to basically hide the specifics and messy details of
every device ever. For example, say we have a file system. We'd like it to just have a general read/write, and not really care about how to issue
these reads/write requests to different types of disks/drives. Because if we did, then it'd get really messy (i.e., we would need a specific read/write
for, say, the hard drive, or the USB, or SSD, or whatever, and that is not very nice). So, we use abstraction and device drivers. So, now, a file
system issues a block read/write requests to the generic block interface, which routes them to the appropriate device driver, which handles the issues
of each specific request. 

To understand, we can look at the xv6 simple IDE disk driver, which, according again to OSTEP, is the following:
Control Register:
Address 0x3F6 = 0x08 (0000 1RE0): R=reset,
E=0 means "enable interrupt"
Command Block Registers:
Address 0x1F0 = Data Port
Address 0x1F1 = Error
Address 0x1F2 = Sector Count
Address 0x1F3 = LBA low byte
Address 0x1F4 = LBA mid byte
Address 0x1F5 = LBA hi byte
Address 0x1F6 = 1B1D TOP4LBA: B=LBA, D=drive
Address 0x1F7 = Command/status
Status Register (Address 0x1F7):
7 6 5 4 3 2 1 0
BUSY READY FAULT SEEK DRQ CORR IDDEX ERROR
Error Register (Address 0x1F1): (check when ERROR==1)
7 6 5 4 3 2 1 0
BBK UNC MC IDNF MCR ABRT T0NF AMNF
BBK = Bad Block
UNC = Uncorrectable data error
MC = Media Changed
IDNF = ID mark Not Found
MCR = Media Change Requested
ABRT = Command aborted
T0NF = Track 0 Not Found
AMNF = Address Mark Not Found

Basically, the driver presents an interface to the OS consisting of the control, command block, status, and error registers. The OS can then read/write
to/from these registers (either by in/out with x86, or something else). Since we're provided the basic and necessary registers, the OS is able to 
poll this specific device now by reading the status register (0x1F7) until the driver is READY, write the sector count, or LBA addresses, of the sectors
to be used, and also drive number. Then, write the READ/WRITE command to the command register (0x1F7 as well). Then we wait again until the driver
is ready, then write data to data port. Handle any interrupts that occur (some approaches allow bashing, or just 1 final interrupt when the transfer
is complete), and then afterwards, read the status register and the error register to see if the process was successful or if it failed. 

Next up are hard drives. I believe I've talked about it a little bit through my CHS addressing thing, but they work relatively simply. Basically,
hard drives are made up of n sectors, and the address space of a drive goes from 0 to n - 1, (so.. n sectors). Each sector is 512 bytes, and, this is
something I didn't mention last time, but, when you read/write from the hard drive into memory, the hard drive can guarantee that a single 512-byte 
write is atomic. So, if an interrupt DOES occur during a write, then that interrupt will not cause a line in the hard drive to be like:
"mov eax, ", and just be incomplete. No, it will either write the full 512 bytes, or it won't write anything at all. An interrupt will cause a torn
write, though, as only a portion of the write will be complete.

OSTEP also gives a more specific geometry/layout of a hard drive. First up is the platter. It is a circular hard surface on which data is stored. 
Modern disks have, obviously, more than one platter, and each platter is generally made up of aluminum, and coated with a thin magnetic layer that
enables the drive to store bits even when the drive is powered off. Platters are connected via a spindle. The spindle spin the platter
around at a fixed rate when the drive is on. Data is encoded on each surface in concentric circles, called tracks, with each arc being a sector (you
should know that from geometry!). There are hundreds of tracks, each the width of a human hair or smaller. To read or write we have a disk head that
is able to sense the magnetic patterns on the disk, (read) and also induce a charge onto the disk (write). Each disk head is attached to a disk arm,
and I should specify, there is a disk head for each surface of the drive (the platters have two surfaces, duh). To find the right track that contains
the correct sector the operations want to access, the disk arm must physically move the head to the correct track using an operation known as seek,
which is quite costly, as it takes multiple milliseconds for the arm to become "settled" and thus start checking for the correct sector. 

Next up is the disk scheduler. Since I/O is expensive, we would like to be able to manage which I/O requests go first and whatnot. Unlike process
scheduling, where the OS had no ability to know how big a process was, we can figure out how big an I/O request is from a process/thread. By 
estimating the seek time and possible rotational delay of a request, the disk scheduler can know how long each request will take, and be able to choose
the one that will take the least amount of time (shortest job first, or SJF).

There is also Shortest Seek Time First (SSTF). Basically, if an I/O request is requesting the I/O from the same track that the disk arm is already over,
then the SSTF will prioritize those requests first before moving on to a request that wants to access a different track from the platter. However,
the OS doesn't really see the disk geometry, and only sees an array of blocks that represent the disk. So, what the OS can do instead is just use
a Nearest Block First algorithm (NBF) to achieve SSTF. But, like any algorithms similar to "SHOREST __ FIRST", we have the problem of starvation.
If we have a steady stream of I/O requests, with a few from the same track the arm is over, and the others from different tracks, the other requests
could starve, as those requests will be ignored, and the requests that keep the arm in the same track are prioritized. 
To get around this, we can use the elevator approach. The elevator approach (SCAN, F-SCAN, C-SCAN, CIRCULAR-SCAN) have the disk arm move back and forth,
almost in a sweeping motion, to cover all the tracks. So, if a request was handled on track A, and another request comes for track A immediately after,
that request is not immediately handled, but rather queued until the next sweep occurs.

However, SCAN and whatnot have an issue, and that is that they ignore rotation. So, to fix that, we use SPTF (or SATF), also known as Shortest 
Positioning Time First, (or Shortest Access Time First). To really understand why rotation matters, the textbook has an excellent example that
I recommend reading and looking at for yourself:
https://pages.cs.wisc.edu/~remzi/OSTEP/file-disks.pdf (page 12-13). 
Imagine the platter rotates counter-clockwise and starts at sector 30, which is in the inner-most track. Two requests have come in, one for sector 16,
and one for sector 8, which are in the middle and outer-most track respectively. Which request should the I/O accept first? The answer is, it depends.
If our seek time (the time it takes to move the arm to another track) is relative to the rotation speed, then we can use SSTF and SCAN and whatever.
But, if our seek time is FASTER than the rotational speed, then it would make more sense to go further to the outer track, for sector 8, as by the
time our arm would reach the outer track, sector 8 would be there, whereas sector 16 would have to do a full rotation (or almost a full rotation)
to reach our disk head.

There are other things to consider for a disk scheduler, and honestly, it behaves quite similarly to that of malloc and/or a job scheduler, (merging
is also something we should consider: take requests for sector 8, 12, 33, 26, 34. We should merge requests for 33 and 34 and do them in one try)

RAIDS
One of the things that I've mentioned in these notes is that memory is easy for the CPU to access, are fast, but are super small. Meanwhile, the disk
is expensive, holds nonvolatile data, and is super large. How can we make a large, fast, and reliable storage system? 
This is where RAIDS come in, better known as Redundant Array of Inexpensive Disks, which is a technique to use MULTIPLE disks in concert to build a
faster, bigger, and more reliable disk system. RAID is an incredibly complex beast that I will really only study, but likely I will never have
to actually deal with it (unless I manage to get a systems job and they make me do stuff with it! That'd be cool). RAIDs improve performance, 
capacity, AND reliability, (as when a disk gets corrupted, we lose precious data. So, we spread data across multiple disks so as to reduce the chance
of losing it forever in case a disk gets corrupted). 
RAID gives each disk two states: either it is working, or it has failed and all the stuff on that disk has been permanently lost. We assume that
RAID has software that can immediately identify if a disk has failed, so we ignore the silent failure of disks for now. RAID is able to achieve
better reliability through the use of redundancy, but, this is at the cost of some disk space. Assume we have N disks with B blocks in each disk.
Thus, the available memory is N * B. But, if we have redundancy with each block having a copy, we get a useful capacity of (N*B)/2.
There are three important RAID designs: RAID level 0 (striping), RAID level 1 (mirroring), and RAID levels 4/5 (parity-based redundancy).

RAID level 0 has no redundancy, instead, it spreads the blocks out in a round-robin style. Assuming each disk can only hold 4 blocks, disk 1 might have
0, 1, 8, and 9, while disk 2 might have 2, 3, 10, 11, etc. etc., you get the idea. We could also have it so disk 1 has 0, 4, 8, 12, and disk 2 has
1, 5, 9, 13, etc. etc., it doesn't matter. We call blocks in the same row (as in disk 1 has 0, disk 2 has 1, disk 3 has 2, etc. etc.,) striping. Chunk
sizes 

Honnnessttlllyy... I think I'll come back to RAID. It's not something super important to understand right now. The next chapter, though, is quite
important.


----------------------------------
DAY 37

FILES AND DIRECTORIES
I am writing all of this AFTER I have done todays studying. 
Today I only managed to read 3 chapters, but those 3 chapters were looonngg (29 pages, 18 pages, 21 pages). Which... Alright, it wasn't THAT long,
but they were quite packed and I was starting to get burnt out of reading. 
Anyway, files and directories are relatively simple. We want files and directories so that we can allow for the user to store non-volatile things
so that they can use those things in the future, even after they've shut down the computer. This just isn't possible with volatile memory, because
RAM gets corrupted when the computer shuts down. So, we need a FILE SYSTEM. Files and directories work relatively simply, they work in a tree-like
manner in more modern systems, where the root directory is, of course, the root of the tree, and each directory/file is a descendant of the root
(which will be /, or .). Whenever you read a file (like using cat in a UNIX system), a couple of syscalls are done, but most importantly, the
open(), read(), and write() syscalls are performed for both reading and writing from a file. I guess before I go into that, I need to go into inodes
and what not. Each file AND directory has its own unique identifier that only the computer can see, known as an inode. An inode basically stores a bunch
of information about the file/directory connected to it, like its size, privilege values, dirty bit, present bit, all of that stuff. There is a 
unique inode for each file, and so, whenever the CPU wants to access a file, it generally wants to know what the inode of the file is. This is where
the tree hierarchy of the file system becomes important. Say you want to read from /foo/bar/turkey.txt. Well, to do that, a bunch of syscalls will
occur. First, it will begin at the ROOT inode, (/), and read that inode. That inode contains a pointer called the "direct address" that points directly
to the data block on the disk that the file is associated with. If the file is bigger than the block (generally 4KB), it will numerous "indirect 
addresses". But, the direct address will always point to the starting data block. So, when we want to read, we access the inode of the root, go
to the data block associated with the root inode, sift through the data block to find the next directory (foo), repeat that with foo, repeat that
with bar, and then finally, we'll get the inode for turkey.txt. Then, we use the syscall open(), which will put the file into memory, generate a file
descriptor, and return the file descriptor, which is a unique integer that we can use in other syscalls to do stuff with the file. Now, we can use
the read syscall, which will store the first block of the file into a buffer. Then, we can do a write syscall, which will read from the buffer 
generated by the read file, and write it to something, (which can be a different device, like the screen or disk, or to the file itself, or whatever).
And that's that! Quite long, no? I HEAAVVILLLYY simplified what the chapters discussed, but I do have a general idea of what I want to do. If you want
to SEE what I want to do for my file system... Well, I'm not fully confident, but I will either implement VSFS (or FFS, actually..), or I'll just
implement the FAT32 file system, which is much simpler than making a file system from scratch. BUT, none of this matters. Right now, I'm more focused
on setting up my own malloc. 

----------------------------------
DAY 38

Hello,
I'm writing this on DAY 39 because I accidentally fell asleep on day 38 before I got to write this. Basically, day 38 was nice. I didn't read anything,
and instead, I started implementing malloc. I had already started thinking of how to implement malloc from the day I read about it in OSTEP, so I had a
pretty good idea for what I wanted to do. Basically, my malloc implementation is quite simple, (and I have talked about it in the OSTEP notes, but I
will repeat it here). Each process will begin by calling malloc_init(), which will reserve 1MB of memory for the processes starting at virtual address 4MB. 
Then, if the processes ever wanted some memory <1MB, they will perform a split operation, and simply split the necessary amount of memory they wanted from
that 4MB block of memory. I should note, our malloc implementation is similar, if not exactly the same, as a free list malloc implementation. Basically, we 
use a doubly linked list to build up our malloc, and each node corresponds to either a free or reserved piece of memory, which is marked by each nodes 
"free" value. Each node also keeps track of the next node, the previous node, the size of the allocated memory, and the starting virtual address of the 
allocated memory. Beyond split and init, we also have an sbrk. The sbrk method basically expands the heap_end, and allocates more memory from SPECIFICALLY
the end of the heap. We also have free and merge_free. Free is simple, as all we do is ask for a ptr to the starting address of a block of memory, find which
node that address corresponds to, and then set the free value of that node to be true. Free merge is also quite simple. We simply scan through the list of nodes,
and see if a node is free. If a node is free AND the node AFTER it is free, (now that I'm writing this, I might also add a checker for the PREVIOUS node), then we
can "merge" them together, by rewiring the next and previous pointers for the two nodes, and combine the size values, so it looks like we've merged them into one
big block. That's basically all I did. I BELIEVE I have a working malloc implementation, but I haven't done enough testing to truly verify it. I just know that
it isn't causing any freaky bugs from just existing, (which is a good sign). At first, I thought there WAS a freaky bug occuring, but then I realized it was just
because I didn't copy enough sectors for loading in the kernel (I have to constantly update that everytime my code increases by like 512 bytes or more. It's
annoying..). But yeah, that was day 38. Today will be testing to make sure my malloc implementation works, then perhaps editing kprintf so that I can use it
in user space. Because right now, it's causing a page fault to occur, (obviously, because it's trying to access the VGA memory, or the 0xB8000 area, which
I'm not sure if I want to give the user access to just right now). 

----------------------------------
DAY 39

Hello,
I did some research on day 39 (I am writing this on day 40), and I realized what I need to do next. The next step (which I will begin next week. Today will just be studying
more for the next step), HAS to be a filesystem. Not because I want to really be able to read/write from the disk, (though, that would be quite nice), but because the three
main syscall functions (open(), read(), write()) for a filesystem are VEERRRYYY IMPORTANT for an OS. Yesterday I talked about how I need to edit kprintf so I can use it in
user space. That's not true, I don't actually need to edit kprintf, (at least, I hope I don't). Instead, I need a read() and write() syscall, (and open() for fs stuff), 
so that I can actually do kprintf. A very simple write() syscall, specifically for kprintf, would just switch the CPU to kernel mode, and then call the kprintf 
implementation, but that's... Not right. I've already made malloc "incorrectly", (it's not REAALLLYY incorrect, I just need to move it to exist in user space, which
shouldn't be too hard? I hope? I mean, everything will just the same, but whenever I need to call malloc_more_pages(), I'll need to call an sbrk syscall or something).
Hence why I need to make an FS. Now, I could go the easy route and just implement FAT32, and then google read(), write(), open() for the FAT32 (I assume I need to still
make those? Maybe FAT32 just gives them to me?? I dunno, I haven't googled a thing about FAT32 lol), but I STILLL don't really know.. I might do a little bit of studying
over the weekend, because today (day 40) I didn't really do anything and yesterday, (day 39), I kind of just did some bug-fixing and messing around with more of my code,
and making realloc, which isn't really necessary right now. I have a general idea for what I want to do and what I need to achieve in the coming weeks, (maybe months? 
I don't know how big this task is...). 
1. Understand ATA PIO and implement it (https://wiki.osdev.org/ATA_read/write_sectors). 
2. Change from CHS addressing to LBA addressing (OPTIONAL. Not really necessary, it's just that CHS addressing is obsolete)
3. Implement ATA PIO drivers for future FS (NOT in bootloader, we'll do this in C once we've loaded in the kernel and whatnot)
Next up is file system implementation
1. We will have a header file that will store structs for the superblock, inode block, data blocks, the inode bitmap, and the data bitmap. 
2. Go back and have the bootloader load in the superblock for our FS, and mount it 
3. ??? 


----------------------------------
DAY 40 & 41

I am writing this on day 42. Originally, I was writing this on day 41, but I forgot to press "commit changes", and so it didn't commit, and I was too tired and lazy to
rewrite everything, so here I am.

Anyway, I have decided to combine day 40 and 41 to 1 devlog, because I didn't really do anything on day 40, and I did do stuff on day 41, so whatever. I want to spend
some time explaining ATA PIO and my file system idea, and I will explain a bit more about my file system idea on day 42, and how I am going about implementing 
the various functions.

First off: ATA PIO. I had originally planned on switching up my bootloader from using CHS addressing to using ATA PIO CHS mode to load stuff from the disk into memory, 
because I really didn't want to use my old (and still current) bootloader. It's super messy, terrible to actually read, and it somehow manages to work and defy all
expectations. The new bootloader I made up is a lot more clean, easy to read, and easier to understand, but the issue with it is that it... Doesn't work as expected.
For some reason, the reading of sectors from disk to memory only seems to read 256 words, or 512 bytes, or 1 sector, no matter how I change the values of what I put
into the port 0x1F2. My guess is that I wasn't employing the 400ns delay, or I wasn't looping back and reading each sector, 1 at a time, but oh well. I'm too lazy to
fix it, as it's not really necessary at all, and just using CHS addressing works fine, (though, it is incredibly outdated, and using LBA addressing is much better.) Actually, let me stick to that. If you're early on in your OS project, you will see people talk about
floppy disks vs hard disks, and CHS addressing vs LBA addressing (or ATA PIO), and how outdated CHS is. I also got a little bit curious as to why people think this, and
I decided to do a little bit of research and figure out, why use LBA over CHS, and why use ATA PIO over INT 0x13? 
To answer the first question, let's really quickly look at what CHS addressing does. It exposes the the physical details of the storage device to the software of the
operating systsem, which already isn't great. It uses a combination of three things: cylinder, head, and sector, which only appear in hard disks (which, as you may know
are also being swapped out for the modern-day storage devices known as SSDs). Furthermore, CHS mode placed a cap on how large the disk could be before CHS addressing
failed to reach that part of the disk. This was because the scheme used 16 bits for cylinder, 4 bits for head, and 8 bits for sector, meaning it could only count from
sectors 1-255, heads from 1-15, and cylinders from 0-65535, which in total meant the maximum disk size could only be 128GiB, or 137.4 GB, (which is definitely large
enough for a hobby OS like mine lol). Now, LBA addresses most of these issues. First, it heavily simplifies the stuff the software in the OS has to do to partake in an
I/O operation, as it addresses the disk in a linear way, and instead of having to manage how many sectors are in a cylinder or head, and then adjusting based off of that,
we can simply just use the sector. We no longer need to say sector 62, head 3, cylinder 100, and instead, we can just say sector 500 or whatever that CHS is equivalent to.
Thus, accessing I/O becomes infinitely easier with LBA. But, what about the size issues? Well, that only really got solved with the introduction of LBA48. Instead of 
having 32 bits for a sector address, we can use 48 bits for a sector address and, thus, obviously have larger disks. Also, LBA mode allowed us to use other forms of 
storage devices too, as it simplified all storage to be linear, and thus, we can make use of anything. 
Now, that's CHS vs LBA. What about ATA PIO over INT 0x13? Well, to put it simply, INT 0x13 does use ATA! The issue with it, though, is that it uses an incredibly outdated
version of ATA, (or, more popularly called, an ATA-standards). I don't know exactly what ATA-standard INT 0x13 uses, but it uses an older one, and so it's a little
bit more limiting, or just worse. Newer ATA-standards are well, newer, so we would like to make use of them. You might ask, "what is ATA PIO?". Well, all an OSDev
really needs to know is that the ATA, or Advanced Technology Attachment is simply an interface available to the CPU, (and thus the OS developer), that allows us to
communicate with mass storage devices, such as hard disks. PIO just allows us to execute I/O stuff and use the interface to our liking, (PIO stands for Programmed Input
Output). Of course, it gets more complicated than that. There are currently two buses, known as the primary and secondary ATA bus, each with different ports, and these 
buses are connected to the disks, thereby allowing us to play around with the disks. The main ports we will use are associated with the primary ATA bus, and those
ports go from 0x1F0 - 0x1F7. Each port plays a unique purpose, but the purpose also changes based on what type of addressing you're using, either CHS or LBA mode.
CHS makes ports 0x1F4 and 0x1F5 for cylinder low and high, and you also send different things to port 0x1F6 (0xA0 for master drive when using CHS ATA PIO, or 0x40
for master when using CHS ATA PIO). There's a whole OSDev wiki page on ATA PIO, which I do recommend reading, (I too have to read it entirely, I've only really read
tid-bits of it). https://wiki.osdev.org/ATA_PIO_Mode. If you want to look at an example of what my bootloader would've looked like had we used ATA PIO for the bootloader,
I do have the example file still there in the bootloader folder, though, do be wary, it doesn't work. It only reads 1 sector to the appropriate address. 

I've decided to save the explanation of the FS for a later day, (day 42, 43, 44). I'll just have day 40 & 41 be for ATA PIO.

----------------------------------
DAY 42, 43, 44 (I've decided to combine these 3 days because I just worked on the same thing on all of them. That being my fs)

Hello,
Today I will discuss my filesystem, and a bit about how it works, because my notes on the persistence pillar on OSTEP was... Not very good. It was very small, and I
don't think it really fully explained my OS and what I had planned.
So, I will be implementing the VSFS, or very simple file system. Or, well, I think I am at least. In reality, I'm just implementing whatever, almost, and I'm using
a combination of the ext2, unix, minix, and whatever other filesystem I can find as inspiration for this filesystem. I don't know if it's actually similar to the FAT, 
but I imagine it is, because I am going to be having some of the same cheat-things as FAT. (For example, if a file is less than 4096 bytes, or 4KiB, it will still take
up 4KiB of storage on the disk, because we are going to be partitioning up files to be stored into 4KB data blocks). 
Anyway, now would be a good as time as any to start explaining my fs. As stated literally in the previous paragraph, it is similar to very early forms of file systems, 
like ext2, where it is super simple and relatively easy to understand, but implementing it becomes a different beast (you have to pay attention to a LOT of things).
For help in implementing my FS, I sought out the assistance of OSTEP, particularly the File System Implementation chapter, and the Fast File System chapter that they have,
and I used Queso Fuego's OS, along with the OSDev wiki whenever I needed more inspiration on how I could implement things differently. The part where my FS may differ
from OSDev's EXT2 page, and Queso Fuego's OS is that: Queso Fuego's OS uses extents as a form of managing data blocks, whereas I use a more simple, streamlined, and easier
approach, (at least I think so), of just using direct pointers. Earlier OS's like ext2 had around 12 direct pointers, each pointing to a block (not the address, or sector
#, they just contained the block #). Each block is 4KB, so 4KB * 12 means that ext2 had quite the amount of space for each of their files. Of course, these blocks were
not all initialized immediately. If the blocks were not in use, they pointed to nothing (or held 0 to signify not-in-use). Whenever we needed that new block, we could
just assign it a data block from the bitmap, set the region of the datablock to be 0's (cleareed out), and be on our mary way. Compare this to extents, and I believe
extents seem much more difficult. I don't really know how extents work, but I don't really care, as direct pointers work just fine for me. Again, I'm making a toy, hobby
OS for learning, really. I don't particularly care about having the most efficient, or a FS that is able to take care of literally every edge case in existence. I just
need to be able to tackle the big and major issues that a FS solves, and I am happy with that. I should point out, I mentioned that ext2 has direct pointers. The more 
astute of you might say "wait.. How come I can store like gigabytes worth of data into a file if some FS use direct pointers". Good question, one, I don't think modern
OS's use this outdated approach (I think?). Two, we can still store more data even if we exceed the 4KB * 12 data limit. To do that, we use indirect pointers, which are
a looott more complicated, and I'll deal with them laatteerr, but to explain how they work, it's pretty simple. Direct pointers are, well, direct. They directly point
to the data block that house the data corresponding to the inode. Indirect pointers are not as direct, and instead, they point to a group of pointers that then point
to the corresponding data blocks. Which this approach, we can achieve a lot more data being "housed" in an inode, as an indirect pointer could point to numerous pointers.
Or, we could keep going. We could have indirect pointers point to indirect pointers that point directly to data blocks (double indirect pointers). Or, we could go one
degree further and get a triple indirect pointer. With each degree we go further, we get exponentially more data blocks (duh), and thus, the total capacity an inode file/
dir can hold becomes gigantic. Of course, the issue then becomes traversing these indirect pointers and trying to get a data block, as 1. it is not easy to implement, and
2. it can be quite time inefficient/resource inefficient, hence why we like to use our direct pointers first, then our indirect pointers if we can. 
Anyway, I've been talking too much about direct pointers and indirect pointers, and I haven't really explained my fs, so let me get to that now for real.
The structure is simple, but to first understand the structure, we first have to abandon the way we view the disk. If we use CHS addressing, we view the disk as a group
of cylinders/platters, with various sectors, a spindle, an arm, heads, etc.. This is quite complicated, and it is a pain to remember it all. Furthermore, as I discussed
in one of the previous days, we would like to be able to have our fs kind of be able to be used.. Y'know.. Anywhere? In like, any disk or storage device. So, that is why
for the sake of demonstration, we will be using the LBA addressing mode, or the way that the LBA addressing views the disk: as one big continuous space, separated by
sectors instead of bytes, like so:

--------------------------------------------------------------------------------------------------------------
|		|		|		|		|		|		|
|	SECTOR 	|	SECTOR	|	SECTOR	|	SECTOR	|	SECTOR	|	SECTOR	| ...
|	1	|	2	|	3	|	4	|	5	|	6	|
--------------------------------------------------------------------------------------------------------------

You might recognize this as looking almost identical to how I showed off memory, and you'd be right. We would like to view the disk similarly to memory, BUT, with a 
key distinction. We should not really be allowed to use byte-addressing to access parts of the disk. Instead, we would like to rather use sector-addressing to access
parts of the disk, and that is in fact what we do. Anyway, the first step of our FS is to get rid of this sector addressing, and instead, group up these sectors into
block-addressing:

---------------------------------------------------------------------------------------------------------------
|		|		|		|		|		|		|
|	BLOCK 	|	BLOCK	|	BLOCK	|	BLOCK	|	BLOCK	|	BLOCK	| ...
|	1	|	2	|	3	|	4	|	5	|	6	|
---------------------------------------------------------------------------------------------------------------

Of course, we don't ACTUALLY change the disk, the disk still percieves itself using sector addressing, we would just like to use blocks of 4KB, (4096 bytes), or about
8 sectors, as that is a nice sweet spot, which I will explain later. Now that we are using blocks of 8 sectors, we can move onto the next part of out FS. Superblocks,
inodes, data blocks, and bitmaps.
I'm sure you remember from when I talked about VMM's and PMM's, but a file system works eerily similar to them (not really, not at all, but I'll get to that). What we'd
like to have is a bunch of free blocks in our disk that are going to be specifically used by the user/kernel to house various necessary files, and then call on those
various necessary files. But.. How can we know in which block we've placed each file? Well, one possible solution is to simply give each block an ID, or special number,
as sort of some metadata. Then, if we know what file is associated with what ID, we can iterate over the disk, look for the specific ID, and once we've found it,
hooray! We have the file we want. But there is a HUGE caveat to this method, and that is that it sucks. It works, but it sucks. The disk is not like memory where it's
relatively small, as I've said before. The disk is gigantic, and it can be terabytes large. So, lets take a modern disk, 1TB large, for example. Assume the worst case
scenario, and that the file we want is at the VERY END of the disk, meaning we have to iterate through it entirely. A terabyte is about 1e+12 bytes, each block is 4096
bytes (or 8 sectors), meaning we have to do 244140625 iterations in total, (this calculation is probably wrong because I'm stupid, but the point still stands). That's a 
lot of iterations! Sure, you could remedy this slightly by using a different search algorithm, like perhaps binary search or whatever, (which would also require you to
sort your disk, soo), but surely there is a better way, right? This is where that "sweet spot" talk comes in. What if, instead, we keep a big array for the IDs of each
data block SEPARATE from the data blocks, sort of like this:

---------------------------------------------------------------------------------------------------------------
|		|		|		|		|		|		|
|	BLOCK 	|	INODE	|	INODE	|	DATA	|	DATA	|	DATA	| ...
|	1	|	BLOCK	|	BLOCK	|	BLOCK	|	BLOCK	|	BLOCK	|
---------------------------------------------------------------------------------------------------------------

This is called the INODE block. An inode contains the metadata for a file, such as its unique ID, the size of the file, the data block that the file is located in, among
other things. So, cool! If we want to find a specific file, given its inode, we can search through this array for the specific inode, right?! Well, yes, but that is
still quite inefficient, because inodes are quite large, and to find a specific data block, all we really need is the inumber, or ID, of a specific file/inode, right? 
That is where the bitmap comes in. I talked about bitmaps when I was implementing a physical memory manager, so I won't go into it, but basically, we keep a bitmap, and
each bit in the bitmap basically corresponds to a specific inode. We can then jump to that inode, read where the data block is located, then jump to that data block and
voila! We can read a file now much more quickly and efficiently. This is also where the "sweet spot" I was talking about comes in. Make the block sizes too small, (1KB),
and then your bitmap and inode array is gonna be much larger and take up a lot more of the disk. Make your block sizes too large (1MB), and now you'll have to deal with
a lot of fragmentation, as each file is assigned its own unique block. Hence why we use the sweet spot of 4KB. 
But, you may notice, the first block is seemingly empty for whatever reason. That's where our superblock goes in. You see, we would like to be able to kind of initialize
everything, right? How do we know that the block we're reading right now is the inode block, and not just some random data block? Well, the superblock comes in and helps
with that, as it contains the size of the inode table, along with the first block in the data block section, inode block section, and the first block that contains
the bitmaps, among other things. Basically, the superblock is a universal beacon that we can use as a reference point to guide us to where we need to be. We also use
the superblock for basically everything. One of the things I stole from Queso Fuego's implementation was the "first_free_bit" attribute, which points to the first
free bit in the inode bitmap. Thus, if we want to make a new file, we can very easily give it a unique ID and then just update the superblocks attribute to point to the
next free bit. What about data? How do we assign data blocks to inodes? Also quite simple. We have a bitmap for the inode array block, right? Why not another bitmap for
the data block too, so that we can know when a data block is in usage, or when a data block is free, and also so we can find a data block as easily and quickly as 
possible. With all of that, we have this:
---------------------------------------------------------------------------------------------------------------
|		|		|		|		|		|		|
|	SUPER 	|	BITMAP	|	INODE	|	DATA	|	DATA	|	DATA	| ...
|	BLOCK	|		|	BLOCK	|	BLOCK	|	BLOCK	|	BLOCK	|
---------------------------------------------------------------------------------------------------------------
NOTE: the bitmaps are 1 block each. I was too lazy to separate them.

This is heavily simplified, but it gets the point across. The very first sector of the disk is going to be occupied by the boot record, or the boot file. But then after
that, the "first" thing our file is going to house is our FS. The first thing the bootloader will read is the superblock, and it'll mount it... I think.
This is where the issues arise and I kind of don't really understand the "mounting of a filesystem" thing, and I will have to read about it a little bit. From what I saw
of watching Queso Fuego's videos, he seems to have made an entirely separate C script that generates an image (similar to what I do, but with a makefile), but the image
already has the the filesystem loaded into it.. Which.. Ok, yeah, that makes sense, but why use a C script? Why not just use the available "dd" command? The next issue
comes with the actual reading of the superblock in the bootloader. My plan is to, instead of relying on exact CHS addressing for reading from disk to memory for like
the 2nd stage bootloader, pre_kernel, and kernel, I could rely on the superblock because, well, everything is in fact there. But, how do I load everything INTO the
filesystem before boot? I can't really do it in a Makefile, (unless I can, and I just am not good at coding Makefiles at all). Hmm, I'll have to see, but for today, I
might just make delete_file, make_dir, delete_dir, and that should be enough. I also spent today going over my code, making sure I didn't have any issues/bugs that could
arise. I think, for today (Day 45), I will probably just continue making some fs functions, because the OSDev wiki seems to be down, and I don't really wanna start 
figuring out how to mount my fs with all of the stuff inside of it without any guidance/help/tutorial, (I guess I could use Queso Fuego's video as guidance? I'll see).

Oh, yes, I should state this. The partition I showed you above is... Not accurate for more complex file systems, like ext2/3/4, (and maybe even FAT?). If you look at the
OSDev wiki page for EXT2, you will see some extra things that I do not include in the diagram above. In reality, if you're deciding to just make a driver for an already
exisiting fs, like ext2/3/4, you will instead have this diagram:

--------------------------------------------------------------------------------------------------------------------------
|		|	BLOCK	|		|		|		|		|		|
|	SUPER 	|	GROUP	|	DATA	|	INODE	|	INODE	|	DATA	| 	DATA	| ...
|	BLOCK	|DESCRIPTOR	|	BITMAP	|	BITMAP	|	BLOCK	|	BLOCK	|	BLOCK	|
---------------------------------------------------------------------------------------------------------------------------

That's right, many file systems have an extra block for the BLOCK GROUP DESCRIPTOR. Basically, each block is 4KB, right? Well, if if multiple 4KB blocks are being used
for the same general purpose, then we should realistically group them up so that we KNOW that this group. I think. I don't fully get the point of the block group
descriptor, and my few minutes of researching it didn't really bear any fruit, but just know that file systems like ext2/3/4 (and maybe FAT?) have block group 
descriptors, that are basically just larger, bigger blocks grouped together and are generally used in larger file systems. Like, for example, in ext2 
block group 0 is where the file systems superblock, block group descriptor, data bitmap, inode bitmap, inode block, and a few data blocks live. The descriptor
keeps track of a bunch of stuff regarding these block groups, and I suggest looking at the OSDev wiki for what they are. BUT, for the sake of my fs, I don't care about
block group descriptors, because, again, this OS is mostly for LEARNING about low-level stuff right now. In the future, I may decide to sophisticate my OS and make it
much better in the future, but for now, we are just going to have our fs looking like so: 

----------------------------------------------------------------------------------------------------------
|		|		|		|		|		|		|
|	SUPER 	|	DATA	|	INODE	|	INODE	|	DATA	| 	DATA	| ...
|	BLOCK	|	BITMAP	|	BITMAP	|	BLOCK	|	BLOCK	|	BLOCK	|
----------------------------------------------------------------------------------------------------------

----------------------------------
DAY 45

I wrote DAY 42, 43, 44's entry today, (day 45), so I will make the day 45 devlog a little bit weirder. I am going to write as I go, because I have come to despise writing
as much as I do for the devlogs. 

Anyway, I ended off day 42, 43, 44's entry with talking about how I can mount the file system. I believe I've found something that can help me with that. Ext2 and the 
other Linux file systems, (and I think FAT too, even), have a special "command" called "mkfs", which basically mounts and puts the filesystem into the disk. Great! Cool,
I just need to understand how mkfs works, and then implement an mkfs for MY FS too. Or, perhaps I can just use their mkfs? I have no idea, I just found out about this
from a stackexchange page: (https://unix.stackexchange.com/questions/727222/how-do-i-make-a-new-filesystem-image)

Alright, I've done some more research with mkfs and I've uncovered what I believe I have to do. Of course, I need to make something similar to mkfs. The way mkfs works
is that it has a couple of different flags and whatnot, but I will be particularly focussing on this line:
mkfs -t ext2 $(BUILD_DIR)/os-image.img
What the line above does is it overlays the ext2 file system over the disk image, os-image.img. But, that's not all. See, as I stated before in the previous devlog entry,
the file system has a couple of things it needs to initialize almost immediately in order for it to work properly. I gave you this showcase of a disk:

----------------------------------------------------------------------------------------------------------
|		|		|		|		|		|		|
|	SUPER 	|	DATA	|	INODE	|	INODE	|	DATA	| 	DATA	| ...
|	BLOCK	|	BITMAP	|	BITMAP	|	BLOCK	|	BLOCK	|	BLOCK	|
----------------------------------------------------------------------------------------------------------

But that's not fully accurate. See, there is actually ONE MORE THING that is within the disk before all of this, (which I also briefly mentioned), and that is the boot
sector. The bootsector is sector 0 of the disk, always, and it is where the BIOS will read to check and see if there is a functioning bootloader at that sector, (hence
why we are able to even boot in the first place despite nothing being in memory when we type "make run"). So, really, on sector 0, (and thus, really block 0), we have 
the bootloader existing there that will be loaded in first. Afterwards, that bootloader will then load the superblock to memory, read it, and then also load everything
else it needs to load from the disk into memory (i.e., it will load the second stage bootloader, the pre-kernel, and perhaps also the kernel). That is exactly what
mkfs does, (sort of). See, on top  of  making sure the bootsector is zero'd out and putting down a very basic bootloader into the bootsector, (or sector 0), that will
try to jump to a bootable "device", it also initializes the superblock with all of the necessary information that we need to be able to actually jump to our other pieces
of code. Also, ext2 has a bunch of other stuff it does to format the disk too, but, yeah. We need to do the same thing with our fs, and that is also why it seems that
Queso Fuego makes a make_disk.c script, which I will too. I will TRY to do this on my own, but I have never actually used the file reader writer library in C, so it will
be quite a bit of a struggle. (I might cave in and look at Queso Fuego's implementation if it really is not working out at all, though. But, I want to avoid that).

Alright, so I tried to do it and.. It didn't really work. I couldn't think of anything, and had to look at Queso Fuego's code to really get an idea of what to do. I think,
tomorrow, instead of trying to brute force my way through format_disk, I'll take a step back and try to understand the library functions and whatnot and how they work.
I did learn, today, about a trick you can use to read files, which is via fopen(), fseek(), and ftell(), as you can open a file (set a pointer) into a file, use fseek
to set the pointer to be at the end of the file, and use ftell to read where the pointer is, which will basically give you the size of the file. Tomorrow will be mostly
experimenting with the file stuff, and Sunday might be trying to write it. That's all for today, adios.

----------------------------------
DAY 46 & 47

Combining Monday and Tuesday because I'm lazy.
I finished up the file system implementation on day 45, as I stated before, so these two days were mostly just working on figuring out how to mount the filesystem. I 
used Queso Fuego's make_disk.c as a guide, for now. Right now, I just want to make sure that my file system implementation is CORRECT and that it WORKS, because the 
details and implementation of my filesystem are still fresh on my mind. Had I moved to explicitly study how I/O stuff works with C (i.e, stdlib, stdio, and dirent), I
would've probably lost some of the focus I had for my fs. In the end, it was fine, though. On monday I was actually still able to learn a LOT about dirent and stdlib
because, although I was using his make_disk.c as a guide, I was still trying to do things on my own and figure things out. I don't really like copy and pasting code, 
because that just feels awful. Monday I "finished" the format_disk.c, and Tuesday, today, I actually spent the whole day debugging my format_disk.c, because it turns out,
I am actually quite stupid and made a lot of mistakes, (over university I took a grand total of 0 C classes, so it seems I just forgot all about C, and the fact that 
printf holds onto the strings in a buffer until either we put in an escape key, like \n, or we do like fflush(stdout) that will flush out the stdout buffer). I also just
seemingly forgot about the infinite loops occuring in my code, AND I had such an atrocious typo in the middle of my code that it took me 4 hours to debug, (I read through
the code like 8 times before I found the issue. I was updating dir_inode instead of new_file). That atriocous typo was quite bad, as it was basically causing the entire
prekernel and kernel to override the first two blocks, making it so the OS never was able to boot because there was no boot signature, (duh). That was basically all I did
today. Tomorrow I plan on actually making it so the bootloader, 2nd stage bootloader, and prekernel correctly set things up. Queso Fuego uses ATA PIO for the bootloader,
and I tried using ATA PIO before, but it just didn't work, and I reaalllyy don't want have to mess with all of that again, so I'll try to just stick to INT 0x13, but this
time, I'm gonna be using LBA addressing mode instead of CHS, because it seems a lot easier to manage and whatnot. I'll also be stealing Queso Fuego's basic idea, which
is to load in the 1st block AND 2nd block, (or 15 sectors. It should be 16, but 1st sector = bootloader, so 15 sectors) into memory, thereby loading the 2nd stage 
bootloader and superblock into memory at starting at 0x7E00. After that, loading the prekernel and kernel might be a bit difficult, as I will have to do some inode 
shenanigans with them, but it should be fine, as I know which inode to look into to find the inode stuff. Actually, while writing this, the format_disk.c file on the
github still has quite a few issues with it. Here they are:
1. It doesn't write out the data bitmap
2. It doesn't write out the inode table block
3. It doesn't write out the kernel.bin dir_entry
Now these are quite severe issues. Thankfully, after literally 3 more hours of slamming my head against the desk and going insane, I figured out that the issue was that
I was overriding it because my superblock sucks... And also because I made some stupid mistakes in the function, but those were minor and I fixed those one quickly.
I made quite a few typos in the superblock, and I believe I've fixed everything? There are still a few quirks, I think? For one, I cannot decipher the inode table 
block at all, I genuinely have no idea if it works or not, guess I'll have to test it and see. Two, in the inode table block, it randomly skips 00004140. Not sure what
that's about. Hope it wasn't something important. And finally, the boot.bin file, inside the dir_entry thing is called boot.bin.n.bin, which is definitely not the right
name. This one should be easy to fix, though, I am pretty sure that this is only occuring because of strcpy and not wiping the buffer clean after each use, which I should
do. Everything else looks correct based on the hexdump? Like, obviously I cannot decipher everything, but it seems the string literals in the hexdump for the kernel and
prekernel look correct, and their sizes look correct too: kernel is 4 blocks, (so around 32 sectors, which makes sense, I believe the kernel is 27 sectors, and prekernel
is 10 sectors, and its correctly allocated 2 blocks. Sooo, everything seems normal). 
Anyway, that's all for today. Adios.

----------------------------------
DAY 48

Hello,
I have managed to actually mount the file system fully. I did a bit of bug fixing today and managed to fix the issues that were plaguing the disk, (except the 
boot.bin.n.bin one, but that's a pretty minor issue). I found out why it was skipping 00004140, and why the inode table was next to impossible to read. It was because
my inode_t and fs_date_t or whatever it was called, were not actually their right size! I forgot that the compiler likes to pad out bytes to keep them 4-byte aligned, 
hence why if you look at the structs now, they'll have __attribute__((packed)), as that tells the compiler to NOT pad out the bytes and make them 4-byte aligned, and 
allows us to achieve whatever byte number we wanted. Now, the inode_t struct is actually 64 bytes, and the fs_date_t struct is actually 8 bytes, and the inode array in
the the disk image is much more readable and understandable. Now, I think we have actually mounted the fs. After doing that, I started work on a new bootloader. I was
tempted to retry ATA PIO, but decided against it, and just went with the classic INT 0x13, because I don't really care about efficiency and nice-ness in this OS. All
I care about is if it works. Originally my plan was to NOT load in the kernel immediately, as it is quite a large file (4 blocks almost), and instead I'll load the kernel
straight from disk in the prekernel, like Queso Fuego. After 3 hours of trying to figure out how I'll do this, I decided against it for several reasons.
1. If I do decide to load from prekernel, I'll need to link fs.o into the prekernel linker, along with stdio.o, exceptions, idt, among other things, and that'll make
the prekernel absolutely gigantic.
2. I have 0 hindsight apparently, and I didn't seem to catch that: if I made bytes_to_blocks in fs.h, and fs.c includes fs.h, and prekernel.c will also include fs.h,
then there will be multiple versions of bytes_to_blocks. Granted, this is not an issue unique to the prekernel, and it is something I'll have to fix regardless of if I
include it in the prekernel or kernel, but still. 
3. On top of the fact that it'll make the prekernel absolutely gigantic, it'll also force me to deal with global variables shenanigans. (Not really, as I can just NOT 
touch the global variables for those files, but I will have to deal with global variables shenanigans for the fs.c, which is not very fun.)
With these three reasons combined, I just decided "who cares", and I decided to just load the kernel from the bootloader and 2ndstage bootloader. Then, I can just have
the kernel have access to fs.o, which will remove the worry and headaches of multiple bin files with linker scripts that link together the same file. Great. Then, I can
just fix the one problem which is the bytes_to_blocks thing, and I'll be good to go to test out my filesystem implementation. I'll also have to go to format_disk.c and
reserve the inode for the kernel to be locked to 5, or maybe 3 I dunno, so that I can always load the kernel from the bootloader, even if we add more files and whatnot.
I'll also also have to change the inode bitmap and data bitmaps in format_disk.c so that they reserve the inodes for the kernel and whatnot and the data blocks for the
kernel and whatnot. I imagine other file systems do this too, as it would be a pretty big security risk if the user could just, y'know, override the kernels inode and
data blocks. Anyway, yeah, if my file system implementation turns out to work relatively well without any MAJOR issues, then cool, we can move on to user land and start
making programs for it, and start making us actually be able to do stuff in user land, (all we do right now is enter user mode, and immediately trigger a user-allowed
interrupt, and then a user-banned interrupt, causing everything to halt). 
That's all for today, adios.

----------------------------------
DAY 49
Hello,
Today I wanted to really test out my fs, but I had a couple of things to do before I could do that. First and foremost, I just tried to simply compile and link my fs.o
into the kernel, and also obviously do #include "fs/fs.h", but that... Created issues. For some reason, interrupt handlers that were doing level changes from ring 3 to
ring 0 were working, but also not working, as my kprintf just stopped working inside these functions, (not fully, but they printed garbage). Now, I have come across this
a million times already, and I knew this was some memory corruption going on. But, despite this, it still took me forever to realize that this was happening because I
wasn't initializing the global variables in my fs implementation. Now, why is this an issue? I don't fully know, because uninitialized variables don't exist on the stack,
they exist in their own separate place known as the bss section, and realistically, the stack should never really be interracting with the bss section, as they both exist
in separate parts of memory. Matter of fact, global variables should have no INFLUENCE on the stack, as the stack is only for local variables and anything similar, so..
Why was stdio breaking because of uninitialized global variables? I dunno. But whatever, it fixed itself, hopefully, so that's nice. Anyway, second issue was the
bytes_to_blocks that I mentioned. For that, I decided to just split the functions/commands of fs from fs.h and put it into a separate header file called fs_commands.h,
and also made fs.c fs_commands.c instead. Now, fs.h has no bytes_to_blocks function. But wait! format_disk needs it, no? You're right, so to fix that I just put
bytes_to_blocks inside format_disk.c. Also, since I'm lazy, I decided to also save inode 3 for the kernel, so now the first 4 bits are reserved ( inode 0 = not-existant,
inode 1 = root dir, inode 2 = prekernel, inode 3 = kernel). Next issue was RW_SECTORS. See, for some reason, I was trying to read/write one byte at a time from the ATA PIO
data ports, but I'm not supposed to do that, at least I don't think. At least, whenever I was doing it, it wasn't working as expected, so I just moved over to reading/
writing 1 word at a time, (which is what I was supposed to be doing anyway, because according to OSDev: https://wiki.osdev.org/ATA_PIO_Mode#Registers, the data port is
16-bits, so I was supposed to be reading 1 word at a time). So I spent another 4 hours trying to debug that before I gave up and decided to read the ATA PIO wiki, found
out that the data port is 16 bits, realized I was being an idiot, and then started reading 16-bits. Now rw_sectors works, and so does ONE OTHER FUNCTION that I tested out!
(that function was get_inode_in_dir(), so that's nice. Tomorrow I'll test more functions). Anyway, that's all for today. 

----------------------------------
DAY 50
Hello,
This morning my computer was failing to boot and I was experiencing a DPC WATCHDOG VIOLATION, which basically means my drivers (or hardware) were having some issues. I
freaked out, took my dog on a walk, came back like 2 hours later, tried again, and suddenly it was working. Then, due to lack of sleep, I fell asleep at like 2 PM.
In the end, I kind of didn't do a whole lot today, BUT, I managed to confirm that my load_file is in fact working as well, along with the get_inode() function, so that's
nice. Tomorrow I'll probably do a little bit more testing, but that's all for today.

----------------------------------
DAY 51
Hello, 
I am actually writing this on day 53, because I did some stuff on day 51 that I thought was interesting, or just confused me. Basically, I am relatively confident that
my FS works, (of course, printdir doesn't work, but I can fix that up later. It's not a huge deal that it doesn't work right now), so I want to move on to the "final"
stage, which is making user processes and commands, and allowing for the user to do stuff and making my OS look visually appealing. While testing out my syscalls and
stlib, I found out that... It doesn't work. For some reason, some random interrupts occured, like an interrupt 13 with a random error code that didn't make any sense, 
(like 0x5AA or something), OR, in other cases, an interrupt 6 happened (interrupt 13 is a GPF, interrupt 6 is invalid opcode). I had no idea why these issues were
occuring until I decided to go back and initialize the malloc global variables inside the kernel. Then, all of a sudden, the stdlib and syscall stuff started magically
working! Why wasn't it working beforehand and why does it work now? I have no clue. After that, I got curious, and I decided to go back and update my prekernel to not
use printlite.c, but instead use my stdio, and would you believe it, there were no issues! So then why were there issues with my stdio before way back when I tried to
make my trampoline code? Why is it suddenly OK with stdio.c in the prekernel? These are all things that I have no answer for and I really don't understand at all. It's
definitely related to how compilers treat global variables vs local variables, along with linkers and whatnot, but I'm afraid I don't know enough about programs and
processes and linkers to actually be able to understand why these were issues, or are no longer issues anymore. BUT, at the very least, I now know that I can SUPPOSEDLY
link stdio.c with other programs and be able to use it without any real issues, as long as I clear the screen (if I don't clear the screen it just looks ugly. It doesn't
break anything, it just looks ugly). Soo.. That's nice. Anyway, plan for today (day 53) is to write some more syscalls, primarily the important ones first: sysenter
and sysexit, as they supposedly enable fast entry/exit into kernel mode/user mode without the interrupt overhead that INT 0x80 provides (according to OSDev at least),
and then also do some syscalls relating to files. When I read OSTEP and was reading the pillar on persistence, it mentioned that there are 3 important syscalls that an
OS implements for anything related to files. Those 3 syscalls are: sysopen, sysread, syswrite. If you want to write to a file, what happens is the following:
sysopen finds the location of the and "opens" it in a certain mode with certain permissions (like O_READ, O_WRITE, etc.), (I heavily simplified what sysopen does here. 
In reality, sysopen is arguably one of the most intensive syscalls, as it has to read the inode of the root inode, go to the root inode block, read the root inode block
for the directory entry that contains the i_number for the directory that houses the file we want, go to that inode, go to that directory's data block, read the i_number
of the file/keep going down directories, go to the files inode, and NOW all we have to do is read the inode into memory, creates a file descriptor, and returns the file 
descriptor). After performing sysopen, sysread can read from the memory location where sysopen placed the inode, (thanks to the file descriptor) it then goes to read the 
data blocks the inode is associated with, and then it will update an in memory open file table by updating the file descriptor/creating a file descriptor (I think) for 
the file by updating the offset, and updating its permissions, (again, I think. It's been a while since I read that chapter in OSTEP). Sysread also does some other things
too, specifically, it reads from the file and writes to a buffer. We can also give sysread more arguments to specify how many bytes we want to write to the buffer. Then
syswrite can USE that buffer and write to the appropriate device/file/whatever based on what the user wants. There's also a fourth important syscall, close, which simply
says "we're done with this file", and removes it from the open file table, allowing us to open other files, (we don't want the user to be able to open an infinite
number of files at one time, right?). So yeah, that's the plan for today. Do some syscalls and whatnot.
